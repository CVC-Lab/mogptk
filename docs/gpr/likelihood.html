<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>mogptk.gpr.likelihood API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{overflow:hidden;margin:0;padding:0;line-height:1.5em}iframe{border:0}#content{overflow:auto;height:100vh;box-sizing:border-box;padding:20px}#sidebar{box-sizing:border-box;padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{display:none;font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;max-width:400px;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>mogptk.gpr.likelihood</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L1-L772" class="git-link">Browse git</a>
</summary>
<pre><code class="python">import torch
import numpy as np
from . import config, Parameter

def identity(x):
    &#34;&#34;&#34;
    The identity link function given by

    $$ y = x $$
    &#34;&#34;&#34;
    return x

def square(x):
    &#34;&#34;&#34;
    The square link function given by

    $$ y = x^2 $$
    &#34;&#34;&#34;
    return torch.square(x)

def exp(x):
    &#34;&#34;&#34;
    The exponential link function given by

    $$ y = e^{x} $$
    &#34;&#34;&#34;
    return torch.exp(x)

def inv_probit(x):
    &#34;&#34;&#34;
    The inverse probit link function given by

    $$ y = \\frac{1}{2} \\left(1 + \\operatorname{erf}(x/\\sqrt{2})\\right) $$
    &#34;&#34;&#34;
    jitter = 1e-3
    return 0.5*(1.0+torch.erf(x/np.sqrt(2.0))) * (1.0-2.0*jitter) + jitter

# also inv_logit or sigmoid
def logistic(x):
    &#34;&#34;&#34;
    The logistic, inverse logit, or sigmoid link function given by

    $$ y = \\frac{1}{1 + e^{-x}} $$
    &#34;&#34;&#34;
    return 1.0/(1.0+torch.exp(-x))

class GaussHermiteQuadrature:
    def __init__(self, deg=20, t_scale=None, w_scale=None):
        t, w = np.polynomial.hermite.hermgauss(deg)
        t = t.reshape(-1,1)
        w = w.reshape(-1,1)
        if t_scale is not None:
            t *= t_scale
        if w_scale is not None:
            w *= w_scale
        self.t = torch.tensor(t, device=config.device, dtype=config.dtype)  # degx1
        self.w = torch.tensor(w, device=config.device, dtype=config.dtype)  # degx1
        self.deg = deg

    def __call__(self, mu, var, F):
        return F(mu + var.sqrt().mm(self.t.T)).mm(self.w)  # Nx1

class Likelihood(torch.nn.Module):
    &#34;&#34;&#34;
    Base likelihood.

    Args:
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.
    &#34;&#34;&#34;
    def __init__(self, quadratures=20):
        super().__init__()

        self.quadrature = GaussHermiteQuadrature(deg=quadratures, t_scale=np.sqrt(2), w_scale=1.0/np.sqrt(np.pi))
        self.output_dims = None

    def __setattr__(self, name, val):
        if name == &#39;train&#39;:
            for p in self.parameters():
                p.train = val
            return
        if hasattr(self, name) and isinstance(getattr(self, name), Parameter):
            raise AttributeError(&#34;parameter is read-only, use Parameter.assign()&#34;)
        if isinstance(val, Parameter) and val._name is None:
            val._name = &#39;likelihood.&#39; + name
        super().__setattr__(name, val)

    def validate_y(self, y, X=None):
        &#34;&#34;&#34;
        Validate whether the y input is within the likelihood&#39;s support.
        &#34;&#34;&#34;
        pass

    def log_prob(self, y, f, X=None):
        &#34;&#34;&#34;
        Calculate the log probability

        $$ \\log(p(y|f)) $$

        with \\(p(y|f)\\) our likelihood.
        &#34;&#34;&#34;
        # y: Nx1
        # f: NxM
        raise NotImplementedError()

    def variational_expectation(self, y, mu, var, X=None):
        &#34;&#34;&#34;
        Calculate the variational expectation

        $$ \\int \\log(p(y|f)) \\; q(f) \\; df $$

        where \\(q(f) \\sim \\mathcal{N}(\\mu,\\Sigma)\\) and \\(p(y|f)\\) our likelihood. By default this uses Gauss-Hermite quadratures to approximate the integral.

        Args:
            y (torch.tensor): Values for y of shape (data_points,).
            mu (torch.tensor): Mean of the posterior \\(q(f)\\) of shape (data_points,).
            var (torch.tensor): Variance of the posterior \\(q(f)\\) of shape (data_points,).
            X (torch.tensor): Input of shape (data_points,input_dims) needed for multi-output likelihood.
        &#34;&#34;&#34;
        # y,mu,var: Nx1
        q = self.quadrature(mu, var, lambda f: self.log_prob(y,f,X=X))  # Nx1
        return q.sum()  # sum over N

    def mean(self, f, X=None):
        &#34;&#34;&#34;
        Calculate the mean of the likelihood.

        Args:
            f (torch.tensor): Posterior values for f of shape (data_points,input_dims).
            X (torch.tensor): Input of shape (data_points,input_dims) needed for multi-output likelihood.

        Returns:
            torch.tensor: Mean of the predictive posterior \\(p(y|f)\\) of shape (data_points,).
        &#34;&#34;&#34;
        # f: NxM
        raise NotImplementedError()

    def variance(self, f, X=None):
        &#34;&#34;&#34;
        Calculate the variance of the likelihood.

        Args:
            f (torch.tensor): Posterior values for f of shape (data_points,input_dims).
            X (torch.tensor): Input of shape (data_points,input_dims) needed for multi-output likelihood.

        Returns:
            torch.tensor: Variance of the predictive posterior \\(p(y|f)\\) of shape (data_points,).
        &#34;&#34;&#34;
        # f: NxM
        raise NotImplementedError()

    def predict(self, mu, var, X=None, full=False):
        &#34;&#34;&#34;
        Calculate the mean and variance of the predictive distribution

        $$ \\mu = \\iint y \\; p(y|f) \\; q(f) \\; df dy $$
        $$ \\Sigma = \\iint y^2 \\; p(y|f) \\; q(f) \\; df dy - \\mu^2 $$

        where \\(q(f) \\sim \\mathcal{N}(\\mu,\\Sigma)\\) and \\(p(y|f)\\) our likelihood. By default this uses Gauss-Hermite quadratures to approximate both integrals.

        Args:
            mu (torch.tensor): Mean of the posterior \\(q(f)\\) of shape (data_points,).
            var (torch.tensor): Variance of the posterior \\(q(f)\\) of shape (data_points,).
            X (torch.tensor): Input of shape (data_points,input_dims) needed for multi-output likelihood.
            full (boolean): Return the full covariance matrix.

        Returns:
            torch.tensor: Mean of the predictive posterior \\(p(y|f)\\) of shape (data_points,).
            torch.tensor: Variance of the predictive posterior \\(p(y|f)\\) of shape (data_points,) or (data_points,data_points).
        &#34;&#34;&#34;
        # mu,var: Nx1
        # TODO: full covariance matrix for likelihood predictions
        if full:
            raise NotImplementedError(&#34;full covariance not support for likelihood predictions&#34;)

        Ey = self.quadrature(mu, var, lambda f: self.mean(f,X=X))
        Eyy = self.quadrature(mu, var, lambda f: self.mean(f,X=X).square() + self.variance(f,X=X))
        return Ey, Eyy-Ey**2

class MultiOutputLikelihood(Likelihood):
    &#34;&#34;&#34;
    Multi-output likelihood to assign a different likelihood per channel.

    Args:
        likelihoods (mogptk.gpr.likelihood.Likelihood): List of likelihoods equal to the number of output dimensions.
    &#34;&#34;&#34;
    def __init__(self, *likelihoods):
        super().__init__()

        if isinstance(likelihoods, tuple):
            if len(likelihoods) == 1 and isinstance(likelihoods[0], list):
                likelihoods = likelihoods[0]
            else:
                likelihoods = list(likelihoods)
        elif not isinstance(likelihoods, list):
            likelihoods = [likelihoods]
        if len(likelihoods) == 0:
            raise ValueError(&#34;must pass at least one likelihood&#34;)
        for i, likelihood in enumerate(likelihoods):
            if not issubclass(type(likelihood), Likelihood):
                raise ValueError(&#34;must pass likelihoods&#34;)
            elif isinstance(likelihood, MultiOutputLikelihood):
                raise ValueError(&#34;can not nest MultiOutputLikelihoods&#34;)

        self.output_dims = len(likelihoods)
        self.likelihoods = likelihoods

    def _channel_indices(self, X):
        c = X[:,0].long()
        m = [c==j for j in range(self.output_dims)]
        r = [torch.nonzero(m[j], as_tuple=False).reshape(-1) for j in range(self.output_dims)]  # as_tuple avoids warning
        return r

    def validate_y(self, y, X=None):
        if self.output_dims == 1:
            self.likelihoods[0].validate_y(y, X=X)
            return

        r = self._channel_indices(X)
        for i in range(self.output_dims):
            self.likelihoods[i].validate_y(y[r[i],:], X=X)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        if self.output_dims == 1:
            return self.likelihoods[0].log_prob(y,f)

        r = self._channel_indices(X)
        res = torch.empty(f.shape, device=config.device, dtype=config.dtype)
        for i in range(self.output_dims):
            res[r[i],:] = self.likelihoods[i].log_prob(y[r[i],:], f[r[i],:])
        return res  # NxM

    def variational_expectation(self, y, mu, var, X=None):
        # y,mu,var: Nx1
        if self.output_dims == 1:
            return self.likelihoods[0].variational_expectation(y,mu,var)

        q = torch.tensor(0.0, dtype=config.dtype, device=config.device)
        r = self._channel_indices(X)
        for i in range(self.output_dims):
            q += self.likelihoods[i].variational_expectation(y[r[i],:], mu[r[i],:], var[r[i],:]).sum()  # sum over N
        return q

    def mean(self, f, X=None):
        # f: NxM
        if self.output_dims == 1:
            return self.likelihoods[0].mean(f)

        r = self._channel_indices(X)
        res = torch.empty(f.shape, device=config.device, dtype=config.dtype)
        for i in range(self.output_dims):
            res[r[i],:] = self.likelihoods[i].mean(f[r[i],:])
        return res  # NxM

    def variance(self, f, X=None):
        # f: NxM
        if self.output_dims == 1:
            return self.likelihoods[0].variance(f)

        r = self._channel_indices(X)
        res = torch.empty(f.shape, device=config.device, dtype=config.dtype)
        for i in range(self.output_dims):
            res[r[i],:] = self.likelihoods[i].variance(f[r[i],:])
        return res  # NxM

    # TODO: predict is not possible?
    #def predict(self, mu, var, X=None, full=False):
    #    # mu: Nx1
    #    # var: Nx1 or NxN
    #    if self.output_dims == 1:
    #        return self.likelihoods[0].predict(mu,var,full=full)

    #    r = self._channel_indices(X)
    #    Ey = torch.empty(mu.shape, device=config.device, dtype=config.dtype)
    #    Eyy = torch.empty(var.shape, device=config.device, dtype=config.dtype)
    #    if full:
    #        for i in range(self.output_dims):
    #            r1 = r[i].reshape(-1,1)
    #            r2 = r[i].reshape(1,-1)
    #            Ey[r[i],:], Eyy[r1,r2] = self.likelihoods[i].predict(mu[r[i],:], var[r1,r2], full=True)
    #    else:
    #        for i in range(self.output_dims):
    #            Ey[r[i],:], Eyy[r[i],:] = self.likelihoods[i].predict(mu[r[i],:], var[r[i],:], full=False)
    #    return Ey, Eyy-Ey.square()

class GaussianLikelihood(Likelihood):
    &#34;&#34;&#34;
    Gaussian likelihood given by

    $$ p(y|f) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(y-f)^2}{2\\sigma^2}} $$

    with \\(\\sigma\\) the scale.

    Args:
        scale (float): Scale.

    Attributes:
        scale (mogptk.gpr.parameter.Parameter): Scale \\(\\sigma\\).
    &#34;&#34;&#34;
    def __init__(self, scale=1.0):
        super().__init__()

        self.scale = Parameter(scale, lower=config.positive_minimum)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        p = -0.5 * (np.log(2.0*np.pi) + 2.0*self.scale().log() + ((y-f)/self.scale()).square())
        return p  # NxM

    def variational_expectation(self, y, mu, var, X=None):
        # y,mu,var: Nx1
        p = -((y-mu).square() + var) / self.scale().square()
        p -= np.log(2.0 * np.pi)
        p -= 2.0*self.scale().log()
        return 0.5*p.sum()  # sum over N

    def mean(self, f, X=None):
        return f

    def variance(self, f, X=None):
        return self.scale().square()

    def predict(self, mu, var, X=None, full=False):
        if full:
            return mu, var + self.scale().square()*torch.eye(var.shape[0])
        else:
            return mu, var + self.scale().square()

class StudentTLikelihood(Likelihood):
    &#34;&#34;&#34;
    Student&#39;s t likelihood given by

    $$ p(y|f) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\pi\\nu}\\sigma} \\left( 1 + \\frac{(y-f)^2}{\\nu\\sigma^2} \\right)^{-(\\nu+1)/2} $$

    with \\(\\nu\\) the degrees of freedom and \\(\\sigma\\) the scale.

    Args:
        dof (float): Degrees of freedom.
        scale (float): Scale.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.

    Attributes:
        scale (mogptk.gpr.parameter.Parameter): Scale \\(\\sigma\\).
    &#34;&#34;&#34;
    def __init__(self, dof=3, scale=1.0, quadratures=20):
        super().__init__(quadratures)

        self.dof = torch.tensor(dof, device=config.device, dtype=config.dtype)
        self.scale = Parameter(scale, lower=config.positive_minimum)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        p = -0.5 * (self.dof+1.0)*torch.log1p(((y-f)/self.scale()).square()/self.dof)
        p += torch.lgamma((self.dof+1.0)/2.0)
        p -= torch.lgamma(self.dof/2.0)
        p -= 0.5 * torch.log(self.dof*np.pi*self.scale().square())
        return p  # NxM

    def mean(self, f, X=None):
        if self.dof &lt;= 1.0:
            return torch.full(f.shape, np.nan, device=config.device, dtype=config.dtype)
        return f

    def variance(self, f, X=None):
        if self.dof &lt;= 2.0:
            return np.nan
        return self.scale().square() * self.dof/(self.dof-2.0)

class ExponentialLikelihood(Likelihood):
    &#34;&#34;&#34;
    Exponential likelihood given by

    $$ p(y|f) = h(f) e^{-h(f)y} $$

    with \\(h\\) the link function and \\(y \\in [0.0,\\infty)\\).

    Args:
        link (function): Link function to map function values to the support of the likelihood.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.
    &#34;&#34;&#34;
    def __init__(self, link=exp, quadratures=20):
        super().__init__(quadratures)

        self.link = link

    def validate_y(self, y, X=None):
        if torch.any(y &lt; 0.0):
            raise ValueError(&#34;y must be positive&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        if self.link == exp:
            p = -y/self.link(f) - f
        else:
            p = -y/self.link(f) - self.link(f).log()
        return p  # NxM

    def mean(self, f, X=None):
        return self.link(f)

    def variance(self, f, X=None):
        return self.link(f).square()

class LaplaceLikelihood(Likelihood):
    &#34;&#34;&#34;
    Laplace likelihood given by

    $$ p(y|f) = \\frac{1}{2\\sigma}e^{-\\frac{|y-f|}{\\sigma}} $$

    with \\(\\sigma\\) the scale.

    Args:
        scale (float): Scale.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.

    Attributes:
        scale (mogptk.gpr.parameter.Parameter): Scale \\(\\sigma\\).
    &#34;&#34;&#34;
    def __init__(self, scale=1.0, quadratures=20):
        super().__init__(quadratures)

        self.scale = Parameter(scale, lower=config.positive_minimum)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        p = -torch.log(2.0*self.scale()) - (y-f).abs()/self.scale()
        return p  # NxM

    def mean(self, f, X=None):
        return f

    def variance(self, f, X=None):
        return 2.0 * self.scale().square()

class BernoulliLikelihood(Likelihood):
    &#34;&#34;&#34;
    Bernoulli likelihood given by

    $$ p(y|f) = h(f)^k (1-h(f))^{n-k} $$

    with \\(h\\) the link function, \\(k\\) the number of \\(y\\) values equal to 1, \\(n\\) the number of data points, and \\(y \\in \\{0.0,1.0\\}\\).

    Args:
        link (function): Link function to map function values to the support of the likelihood.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.
    &#34;&#34;&#34;
    def __init__(self, link=inv_probit, quadratures=20):
        super().__init__(quadratures)

        self.link = link

    def validate_y(self, y, X=None):
        if torch.any((y != 0.0) &amp; (y != 1.0)):
            raise ValueError(&#34;y must have only 0.0 and 1.0 values&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        p = self.link(f)
        return torch.log(torch.where(0.5 &lt;= y, p, 1.0-p))  # NxM

    def mean(self, f, X=None):
        return self.link(f)

    def variance(self, f, X=None):
        return self.link(f) - self.link(f).square()

    def predict(self, mu, var, X=None, full=False):
        if self.link == inv_probit:
            p = self.link(mu / torch.sqrt(1.0 + var))
            if full:
                return p.diagonal().reshape(-1,1), p-p.square() # TODO: correct?
            return p, p-p.square()
        else:
            return super().predict(mu, var, X=X, full=full)

class BetaLikelihood(Likelihood):
    &#34;&#34;&#34;
    Beta likelihood given by

    $$ p(y|f) = \\frac{\\Gamma(\\sigma)}{\\Gamma\\left(h(f)\\sigma\\right)\\Gamma\\left((1-h(f)\\sigma\\right)} y^{h(f)\\sigma} (1-y)^{(1-h(f))\\sigma} $$

    with \\(h\\) the link function, \\(\\sigma\\) the scale, and \\(y \\in (0.0,1.0)\\).

    Args:
        scale (float): Scale.
        link (function): Link function to map function values to the support of the likelihood.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.

    Attributes:
        scale (mogptk.gpr.parameter.Parameter): Scale \\(\\sigma\\).
    &#34;&#34;&#34;
    def __init__(self, scale=1.0, link=inv_probit, quadratures=20):
        super().__init__(quadratures)

        self.link = link
        self.scale = Parameter(scale, lower=config.positive_minimum)

    def validate_y(self, y, X=None):
        if torch.any((y &lt;= 0.0) | (1.0 &lt;= y)):
            raise ValueError(&#34;y must be in the range (0.0,1.0)&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        mixture = self.link(f)
        alpha = mixture * self.scale()
        beta = (1.0-mixture) * self.scale()

        p = (alpha-1.0)*y.log()
        p += (beta-1.0)*torch.log1p(-y)
        p += torch.lgamma(alpha+beta)
        p -= torch.lgamma(alpha)
        p -= torch.lgamma(beta)
        return p  # NxM

    def mean(self, f, X=None):
        return self.link(f)

    def variance(self, f, X=None):
        mixture = self.link(f)
        return (mixture - mixture.square()) / (self.scale() + 1.0)

class GammaLikelihood(Likelihood):
    &#34;&#34;&#34;
    Gamma likelihood given by

    $$ p(y|f) = \\frac{1}{\\Gamma(k)h(f)^k} y^{k-1} e^{-y/h(f)} $$

    with \\(h\\) the link function, \\(k\\) the shape, and \\(y \\in (0.0,\\infty)\\). 

    Args:
        shape (float): Shape.
        link (function): Link function to map function values to the support of the likelihood.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.

    Attributes:
        shape (mogptk.gpr.parameter.Parameter): Shape \\(k\\).
    &#34;&#34;&#34;
    def __init__(self, shape=1.0, link=exp, quadratures=20):
        super().__init__(quadratures)

        self.link = link
        self.shape = Parameter(shape, lower=config.positive_minimum)

    def validate_y(self, y, X=None):
        if torch.any(y &lt;= 0.0):
            raise ValueError(&#34;y must be in the range (0.0,inf)&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        p = -y/self.link(f)
        p += (self.shape()-1.0)*y.log()
        p -= torch.lgamma(self.shape())
        if self.link == exp:
            p -= self.shape()*f
        else:
            p -= self.shape()*self.link(f).log()
        return p  # NxM

    def mean(self, f, X=None):
        return self.shape()*self.link(f)

    def variance(self, f, X=None):
        return self.shape()*self.link(f).square()

class PoissonLikelihood(Likelihood):
    &#34;&#34;&#34;
    Poisson likelihood given by

    $$ p(y|f) = \\frac{1}{y!} h(f)^y e^{-h(f)} $$

    with \\(h\\) the link function and \\(y \\in \\mathbb{N}_0\\).

    Args:
        link (function): Link function to map function values to the support of the likelihood.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.
    &#34;&#34;&#34;
    def __init__(self, link=exp, quadratures=20):
        super().__init__(quadratures)

        self.link = link

    def validate_y(self, y, X=None):
        if torch.any(y &lt; 0.0):
            raise ValueError(&#34;y must be in the range [0.0,inf)&#34;)
        if not torch.all(y == y.long()):
            raise ValueError(&#34;y must have integer count values&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        if self.link == exp:
            p = y*f
        else:
            p = y*self.link(f).log()
        p -= torch.lgamma(y+1.0)
        p -= self.link(f)
        return p  # NxM

    def mean(self, f, X=None):
        return self.link(f)

    def variance(self, f, X=None):
        return self.link(f)

class WeibullLikelihood(Likelihood):
    &#34;&#34;&#34;
    Weibull likelihood given by

    $$ p(y|f) = \\frac{k}{h(f)} \\left( \\frac{y}{h(f)} \\right)^{k-1} e^{-(y/h(f))^k} $$

    with \\(h\\) the link function, \\(k\\) the shape, and \\(y \\in (0.0,\\infty)\\).

    Args:
        shape (float): Shape.
        link (function): Link function to map function values to the support of the likelihood.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.

    Attributes:
        shape (mogptk.gpr.parameter.Parameter): Shape \\(k\\).
    &#34;&#34;&#34;
    def __init__(self, shape=1.0, link=exp, quadratures=20):
        super().__init__(quadratures)

        self.link = link
        self.shape = Parameter(shape, lower=config.positive_minimum)

    def validate_y(self, y, X=None):
        if torch.any(y &lt;= 0.0):
            raise ValueError(&#34;y must be in the range (0.0,inf)&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        if self.link == exp:
            p = -self.shape()*f
        else:
            p = -self.shape()*self.link(f).log()
        p += self.shape().log() + (self.shape()-1.0)*y.log()
        p -= (y/self.link(f))**self.shape()
        return p  # NxM

    def mean(self, f, X=None):
        return f * torch.lgamma(1.0 + 1.0/self.shape()).exp()

    def variance(self, f, X=None):
        a = torch.lgamma(1.0 + 2.0/self.shape()).exp()
        b = torch.lgamma(1.0 + 1.0/self.shape()).exp()
        return f.square() * (a - b.square())

class LogLogisticLikelihood(Likelihood):
    &#34;&#34;&#34;
    Log-logistic likelihood given by

    $$ p(y|f) = \\frac{(k/h(f)) (y/h(f))^{k-1}}{\\left(1 + (y/h(f))^k\\right)^2} $$

    with \\(h\\) the link function, \\(k\\) the shape, and \\(y \\in (0.0,\\infty)\\).

    Args:
        shape (float): Shape.
        link (function): Link function to map function values to the support of the likelihood.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.

    Attributes:
        shape (mogptk.gpr.parameter.Parameter): Shape \\(k\\).
    &#34;&#34;&#34;
    def __init__(self, shape=1.0, link=exp, quadratures=20):
        super().__init__(quadratures)

        self.link = link
        self.shape = Parameter(shape, lower=config.positive_minimum)

    def validate_y(self, y, X=None):
        if torch.any(y &lt; 0.0):
            raise ValueError(&#34;y must be in the range [0.0,inf)&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        if self.link == exp:
            p = -self.shape()*f
        else:
            p = -self.shape()*self.link(f).log()
        p -= 2.0*torch.log1p((y/self.link(f))**self.shape())
        p += self.shape().log()
        p += (self.shape()-1.0)*y.log()
        return p  # NxM

    def mean(self, f, X=None):
        return self.link(f) / torch.sinc(1.0/self.shape())

    def variance(self, f, X=None):
        if self.shape() &lt;= 2.0:
            return np.nan
        a = 1.0/torch.sinc(2.0/self.shape())
        b = 1.0/torch.sinc(1.0/self.shape())
        return self.link(f).square() * (a - b.square())

class LogGaussianLikelihood(Likelihood):
    &#34;&#34;&#34;
    Log-Gaussian likelihood given by

    $$ p(y|f) = \\frac{1}{y\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(\\log(y) - f)^2}{2\\sigma^2}} $$

    with \\(\\sigma\\) the scale and \\(y \\in (0.0,\\infty)\\).

    Args:
        scale (float): Scale.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.

    Attributes:
        scale (mogptk.gpr.parameter.Parameter): Scale \\(\\sigma\\).
    &#34;&#34;&#34;
    def __init__(self, scale=1.0, quadratures=20):
        super().__init__(quadratures)

        self.scale = Parameter(scale, lower=config.positive_minimum)

    def validate_y(self, y, X=None):
        if torch.any(y &lt;= 0.0):
            raise ValueError(&#34;y must be in the range (0.0,inf)&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        logy = y.log()
        p = -0.5 * (np.log(2.0*np.pi) + 2.0*self.scale().log() + ((logy-f)/self.scale()).square())
        p -= logy
        return p  # NxM

    def mean(self, f, X=None):
        return torch.exp(f + 0.5*self.scale().square())

    def variance(self, f, X=None):
        return (self.scale().square().exp() - 1.0) * torch.exp(2.0*f + self.scale().square())

class ChiSquaredLikelihood(Likelihood):
    &#34;&#34;&#34;
    Chi-squared likelihood given by

    $$ p(y|f) = \\frac{1}{2^{f/2}\\Gamma(f/2)} y^{f/2-1} e^{-y/2} $$

    with \\(y \\in (0.0,\\infty)\\).

    Args:
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.
    &#34;&#34;&#34;
    def __init__(self, quadratures=20):
        super().__init__(quadratures)

    def validate_y(self, y, X=None):
        if torch.any(y &lt;= 0.0):
            raise ValueError(&#34;y must be in the range (0.0,inf)&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        p = -0.5*f*np.log(2.0) - torch.lgamma(f/2.0) + (f/2.0-1.0)*y.log() - 0.5*y
        return p  # NxM

    def mean(self, f, X=None):
        return f

    def variance(self, f, X=None):
        return 2.0*f</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="mogptk.gpr.likelihood.exp"><code class="name flex">
<span>def <span class="ident">exp</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>The exponential link function given by</p>
<p><span><span class="MathJax_Preview"> y = e^{x} </span><script type="math/tex; mode=display"> y = e^{x} </script></span></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L21-L27" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def exp(x):
    &#34;&#34;&#34;
    The exponential link function given by

    $$ y = e^{x} $$
    &#34;&#34;&#34;
    return torch.exp(x)</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.likelihood.identity"><code class="name flex">
<span>def <span class="ident">identity</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>The identity link function given by</p>
<p><span><span class="MathJax_Preview"> y = x </span><script type="math/tex; mode=display"> y = x </script></span></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L5-L11" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def identity(x):
    &#34;&#34;&#34;
    The identity link function given by

    $$ y = x $$
    &#34;&#34;&#34;
    return x</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.likelihood.inv_probit"><code class="name flex">
<span>def <span class="ident">inv_probit</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>The inverse probit link function given by</p>
<p><span><span class="MathJax_Preview"> y = \frac{1}{2} \left(1 + \operatorname{erf}(x/\sqrt{2})\right) </span><script type="math/tex; mode=display"> y = \frac{1}{2} \left(1 + \operatorname{erf}(x/\sqrt{2})\right) </script></span></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L29-L36" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def inv_probit(x):
    &#34;&#34;&#34;
    The inverse probit link function given by

    $$ y = \\frac{1}{2} \\left(1 + \\operatorname{erf}(x/\\sqrt{2})\\right) $$
    &#34;&#34;&#34;
    jitter = 1e-3
    return 0.5*(1.0+torch.erf(x/np.sqrt(2.0))) * (1.0-2.0*jitter) + jitter</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.likelihood.logistic"><code class="name flex">
<span>def <span class="ident">logistic</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>The logistic, inverse logit, or sigmoid link function given by</p>
<p><span><span class="MathJax_Preview"> y = \frac{1}{1 + e^{-x}} </span><script type="math/tex; mode=display"> y = \frac{1}{1 + e^{-x}} </script></span></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L39-L45" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def logistic(x):
    &#34;&#34;&#34;
    The logistic, inverse logit, or sigmoid link function given by

    $$ y = \\frac{1}{1 + e^{-x}} $$
    &#34;&#34;&#34;
    return 1.0/(1.0+torch.exp(-x))</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.likelihood.square"><code class="name flex">
<span>def <span class="ident">square</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>The square link function given by</p>
<p><span><span class="MathJax_Preview"> y = x^2 </span><script type="math/tex; mode=display"> y = x^2 </script></span></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L13-L19" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def square(x):
    &#34;&#34;&#34;
    The square link function given by

    $$ y = x^2 $$
    &#34;&#34;&#34;
    return torch.square(x)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="mogptk.gpr.likelihood.BernoulliLikelihood"><code class="flex name class">
<span>class <span class="ident">BernoulliLikelihood</span></span>
<span>(</span><span>link=&lt;function inv_probit&gt;, quadratures=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Bernoulli likelihood given by</p>
<p><span><span class="MathJax_Preview"> p(y|f) = h(f)^k (1-h(f))^{n-k} </span><script type="math/tex; mode=display"> p(y|f) = h(f)^k (1-h(f))^{n-k} </script></span></p>
<p>with <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> the link function, <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> the number of <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> values equal to 1, <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> the number of data points, and <span><span class="MathJax_Preview">y \in \{0.0,1.0\}</span><script type="math/tex">y \in \{0.0,1.0\}</script></span>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>link</code></strong> :&ensp;<code>function</code></dt>
<dd>Link function to map function values to the support of the likelihood.</dd>
<dt><strong><code>quadratures</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of quadrature points to use when approximating using Gauss-Hermite quadratures.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L440-L480" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class BernoulliLikelihood(Likelihood):
    &#34;&#34;&#34;
    Bernoulli likelihood given by

    $$ p(y|f) = h(f)^k (1-h(f))^{n-k} $$

    with \\(h\\) the link function, \\(k\\) the number of \\(y\\) values equal to 1, \\(n\\) the number of data points, and \\(y \\in \\{0.0,1.0\\}\\).

    Args:
        link (function): Link function to map function values to the support of the likelihood.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.
    &#34;&#34;&#34;
    def __init__(self, link=inv_probit, quadratures=20):
        super().__init__(quadratures)

        self.link = link

    def validate_y(self, y, X=None):
        if torch.any((y != 0.0) &amp; (y != 1.0)):
            raise ValueError(&#34;y must have only 0.0 and 1.0 values&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        p = self.link(f)
        return torch.log(torch.where(0.5 &lt;= y, p, 1.0-p))  # NxM

    def mean(self, f, X=None):
        return self.link(f)

    def variance(self, f, X=None):
        return self.link(f) - self.link(f).square()

    def predict(self, mu, var, X=None, full=False):
        if self.link == inv_probit:
            p = self.link(mu / torch.sqrt(1.0 + var))
            if full:
                return p.diagonal().reshape(-1,1), p-p.square() # TODO: correct?
            return p, p-p.square()
        else:
            return super().predict(mu, var, X=X, full=full)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.likelihood.Likelihood.log_prob" href="#mogptk.gpr.likelihood.Likelihood.log_prob">log_prob</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.mean" href="#mogptk.gpr.likelihood.Likelihood.mean">mean</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.predict" href="#mogptk.gpr.likelihood.Likelihood.predict">predict</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.validate_y" href="#mogptk.gpr.likelihood.Likelihood.validate_y">validate_y</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variance" href="#mogptk.gpr.likelihood.Likelihood.variance">variance</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variational_expectation" href="#mogptk.gpr.likelihood.Likelihood.variational_expectation">variational_expectation</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.likelihood.BetaLikelihood"><code class="flex name class">
<span>class <span class="ident">BetaLikelihood</span></span>
<span>(</span><span>scale=1.0, link=&lt;function inv_probit&gt;, quadratures=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Beta likelihood given by</p>
<p><span><span class="MathJax_Preview"> p(y|f) = \frac{\Gamma(\sigma)}{\Gamma\left(h(f)\sigma\right)\Gamma\left((1-h(f)\sigma\right)} y^{h(f)\sigma} (1-y)^{(1-h(f))\sigma} </span><script type="math/tex; mode=display"> p(y|f) = \frac{\Gamma(\sigma)}{\Gamma\left(h(f)\sigma\right)\Gamma\left((1-h(f)\sigma\right)} y^{h(f)\sigma} (1-y)^{(1-h(f))\sigma} </script></span></p>
<p>with <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> the link function, <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> the scale, and <span><span class="MathJax_Preview">y \in (0.0,1.0)</span><script type="math/tex">y \in (0.0,1.0)</script></span>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>scale</code></strong> :&ensp;<code>float</code></dt>
<dd>Scale.</dd>
<dt><strong><code>link</code></strong> :&ensp;<code>function</code></dt>
<dd>Link function to map function values to the support of the likelihood.</dd>
<dt><strong><code>quadratures</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of quadrature points to use when approximating using Gauss-Hermite quadratures.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>scale</code></strong> :&ensp;<code><a title="mogptk.gpr.parameter.Parameter" href="parameter.html#mogptk.gpr.parameter.Parameter">Parameter</a></code></dt>
<dd>Scale <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L482-L527" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class BetaLikelihood(Likelihood):
    &#34;&#34;&#34;
    Beta likelihood given by

    $$ p(y|f) = \\frac{\\Gamma(\\sigma)}{\\Gamma\\left(h(f)\\sigma\\right)\\Gamma\\left((1-h(f)\\sigma\\right)} y^{h(f)\\sigma} (1-y)^{(1-h(f))\\sigma} $$

    with \\(h\\) the link function, \\(\\sigma\\) the scale, and \\(y \\in (0.0,1.0)\\).

    Args:
        scale (float): Scale.
        link (function): Link function to map function values to the support of the likelihood.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.

    Attributes:
        scale (mogptk.gpr.parameter.Parameter): Scale \\(\\sigma\\).
    &#34;&#34;&#34;
    def __init__(self, scale=1.0, link=inv_probit, quadratures=20):
        super().__init__(quadratures)

        self.link = link
        self.scale = Parameter(scale, lower=config.positive_minimum)

    def validate_y(self, y, X=None):
        if torch.any((y &lt;= 0.0) | (1.0 &lt;= y)):
            raise ValueError(&#34;y must be in the range (0.0,1.0)&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        mixture = self.link(f)
        alpha = mixture * self.scale()
        beta = (1.0-mixture) * self.scale()

        p = (alpha-1.0)*y.log()
        p += (beta-1.0)*torch.log1p(-y)
        p += torch.lgamma(alpha+beta)
        p -= torch.lgamma(alpha)
        p -= torch.lgamma(beta)
        return p  # NxM

    def mean(self, f, X=None):
        return self.link(f)

    def variance(self, f, X=None):
        mixture = self.link(f)
        return (mixture - mixture.square()) / (self.scale() + 1.0)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.likelihood.Likelihood.log_prob" href="#mogptk.gpr.likelihood.Likelihood.log_prob">log_prob</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.mean" href="#mogptk.gpr.likelihood.Likelihood.mean">mean</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.predict" href="#mogptk.gpr.likelihood.Likelihood.predict">predict</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.validate_y" href="#mogptk.gpr.likelihood.Likelihood.validate_y">validate_y</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variance" href="#mogptk.gpr.likelihood.Likelihood.variance">variance</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variational_expectation" href="#mogptk.gpr.likelihood.Likelihood.variational_expectation">variational_expectation</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.likelihood.ChiSquaredLikelihood"><code class="flex name class">
<span>class <span class="ident">ChiSquaredLikelihood</span></span>
<span>(</span><span>quadratures=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Chi-squared likelihood given by</p>
<p><span><span class="MathJax_Preview"> p(y|f) = \frac{1}{2^{f/2}\Gamma(f/2)} y^{f/2-1} e^{-y/2} </span><script type="math/tex; mode=display"> p(y|f) = \frac{1}{2^{f/2}\Gamma(f/2)} y^{f/2-1} e^{-y/2} </script></span></p>
<p>with <span><span class="MathJax_Preview">y \in (0.0,\infty)</span><script type="math/tex">y \in (0.0,\infty)</script></span>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>quadratures</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of quadrature points to use when approximating using Gauss-Hermite quadratures.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L744-L772" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ChiSquaredLikelihood(Likelihood):
    &#34;&#34;&#34;
    Chi-squared likelihood given by

    $$ p(y|f) = \\frac{1}{2^{f/2}\\Gamma(f/2)} y^{f/2-1} e^{-y/2} $$

    with \\(y \\in (0.0,\\infty)\\).

    Args:
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.
    &#34;&#34;&#34;
    def __init__(self, quadratures=20):
        super().__init__(quadratures)

    def validate_y(self, y, X=None):
        if torch.any(y &lt;= 0.0):
            raise ValueError(&#34;y must be in the range (0.0,inf)&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        p = -0.5*f*np.log(2.0) - torch.lgamma(f/2.0) + (f/2.0-1.0)*y.log() - 0.5*y
        return p  # NxM

    def mean(self, f, X=None):
        return f

    def variance(self, f, X=None):
        return 2.0*f</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.likelihood.Likelihood.log_prob" href="#mogptk.gpr.likelihood.Likelihood.log_prob">log_prob</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.mean" href="#mogptk.gpr.likelihood.Likelihood.mean">mean</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.predict" href="#mogptk.gpr.likelihood.Likelihood.predict">predict</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.validate_y" href="#mogptk.gpr.likelihood.Likelihood.validate_y">validate_y</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variance" href="#mogptk.gpr.likelihood.Likelihood.variance">variance</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variational_expectation" href="#mogptk.gpr.likelihood.Likelihood.variational_expectation">variational_expectation</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.likelihood.ExponentialLikelihood"><code class="flex name class">
<span>class <span class="ident">ExponentialLikelihood</span></span>
<span>(</span><span>link=&lt;function exp&gt;, quadratures=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Exponential likelihood given by</p>
<p><span><span class="MathJax_Preview"> p(y|f) = h(f) e^{-h(f)y} </span><script type="math/tex; mode=display"> p(y|f) = h(f) e^{-h(f)y} </script></span></p>
<p>with <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> the link function and <span><span class="MathJax_Preview">y \in [0.0,\infty)</span><script type="math/tex">y \in [0.0,\infty)</script></span>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>link</code></strong> :&ensp;<code>function</code></dt>
<dd>Link function to map function values to the support of the likelihood.</dd>
<dt><strong><code>quadratures</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of quadrature points to use when approximating using Gauss-Hermite quadratures.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L372-L406" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ExponentialLikelihood(Likelihood):
    &#34;&#34;&#34;
    Exponential likelihood given by

    $$ p(y|f) = h(f) e^{-h(f)y} $$

    with \\(h\\) the link function and \\(y \\in [0.0,\\infty)\\).

    Args:
        link (function): Link function to map function values to the support of the likelihood.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.
    &#34;&#34;&#34;
    def __init__(self, link=exp, quadratures=20):
        super().__init__(quadratures)

        self.link = link

    def validate_y(self, y, X=None):
        if torch.any(y &lt; 0.0):
            raise ValueError(&#34;y must be positive&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        if self.link == exp:
            p = -y/self.link(f) - f
        else:
            p = -y/self.link(f) - self.link(f).log()
        return p  # NxM

    def mean(self, f, X=None):
        return self.link(f)

    def variance(self, f, X=None):
        return self.link(f).square()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.likelihood.Likelihood.log_prob" href="#mogptk.gpr.likelihood.Likelihood.log_prob">log_prob</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.mean" href="#mogptk.gpr.likelihood.Likelihood.mean">mean</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.predict" href="#mogptk.gpr.likelihood.Likelihood.predict">predict</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.validate_y" href="#mogptk.gpr.likelihood.Likelihood.validate_y">validate_y</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variance" href="#mogptk.gpr.likelihood.Likelihood.variance">variance</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variational_expectation" href="#mogptk.gpr.likelihood.Likelihood.variational_expectation">variational_expectation</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.likelihood.GammaLikelihood"><code class="flex name class">
<span>class <span class="ident">GammaLikelihood</span></span>
<span>(</span><span>shape=1.0, link=&lt;function exp&gt;, quadratures=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Gamma likelihood given by</p>
<p><span><span class="MathJax_Preview"> p(y|f) = \frac{1}{\Gamma(k)h(f)^k} y^{k-1} e^{-y/h(f)} </span><script type="math/tex; mode=display"> p(y|f) = \frac{1}{\Gamma(k)h(f)^k} y^{k-1} e^{-y/h(f)} </script></span></p>
<p>with <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> the link function, <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> the shape, and <span><span class="MathJax_Preview">y \in (0.0,\infty)</span><script type="math/tex">y \in (0.0,\infty)</script></span>. </p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>float</code></dt>
<dd>Shape.</dd>
<dt><strong><code>link</code></strong> :&ensp;<code>function</code></dt>
<dd>Link function to map function values to the support of the likelihood.</dd>
<dt><strong><code>quadratures</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of quadrature points to use when approximating using Gauss-Hermite quadratures.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code><a title="mogptk.gpr.parameter.Parameter" href="parameter.html#mogptk.gpr.parameter.Parameter">Parameter</a></code></dt>
<dd>Shape <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L529-L571" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class GammaLikelihood(Likelihood):
    &#34;&#34;&#34;
    Gamma likelihood given by

    $$ p(y|f) = \\frac{1}{\\Gamma(k)h(f)^k} y^{k-1} e^{-y/h(f)} $$

    with \\(h\\) the link function, \\(k\\) the shape, and \\(y \\in (0.0,\\infty)\\). 

    Args:
        shape (float): Shape.
        link (function): Link function to map function values to the support of the likelihood.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.

    Attributes:
        shape (mogptk.gpr.parameter.Parameter): Shape \\(k\\).
    &#34;&#34;&#34;
    def __init__(self, shape=1.0, link=exp, quadratures=20):
        super().__init__(quadratures)

        self.link = link
        self.shape = Parameter(shape, lower=config.positive_minimum)

    def validate_y(self, y, X=None):
        if torch.any(y &lt;= 0.0):
            raise ValueError(&#34;y must be in the range (0.0,inf)&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        p = -y/self.link(f)
        p += (self.shape()-1.0)*y.log()
        p -= torch.lgamma(self.shape())
        if self.link == exp:
            p -= self.shape()*f
        else:
            p -= self.shape()*self.link(f).log()
        return p  # NxM

    def mean(self, f, X=None):
        return self.shape()*self.link(f)

    def variance(self, f, X=None):
        return self.shape()*self.link(f).square()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.likelihood.Likelihood.log_prob" href="#mogptk.gpr.likelihood.Likelihood.log_prob">log_prob</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.mean" href="#mogptk.gpr.likelihood.Likelihood.mean">mean</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.predict" href="#mogptk.gpr.likelihood.Likelihood.predict">predict</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.validate_y" href="#mogptk.gpr.likelihood.Likelihood.validate_y">validate_y</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variance" href="#mogptk.gpr.likelihood.Likelihood.variance">variance</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variational_expectation" href="#mogptk.gpr.likelihood.Likelihood.variational_expectation">variational_expectation</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.likelihood.GaussHermiteQuadrature"><code class="flex name class">
<span>class <span class="ident">GaussHermiteQuadrature</span></span>
<span>(</span><span>deg=20, t_scale=None, w_scale=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L47-L61" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class GaussHermiteQuadrature:
    def __init__(self, deg=20, t_scale=None, w_scale=None):
        t, w = np.polynomial.hermite.hermgauss(deg)
        t = t.reshape(-1,1)
        w = w.reshape(-1,1)
        if t_scale is not None:
            t *= t_scale
        if w_scale is not None:
            w *= w_scale
        self.t = torch.tensor(t, device=config.device, dtype=config.dtype)  # degx1
        self.w = torch.tensor(w, device=config.device, dtype=config.dtype)  # degx1
        self.deg = deg

    def __call__(self, mu, var, F):
        return F(mu + var.sqrt().mm(self.t.T)).mm(self.w)  # Nx1</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.likelihood.GaussianLikelihood"><code class="flex name class">
<span>class <span class="ident">GaussianLikelihood</span></span>
<span>(</span><span>scale=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Gaussian likelihood given by</p>
<p><span><span class="MathJax_Preview"> p(y|f) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y-f)^2}{2\sigma^2}} </span><script type="math/tex; mode=display"> p(y|f) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y-f)^2}{2\sigma^2}} </script></span></p>
<p>with <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> the scale.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>scale</code></strong> :&ensp;<code>float</code></dt>
<dd>Scale.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>scale</code></strong> :&ensp;<code><a title="mogptk.gpr.parameter.Parameter" href="parameter.html#mogptk.gpr.parameter.Parameter">Parameter</a></code></dt>
<dd>Scale <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L287-L329" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class GaussianLikelihood(Likelihood):
    &#34;&#34;&#34;
    Gaussian likelihood given by

    $$ p(y|f) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(y-f)^2}{2\\sigma^2}} $$

    with \\(\\sigma\\) the scale.

    Args:
        scale (float): Scale.

    Attributes:
        scale (mogptk.gpr.parameter.Parameter): Scale \\(\\sigma\\).
    &#34;&#34;&#34;
    def __init__(self, scale=1.0):
        super().__init__()

        self.scale = Parameter(scale, lower=config.positive_minimum)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        p = -0.5 * (np.log(2.0*np.pi) + 2.0*self.scale().log() + ((y-f)/self.scale()).square())
        return p  # NxM

    def variational_expectation(self, y, mu, var, X=None):
        # y,mu,var: Nx1
        p = -((y-mu).square() + var) / self.scale().square()
        p -= np.log(2.0 * np.pi)
        p -= 2.0*self.scale().log()
        return 0.5*p.sum()  # sum over N

    def mean(self, f, X=None):
        return f

    def variance(self, f, X=None):
        return self.scale().square()

    def predict(self, mu, var, X=None, full=False):
        if full:
            return mu, var + self.scale().square()*torch.eye(var.shape[0])
        else:
            return mu, var + self.scale().square()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.likelihood.Likelihood.log_prob" href="#mogptk.gpr.likelihood.Likelihood.log_prob">log_prob</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.mean" href="#mogptk.gpr.likelihood.Likelihood.mean">mean</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.predict" href="#mogptk.gpr.likelihood.Likelihood.predict">predict</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.validate_y" href="#mogptk.gpr.likelihood.Likelihood.validate_y">validate_y</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variance" href="#mogptk.gpr.likelihood.Likelihood.variance">variance</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variational_expectation" href="#mogptk.gpr.likelihood.Likelihood.variational_expectation">variational_expectation</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.likelihood.LaplaceLikelihood"><code class="flex name class">
<span>class <span class="ident">LaplaceLikelihood</span></span>
<span>(</span><span>scale=1.0, quadratures=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Laplace likelihood given by</p>
<p><span><span class="MathJax_Preview"> p(y|f) = \frac{1}{2\sigma}e^{-\frac{|y-f|}{\sigma}} </span><script type="math/tex; mode=display"> p(y|f) = \frac{1}{2\sigma}e^{-\frac{|y-f|}{\sigma}} </script></span></p>
<p>with <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> the scale.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>scale</code></strong> :&ensp;<code>float</code></dt>
<dd>Scale.</dd>
<dt><strong><code>quadratures</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of quadrature points to use when approximating using Gauss-Hermite quadratures.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>scale</code></strong> :&ensp;<code><a title="mogptk.gpr.parameter.Parameter" href="parameter.html#mogptk.gpr.parameter.Parameter">Parameter</a></code></dt>
<dd>Scale <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L408-L438" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class LaplaceLikelihood(Likelihood):
    &#34;&#34;&#34;
    Laplace likelihood given by

    $$ p(y|f) = \\frac{1}{2\\sigma}e^{-\\frac{|y-f|}{\\sigma}} $$

    with \\(\\sigma\\) the scale.

    Args:
        scale (float): Scale.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.

    Attributes:
        scale (mogptk.gpr.parameter.Parameter): Scale \\(\\sigma\\).
    &#34;&#34;&#34;
    def __init__(self, scale=1.0, quadratures=20):
        super().__init__(quadratures)

        self.scale = Parameter(scale, lower=config.positive_minimum)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        p = -torch.log(2.0*self.scale()) - (y-f).abs()/self.scale()
        return p  # NxM

    def mean(self, f, X=None):
        return f

    def variance(self, f, X=None):
        return 2.0 * self.scale().square()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.likelihood.Likelihood.log_prob" href="#mogptk.gpr.likelihood.Likelihood.log_prob">log_prob</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.mean" href="#mogptk.gpr.likelihood.Likelihood.mean">mean</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.predict" href="#mogptk.gpr.likelihood.Likelihood.predict">predict</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.validate_y" href="#mogptk.gpr.likelihood.Likelihood.validate_y">validate_y</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variance" href="#mogptk.gpr.likelihood.Likelihood.variance">variance</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variational_expectation" href="#mogptk.gpr.likelihood.Likelihood.variational_expectation">variational_expectation</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.likelihood.Likelihood"><code class="flex name class">
<span>class <span class="ident">Likelihood</span></span>
<span>(</span><span>quadratures=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Base likelihood.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>quadratures</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of quadrature points to use when approximating using Gauss-Hermite quadratures.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L63-L177" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Likelihood(torch.nn.Module):
    &#34;&#34;&#34;
    Base likelihood.

    Args:
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.
    &#34;&#34;&#34;
    def __init__(self, quadratures=20):
        super().__init__()

        self.quadrature = GaussHermiteQuadrature(deg=quadratures, t_scale=np.sqrt(2), w_scale=1.0/np.sqrt(np.pi))
        self.output_dims = None

    def __setattr__(self, name, val):
        if name == &#39;train&#39;:
            for p in self.parameters():
                p.train = val
            return
        if hasattr(self, name) and isinstance(getattr(self, name), Parameter):
            raise AttributeError(&#34;parameter is read-only, use Parameter.assign()&#34;)
        if isinstance(val, Parameter) and val._name is None:
            val._name = &#39;likelihood.&#39; + name
        super().__setattr__(name, val)

    def validate_y(self, y, X=None):
        &#34;&#34;&#34;
        Validate whether the y input is within the likelihood&#39;s support.
        &#34;&#34;&#34;
        pass

    def log_prob(self, y, f, X=None):
        &#34;&#34;&#34;
        Calculate the log probability

        $$ \\log(p(y|f)) $$

        with \\(p(y|f)\\) our likelihood.
        &#34;&#34;&#34;
        # y: Nx1
        # f: NxM
        raise NotImplementedError()

    def variational_expectation(self, y, mu, var, X=None):
        &#34;&#34;&#34;
        Calculate the variational expectation

        $$ \\int \\log(p(y|f)) \\; q(f) \\; df $$

        where \\(q(f) \\sim \\mathcal{N}(\\mu,\\Sigma)\\) and \\(p(y|f)\\) our likelihood. By default this uses Gauss-Hermite quadratures to approximate the integral.

        Args:
            y (torch.tensor): Values for y of shape (data_points,).
            mu (torch.tensor): Mean of the posterior \\(q(f)\\) of shape (data_points,).
            var (torch.tensor): Variance of the posterior \\(q(f)\\) of shape (data_points,).
            X (torch.tensor): Input of shape (data_points,input_dims) needed for multi-output likelihood.
        &#34;&#34;&#34;
        # y,mu,var: Nx1
        q = self.quadrature(mu, var, lambda f: self.log_prob(y,f,X=X))  # Nx1
        return q.sum()  # sum over N

    def mean(self, f, X=None):
        &#34;&#34;&#34;
        Calculate the mean of the likelihood.

        Args:
            f (torch.tensor): Posterior values for f of shape (data_points,input_dims).
            X (torch.tensor): Input of shape (data_points,input_dims) needed for multi-output likelihood.

        Returns:
            torch.tensor: Mean of the predictive posterior \\(p(y|f)\\) of shape (data_points,).
        &#34;&#34;&#34;
        # f: NxM
        raise NotImplementedError()

    def variance(self, f, X=None):
        &#34;&#34;&#34;
        Calculate the variance of the likelihood.

        Args:
            f (torch.tensor): Posterior values for f of shape (data_points,input_dims).
            X (torch.tensor): Input of shape (data_points,input_dims) needed for multi-output likelihood.

        Returns:
            torch.tensor: Variance of the predictive posterior \\(p(y|f)\\) of shape (data_points,).
        &#34;&#34;&#34;
        # f: NxM
        raise NotImplementedError()

    def predict(self, mu, var, X=None, full=False):
        &#34;&#34;&#34;
        Calculate the mean and variance of the predictive distribution

        $$ \\mu = \\iint y \\; p(y|f) \\; q(f) \\; df dy $$
        $$ \\Sigma = \\iint y^2 \\; p(y|f) \\; q(f) \\; df dy - \\mu^2 $$

        where \\(q(f) \\sim \\mathcal{N}(\\mu,\\Sigma)\\) and \\(p(y|f)\\) our likelihood. By default this uses Gauss-Hermite quadratures to approximate both integrals.

        Args:
            mu (torch.tensor): Mean of the posterior \\(q(f)\\) of shape (data_points,).
            var (torch.tensor): Variance of the posterior \\(q(f)\\) of shape (data_points,).
            X (torch.tensor): Input of shape (data_points,input_dims) needed for multi-output likelihood.
            full (boolean): Return the full covariance matrix.

        Returns:
            torch.tensor: Mean of the predictive posterior \\(p(y|f)\\) of shape (data_points,).
            torch.tensor: Variance of the predictive posterior \\(p(y|f)\\) of shape (data_points,) or (data_points,data_points).
        &#34;&#34;&#34;
        # mu,var: Nx1
        # TODO: full covariance matrix for likelihood predictions
        if full:
            raise NotImplementedError(&#34;full covariance not support for likelihood predictions&#34;)

        Ey = self.quadrature(mu, var, lambda f: self.mean(f,X=X))
        Eyy = self.quadrature(mu, var, lambda f: self.mean(f,X=X).square() + self.variance(f,X=X))
        return Ey, Eyy-Ey**2</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.likelihood.BernoulliLikelihood" href="#mogptk.gpr.likelihood.BernoulliLikelihood">BernoulliLikelihood</a></li>
<li><a title="mogptk.gpr.likelihood.BetaLikelihood" href="#mogptk.gpr.likelihood.BetaLikelihood">BetaLikelihood</a></li>
<li><a title="mogptk.gpr.likelihood.ChiSquaredLikelihood" href="#mogptk.gpr.likelihood.ChiSquaredLikelihood">ChiSquaredLikelihood</a></li>
<li><a title="mogptk.gpr.likelihood.ExponentialLikelihood" href="#mogptk.gpr.likelihood.ExponentialLikelihood">ExponentialLikelihood</a></li>
<li><a title="mogptk.gpr.likelihood.GammaLikelihood" href="#mogptk.gpr.likelihood.GammaLikelihood">GammaLikelihood</a></li>
<li><a title="mogptk.gpr.likelihood.GaussianLikelihood" href="#mogptk.gpr.likelihood.GaussianLikelihood">GaussianLikelihood</a></li>
<li><a title="mogptk.gpr.likelihood.LaplaceLikelihood" href="#mogptk.gpr.likelihood.LaplaceLikelihood">LaplaceLikelihood</a></li>
<li><a title="mogptk.gpr.likelihood.LogGaussianLikelihood" href="#mogptk.gpr.likelihood.LogGaussianLikelihood">LogGaussianLikelihood</a></li>
<li><a title="mogptk.gpr.likelihood.LogLogisticLikelihood" href="#mogptk.gpr.likelihood.LogLogisticLikelihood">LogLogisticLikelihood</a></li>
<li><a title="mogptk.gpr.likelihood.MultiOutputLikelihood" href="#mogptk.gpr.likelihood.MultiOutputLikelihood">MultiOutputLikelihood</a></li>
<li><a title="mogptk.gpr.likelihood.PoissonLikelihood" href="#mogptk.gpr.likelihood.PoissonLikelihood">PoissonLikelihood</a></li>
<li><a title="mogptk.gpr.likelihood.StudentTLikelihood" href="#mogptk.gpr.likelihood.StudentTLikelihood">StudentTLikelihood</a></li>
<li><a title="mogptk.gpr.likelihood.WeibullLikelihood" href="#mogptk.gpr.likelihood.WeibullLikelihood">WeibullLikelihood</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mogptk.gpr.likelihood.Likelihood.log_prob"><code class="name flex">
<span>def <span class="ident">log_prob</span></span>(<span>self, y, f, X=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the log probability</p>
<p><span><span class="MathJax_Preview"> \log(p(y|f)) </span><script type="math/tex; mode=display"> \log(p(y|f)) </script></span></p>
<p>with <span><span class="MathJax_Preview">p(y|f)</span><script type="math/tex">p(y|f)</script></span> our likelihood.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L93-L103" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def log_prob(self, y, f, X=None):
    &#34;&#34;&#34;
    Calculate the log probability

    $$ \\log(p(y|f)) $$

    with \\(p(y|f)\\) our likelihood.
    &#34;&#34;&#34;
    # y: Nx1
    # f: NxM
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.likelihood.Likelihood.mean"><code class="name flex">
<span>def <span class="ident">mean</span></span>(<span>self, f, X=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the mean of the likelihood.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Posterior values for f of shape (data_points,input_dims).</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Input of shape (data_points,input_dims) needed for multi-output likelihood.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.tensor</code></dt>
<dd>Mean of the predictive posterior <span><span class="MathJax_Preview">p(y|f)</span><script type="math/tex">p(y|f)</script></span> of shape (data_points,).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L123-L135" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def mean(self, f, X=None):
    &#34;&#34;&#34;
    Calculate the mean of the likelihood.

    Args:
        f (torch.tensor): Posterior values for f of shape (data_points,input_dims).
        X (torch.tensor): Input of shape (data_points,input_dims) needed for multi-output likelihood.

    Returns:
        torch.tensor: Mean of the predictive posterior \\(p(y|f)\\) of shape (data_points,).
    &#34;&#34;&#34;
    # f: NxM
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.likelihood.Likelihood.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, mu, var, X=None, full=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the mean and variance of the predictive distribution</p>
<p><span><span class="MathJax_Preview"> \mu = \iint y \; p(y|f) \; q(f) \; df dy </span><script type="math/tex; mode=display"> \mu = \iint y \; p(y|f) \; q(f) \; df dy </script></span>
<span><span class="MathJax_Preview"> \Sigma = \iint y^2 \; p(y|f) \; q(f) \; df dy - \mu^2 </span><script type="math/tex; mode=display"> \Sigma = \iint y^2 \; p(y|f) \; q(f) \; df dy - \mu^2 </script></span></p>
<p>where <span><span class="MathJax_Preview">q(f) \sim \mathcal{N}(\mu,\Sigma)</span><script type="math/tex">q(f) \sim \mathcal{N}(\mu,\Sigma)</script></span> and <span><span class="MathJax_Preview">p(y|f)</span><script type="math/tex">p(y|f)</script></span> our likelihood. By default this uses Gauss-Hermite quadratures to approximate both integrals.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mu</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Mean of the posterior <span><span class="MathJax_Preview">q(f)</span><script type="math/tex">q(f)</script></span> of shape (data_points,).</dd>
<dt><strong><code>var</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Variance of the posterior <span><span class="MathJax_Preview">q(f)</span><script type="math/tex">q(f)</script></span> of shape (data_points,).</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Input of shape (data_points,input_dims) needed for multi-output likelihood.</dd>
<dt><strong><code>full</code></strong> :&ensp;<code>boolean</code></dt>
<dd>Return the full covariance matrix.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.tensor</code></dt>
<dd>Mean of the predictive posterior <span><span class="MathJax_Preview">p(y|f)</span><script type="math/tex">p(y|f)</script></span> of shape (data_points,).</dd>
<dt><code>torch.tensor</code></dt>
<dd>Variance of the predictive posterior <span><span class="MathJax_Preview">p(y|f)</span><script type="math/tex">p(y|f)</script></span> of shape (data_points,) or (data_points,data_points).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L151-L177" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def predict(self, mu, var, X=None, full=False):
    &#34;&#34;&#34;
    Calculate the mean and variance of the predictive distribution

    $$ \\mu = \\iint y \\; p(y|f) \\; q(f) \\; df dy $$
    $$ \\Sigma = \\iint y^2 \\; p(y|f) \\; q(f) \\; df dy - \\mu^2 $$

    where \\(q(f) \\sim \\mathcal{N}(\\mu,\\Sigma)\\) and \\(p(y|f)\\) our likelihood. By default this uses Gauss-Hermite quadratures to approximate both integrals.

    Args:
        mu (torch.tensor): Mean of the posterior \\(q(f)\\) of shape (data_points,).
        var (torch.tensor): Variance of the posterior \\(q(f)\\) of shape (data_points,).
        X (torch.tensor): Input of shape (data_points,input_dims) needed for multi-output likelihood.
        full (boolean): Return the full covariance matrix.

    Returns:
        torch.tensor: Mean of the predictive posterior \\(p(y|f)\\) of shape (data_points,).
        torch.tensor: Variance of the predictive posterior \\(p(y|f)\\) of shape (data_points,) or (data_points,data_points).
    &#34;&#34;&#34;
    # mu,var: Nx1
    # TODO: full covariance matrix for likelihood predictions
    if full:
        raise NotImplementedError(&#34;full covariance not support for likelihood predictions&#34;)

    Ey = self.quadrature(mu, var, lambda f: self.mean(f,X=X))
    Eyy = self.quadrature(mu, var, lambda f: self.mean(f,X=X).square() + self.variance(f,X=X))
    return Ey, Eyy-Ey**2</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.likelihood.Likelihood.validate_y"><code class="name flex">
<span>def <span class="ident">validate_y</span></span>(<span>self, y, X=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Validate whether the y input is within the likelihood's support.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L87-L91" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def validate_y(self, y, X=None):
    &#34;&#34;&#34;
    Validate whether the y input is within the likelihood&#39;s support.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.likelihood.Likelihood.variance"><code class="name flex">
<span>def <span class="ident">variance</span></span>(<span>self, f, X=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the variance of the likelihood.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Posterior values for f of shape (data_points,input_dims).</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Input of shape (data_points,input_dims) needed for multi-output likelihood.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.tensor</code></dt>
<dd>Variance of the predictive posterior <span><span class="MathJax_Preview">p(y|f)</span><script type="math/tex">p(y|f)</script></span> of shape (data_points,).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L137-L149" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def variance(self, f, X=None):
    &#34;&#34;&#34;
    Calculate the variance of the likelihood.

    Args:
        f (torch.tensor): Posterior values for f of shape (data_points,input_dims).
        X (torch.tensor): Input of shape (data_points,input_dims) needed for multi-output likelihood.

    Returns:
        torch.tensor: Variance of the predictive posterior \\(p(y|f)\\) of shape (data_points,).
    &#34;&#34;&#34;
    # f: NxM
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.likelihood.Likelihood.variational_expectation"><code class="name flex">
<span>def <span class="ident">variational_expectation</span></span>(<span>self, y, mu, var, X=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the variational expectation</p>
<p><span><span class="MathJax_Preview"> \int \log(p(y|f)) \; q(f) \; df </span><script type="math/tex; mode=display"> \int \log(p(y|f)) \; q(f) \; df </script></span></p>
<p>where <span><span class="MathJax_Preview">q(f) \sim \mathcal{N}(\mu,\Sigma)</span><script type="math/tex">q(f) \sim \mathcal{N}(\mu,\Sigma)</script></span> and <span><span class="MathJax_Preview">p(y|f)</span><script type="math/tex">p(y|f)</script></span> our likelihood. By default this uses Gauss-Hermite quadratures to approximate the integral.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Values for y of shape (data_points,).</dd>
<dt><strong><code>mu</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Mean of the posterior <span><span class="MathJax_Preview">q(f)</span><script type="math/tex">q(f)</script></span> of shape (data_points,).</dd>
<dt><strong><code>var</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Variance of the posterior <span><span class="MathJax_Preview">q(f)</span><script type="math/tex">q(f)</script></span> of shape (data_points,).</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Input of shape (data_points,input_dims) needed for multi-output likelihood.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L105-L121" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def variational_expectation(self, y, mu, var, X=None):
    &#34;&#34;&#34;
    Calculate the variational expectation

    $$ \\int \\log(p(y|f)) \\; q(f) \\; df $$

    where \\(q(f) \\sim \\mathcal{N}(\\mu,\\Sigma)\\) and \\(p(y|f)\\) our likelihood. By default this uses Gauss-Hermite quadratures to approximate the integral.

    Args:
        y (torch.tensor): Values for y of shape (data_points,).
        mu (torch.tensor): Mean of the posterior \\(q(f)\\) of shape (data_points,).
        var (torch.tensor): Variance of the posterior \\(q(f)\\) of shape (data_points,).
        X (torch.tensor): Input of shape (data_points,input_dims) needed for multi-output likelihood.
    &#34;&#34;&#34;
    # y,mu,var: Nx1
    q = self.quadrature(mu, var, lambda f: self.log_prob(y,f,X=X))  # Nx1
    return q.sum()  # sum over N</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="mogptk.gpr.likelihood.LogGaussianLikelihood"><code class="flex name class">
<span>class <span class="ident">LogGaussianLikelihood</span></span>
<span>(</span><span>scale=1.0, quadratures=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Log-Gaussian likelihood given by</p>
<p><span><span class="MathJax_Preview"> p(y|f) = \frac{1}{y\sqrt{2\pi\sigma^2}} e^{-\frac{(\log(y) - f)^2}{2\sigma^2}} </span><script type="math/tex; mode=display"> p(y|f) = \frac{1}{y\sqrt{2\pi\sigma^2}} e^{-\frac{(\log(y) - f)^2}{2\sigma^2}} </script></span></p>
<p>with <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> the scale and <span><span class="MathJax_Preview">y \in (0.0,\infty)</span><script type="math/tex">y \in (0.0,\infty)</script></span>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>scale</code></strong> :&ensp;<code>float</code></dt>
<dd>Scale.</dd>
<dt><strong><code>quadratures</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of quadrature points to use when approximating using Gauss-Hermite quadratures.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>scale</code></strong> :&ensp;<code><a title="mogptk.gpr.parameter.Parameter" href="parameter.html#mogptk.gpr.parameter.Parameter">Parameter</a></code></dt>
<dd>Scale <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L706-L742" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class LogGaussianLikelihood(Likelihood):
    &#34;&#34;&#34;
    Log-Gaussian likelihood given by

    $$ p(y|f) = \\frac{1}{y\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(\\log(y) - f)^2}{2\\sigma^2}} $$

    with \\(\\sigma\\) the scale and \\(y \\in (0.0,\\infty)\\).

    Args:
        scale (float): Scale.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.

    Attributes:
        scale (mogptk.gpr.parameter.Parameter): Scale \\(\\sigma\\).
    &#34;&#34;&#34;
    def __init__(self, scale=1.0, quadratures=20):
        super().__init__(quadratures)

        self.scale = Parameter(scale, lower=config.positive_minimum)

    def validate_y(self, y, X=None):
        if torch.any(y &lt;= 0.0):
            raise ValueError(&#34;y must be in the range (0.0,inf)&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        logy = y.log()
        p = -0.5 * (np.log(2.0*np.pi) + 2.0*self.scale().log() + ((logy-f)/self.scale()).square())
        p -= logy
        return p  # NxM

    def mean(self, f, X=None):
        return torch.exp(f + 0.5*self.scale().square())

    def variance(self, f, X=None):
        return (self.scale().square().exp() - 1.0) * torch.exp(2.0*f + self.scale().square())</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.likelihood.Likelihood.log_prob" href="#mogptk.gpr.likelihood.Likelihood.log_prob">log_prob</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.mean" href="#mogptk.gpr.likelihood.Likelihood.mean">mean</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.predict" href="#mogptk.gpr.likelihood.Likelihood.predict">predict</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.validate_y" href="#mogptk.gpr.likelihood.Likelihood.validate_y">validate_y</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variance" href="#mogptk.gpr.likelihood.Likelihood.variance">variance</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variational_expectation" href="#mogptk.gpr.likelihood.Likelihood.variational_expectation">variational_expectation</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.likelihood.LogLogisticLikelihood"><code class="flex name class">
<span>class <span class="ident">LogLogisticLikelihood</span></span>
<span>(</span><span>shape=1.0, link=&lt;function exp&gt;, quadratures=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Log-logistic likelihood given by</p>
<p><span><span class="MathJax_Preview"> p(y|f) = \frac{(k/h(f)) (y/h(f))^{k-1}}{\left(1 + (y/h(f))^k\right)^2} </span><script type="math/tex; mode=display"> p(y|f) = \frac{(k/h(f)) (y/h(f))^{k-1}}{\left(1 + (y/h(f))^k\right)^2} </script></span></p>
<p>with <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> the link function, <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> the shape, and <span><span class="MathJax_Preview">y \in (0.0,\infty)</span><script type="math/tex">y \in (0.0,\infty)</script></span>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>float</code></dt>
<dd>Shape.</dd>
<dt><strong><code>link</code></strong> :&ensp;<code>function</code></dt>
<dd>Link function to map function values to the support of the likelihood.</dd>
<dt><strong><code>quadratures</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of quadrature points to use when approximating using Gauss-Hermite quadratures.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code><a title="mogptk.gpr.parameter.Parameter" href="parameter.html#mogptk.gpr.parameter.Parameter">Parameter</a></code></dt>
<dd>Shape <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L658-L704" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class LogLogisticLikelihood(Likelihood):
    &#34;&#34;&#34;
    Log-logistic likelihood given by

    $$ p(y|f) = \\frac{(k/h(f)) (y/h(f))^{k-1}}{\\left(1 + (y/h(f))^k\\right)^2} $$

    with \\(h\\) the link function, \\(k\\) the shape, and \\(y \\in (0.0,\\infty)\\).

    Args:
        shape (float): Shape.
        link (function): Link function to map function values to the support of the likelihood.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.

    Attributes:
        shape (mogptk.gpr.parameter.Parameter): Shape \\(k\\).
    &#34;&#34;&#34;
    def __init__(self, shape=1.0, link=exp, quadratures=20):
        super().__init__(quadratures)

        self.link = link
        self.shape = Parameter(shape, lower=config.positive_minimum)

    def validate_y(self, y, X=None):
        if torch.any(y &lt; 0.0):
            raise ValueError(&#34;y must be in the range [0.0,inf)&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        if self.link == exp:
            p = -self.shape()*f
        else:
            p = -self.shape()*self.link(f).log()
        p -= 2.0*torch.log1p((y/self.link(f))**self.shape())
        p += self.shape().log()
        p += (self.shape()-1.0)*y.log()
        return p  # NxM

    def mean(self, f, X=None):
        return self.link(f) / torch.sinc(1.0/self.shape())

    def variance(self, f, X=None):
        if self.shape() &lt;= 2.0:
            return np.nan
        a = 1.0/torch.sinc(2.0/self.shape())
        b = 1.0/torch.sinc(1.0/self.shape())
        return self.link(f).square() * (a - b.square())</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.likelihood.Likelihood.log_prob" href="#mogptk.gpr.likelihood.Likelihood.log_prob">log_prob</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.mean" href="#mogptk.gpr.likelihood.Likelihood.mean">mean</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.predict" href="#mogptk.gpr.likelihood.Likelihood.predict">predict</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.validate_y" href="#mogptk.gpr.likelihood.Likelihood.validate_y">validate_y</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variance" href="#mogptk.gpr.likelihood.Likelihood.variance">variance</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variational_expectation" href="#mogptk.gpr.likelihood.Likelihood.variational_expectation">variational_expectation</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.likelihood.MultiOutputLikelihood"><code class="flex name class">
<span>class <span class="ident">MultiOutputLikelihood</span></span>
<span>(</span><span>*likelihoods)</span>
</code></dt>
<dd>
<div class="desc"><p>Multi-output likelihood to assign a different likelihood per channel.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>likelihoods</code></strong> :&ensp;<code><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></code></dt>
<dd>List of likelihoods equal to the number of output dimensions.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L179-L285" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class MultiOutputLikelihood(Likelihood):
    &#34;&#34;&#34;
    Multi-output likelihood to assign a different likelihood per channel.

    Args:
        likelihoods (mogptk.gpr.likelihood.Likelihood): List of likelihoods equal to the number of output dimensions.
    &#34;&#34;&#34;
    def __init__(self, *likelihoods):
        super().__init__()

        if isinstance(likelihoods, tuple):
            if len(likelihoods) == 1 and isinstance(likelihoods[0], list):
                likelihoods = likelihoods[0]
            else:
                likelihoods = list(likelihoods)
        elif not isinstance(likelihoods, list):
            likelihoods = [likelihoods]
        if len(likelihoods) == 0:
            raise ValueError(&#34;must pass at least one likelihood&#34;)
        for i, likelihood in enumerate(likelihoods):
            if not issubclass(type(likelihood), Likelihood):
                raise ValueError(&#34;must pass likelihoods&#34;)
            elif isinstance(likelihood, MultiOutputLikelihood):
                raise ValueError(&#34;can not nest MultiOutputLikelihoods&#34;)

        self.output_dims = len(likelihoods)
        self.likelihoods = likelihoods

    def _channel_indices(self, X):
        c = X[:,0].long()
        m = [c==j for j in range(self.output_dims)]
        r = [torch.nonzero(m[j], as_tuple=False).reshape(-1) for j in range(self.output_dims)]  # as_tuple avoids warning
        return r

    def validate_y(self, y, X=None):
        if self.output_dims == 1:
            self.likelihoods[0].validate_y(y, X=X)
            return

        r = self._channel_indices(X)
        for i in range(self.output_dims):
            self.likelihoods[i].validate_y(y[r[i],:], X=X)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        if self.output_dims == 1:
            return self.likelihoods[0].log_prob(y,f)

        r = self._channel_indices(X)
        res = torch.empty(f.shape, device=config.device, dtype=config.dtype)
        for i in range(self.output_dims):
            res[r[i],:] = self.likelihoods[i].log_prob(y[r[i],:], f[r[i],:])
        return res  # NxM

    def variational_expectation(self, y, mu, var, X=None):
        # y,mu,var: Nx1
        if self.output_dims == 1:
            return self.likelihoods[0].variational_expectation(y,mu,var)

        q = torch.tensor(0.0, dtype=config.dtype, device=config.device)
        r = self._channel_indices(X)
        for i in range(self.output_dims):
            q += self.likelihoods[i].variational_expectation(y[r[i],:], mu[r[i],:], var[r[i],:]).sum()  # sum over N
        return q

    def mean(self, f, X=None):
        # f: NxM
        if self.output_dims == 1:
            return self.likelihoods[0].mean(f)

        r = self._channel_indices(X)
        res = torch.empty(f.shape, device=config.device, dtype=config.dtype)
        for i in range(self.output_dims):
            res[r[i],:] = self.likelihoods[i].mean(f[r[i],:])
        return res  # NxM

    def variance(self, f, X=None):
        # f: NxM
        if self.output_dims == 1:
            return self.likelihoods[0].variance(f)

        r = self._channel_indices(X)
        res = torch.empty(f.shape, device=config.device, dtype=config.dtype)
        for i in range(self.output_dims):
            res[r[i],:] = self.likelihoods[i].variance(f[r[i],:])
        return res  # NxM

    # TODO: predict is not possible?
    #def predict(self, mu, var, X=None, full=False):
    #    # mu: Nx1
    #    # var: Nx1 or NxN
    #    if self.output_dims == 1:
    #        return self.likelihoods[0].predict(mu,var,full=full)

    #    r = self._channel_indices(X)
    #    Ey = torch.empty(mu.shape, device=config.device, dtype=config.dtype)
    #    Eyy = torch.empty(var.shape, device=config.device, dtype=config.dtype)
    #    if full:
    #        for i in range(self.output_dims):
    #            r1 = r[i].reshape(-1,1)
    #            r2 = r[i].reshape(1,-1)
    #            Ey[r[i],:], Eyy[r1,r2] = self.likelihoods[i].predict(mu[r[i],:], var[r1,r2], full=True)
    #    else:
    #        for i in range(self.output_dims):
    #            Ey[r[i],:], Eyy[r[i],:] = self.likelihoods[i].predict(mu[r[i],:], var[r[i],:], full=False)
    #    return Ey, Eyy-Ey.square()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.likelihood.Likelihood.log_prob" href="#mogptk.gpr.likelihood.Likelihood.log_prob">log_prob</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.mean" href="#mogptk.gpr.likelihood.Likelihood.mean">mean</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.predict" href="#mogptk.gpr.likelihood.Likelihood.predict">predict</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.validate_y" href="#mogptk.gpr.likelihood.Likelihood.validate_y">validate_y</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variance" href="#mogptk.gpr.likelihood.Likelihood.variance">variance</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variational_expectation" href="#mogptk.gpr.likelihood.Likelihood.variational_expectation">variational_expectation</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.likelihood.PoissonLikelihood"><code class="flex name class">
<span>class <span class="ident">PoissonLikelihood</span></span>
<span>(</span><span>link=&lt;function exp&gt;, quadratures=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Poisson likelihood given by</p>
<p><span><span class="MathJax_Preview"> p(y|f) = \frac{1}{y!} h(f)^y e^{-h(f)} </span><script type="math/tex; mode=display"> p(y|f) = \frac{1}{y!} h(f)^y e^{-h(f)} </script></span></p>
<p>with <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> the link function and <span><span class="MathJax_Preview">y \in \mathbb{N}_0</span><script type="math/tex">y \in \mathbb{N}_0</script></span>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>link</code></strong> :&ensp;<code>function</code></dt>
<dd>Link function to map function values to the support of the likelihood.</dd>
<dt><strong><code>quadratures</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of quadrature points to use when approximating using Gauss-Hermite quadratures.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L573-L611" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class PoissonLikelihood(Likelihood):
    &#34;&#34;&#34;
    Poisson likelihood given by

    $$ p(y|f) = \\frac{1}{y!} h(f)^y e^{-h(f)} $$

    with \\(h\\) the link function and \\(y \\in \\mathbb{N}_0\\).

    Args:
        link (function): Link function to map function values to the support of the likelihood.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.
    &#34;&#34;&#34;
    def __init__(self, link=exp, quadratures=20):
        super().__init__(quadratures)

        self.link = link

    def validate_y(self, y, X=None):
        if torch.any(y &lt; 0.0):
            raise ValueError(&#34;y must be in the range [0.0,inf)&#34;)
        if not torch.all(y == y.long()):
            raise ValueError(&#34;y must have integer count values&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        if self.link == exp:
            p = y*f
        else:
            p = y*self.link(f).log()
        p -= torch.lgamma(y+1.0)
        p -= self.link(f)
        return p  # NxM

    def mean(self, f, X=None):
        return self.link(f)

    def variance(self, f, X=None):
        return self.link(f)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.likelihood.Likelihood.log_prob" href="#mogptk.gpr.likelihood.Likelihood.log_prob">log_prob</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.mean" href="#mogptk.gpr.likelihood.Likelihood.mean">mean</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.predict" href="#mogptk.gpr.likelihood.Likelihood.predict">predict</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.validate_y" href="#mogptk.gpr.likelihood.Likelihood.validate_y">validate_y</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variance" href="#mogptk.gpr.likelihood.Likelihood.variance">variance</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variational_expectation" href="#mogptk.gpr.likelihood.Likelihood.variational_expectation">variational_expectation</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.likelihood.StudentTLikelihood"><code class="flex name class">
<span>class <span class="ident">StudentTLikelihood</span></span>
<span>(</span><span>dof=3, scale=1.0, quadratures=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Student's t likelihood given by</p>
<p><span><span class="MathJax_Preview"> p(y|f) = \frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)\sqrt{\pi\nu}\sigma} \left( 1 + \frac{(y-f)^2}{\nu\sigma^2} \right)^{-(\nu+1)/2} </span><script type="math/tex; mode=display"> p(y|f) = \frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)\sqrt{\pi\nu}\sigma} \left( 1 + \frac{(y-f)^2}{\nu\sigma^2} \right)^{-(\nu+1)/2} </script></span></p>
<p>with <span><span class="MathJax_Preview">\nu</span><script type="math/tex">\nu</script></span> the degrees of freedom and <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> the scale.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dof</code></strong> :&ensp;<code>float</code></dt>
<dd>Degrees of freedom.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>float</code></dt>
<dd>Scale.</dd>
<dt><strong><code>quadratures</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of quadrature points to use when approximating using Gauss-Hermite quadratures.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>scale</code></strong> :&ensp;<code><a title="mogptk.gpr.parameter.Parameter" href="parameter.html#mogptk.gpr.parameter.Parameter">Parameter</a></code></dt>
<dd>Scale <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L331-L370" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class StudentTLikelihood(Likelihood):
    &#34;&#34;&#34;
    Student&#39;s t likelihood given by

    $$ p(y|f) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\pi\\nu}\\sigma} \\left( 1 + \\frac{(y-f)^2}{\\nu\\sigma^2} \\right)^{-(\\nu+1)/2} $$

    with \\(\\nu\\) the degrees of freedom and \\(\\sigma\\) the scale.

    Args:
        dof (float): Degrees of freedom.
        scale (float): Scale.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.

    Attributes:
        scale (mogptk.gpr.parameter.Parameter): Scale \\(\\sigma\\).
    &#34;&#34;&#34;
    def __init__(self, dof=3, scale=1.0, quadratures=20):
        super().__init__(quadratures)

        self.dof = torch.tensor(dof, device=config.device, dtype=config.dtype)
        self.scale = Parameter(scale, lower=config.positive_minimum)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        p = -0.5 * (self.dof+1.0)*torch.log1p(((y-f)/self.scale()).square()/self.dof)
        p += torch.lgamma((self.dof+1.0)/2.0)
        p -= torch.lgamma(self.dof/2.0)
        p -= 0.5 * torch.log(self.dof*np.pi*self.scale().square())
        return p  # NxM

    def mean(self, f, X=None):
        if self.dof &lt;= 1.0:
            return torch.full(f.shape, np.nan, device=config.device, dtype=config.dtype)
        return f

    def variance(self, f, X=None):
        if self.dof &lt;= 2.0:
            return np.nan
        return self.scale().square() * self.dof/(self.dof-2.0)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.likelihood.Likelihood.log_prob" href="#mogptk.gpr.likelihood.Likelihood.log_prob">log_prob</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.mean" href="#mogptk.gpr.likelihood.Likelihood.mean">mean</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.predict" href="#mogptk.gpr.likelihood.Likelihood.predict">predict</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.validate_y" href="#mogptk.gpr.likelihood.Likelihood.validate_y">validate_y</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variance" href="#mogptk.gpr.likelihood.Likelihood.variance">variance</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variational_expectation" href="#mogptk.gpr.likelihood.Likelihood.variational_expectation">variational_expectation</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.likelihood.WeibullLikelihood"><code class="flex name class">
<span>class <span class="ident">WeibullLikelihood</span></span>
<span>(</span><span>shape=1.0, link=&lt;function exp&gt;, quadratures=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Weibull likelihood given by</p>
<p><span><span class="MathJax_Preview"> p(y|f) = \frac{k}{h(f)} \left( \frac{y}{h(f)} \right)^{k-1} e^{-(y/h(f))^k} </span><script type="math/tex; mode=display"> p(y|f) = \frac{k}{h(f)} \left( \frac{y}{h(f)} \right)^{k-1} e^{-(y/h(f))^k} </script></span></p>
<p>with <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> the link function, <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> the shape, and <span><span class="MathJax_Preview">y \in (0.0,\infty)</span><script type="math/tex">y \in (0.0,\infty)</script></span>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>float</code></dt>
<dd>Shape.</dd>
<dt><strong><code>link</code></strong> :&ensp;<code>function</code></dt>
<dd>Link function to map function values to the support of the likelihood.</dd>
<dt><strong><code>quadratures</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of quadrature points to use when approximating using Gauss-Hermite quadratures.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code><a title="mogptk.gpr.parameter.Parameter" href="parameter.html#mogptk.gpr.parameter.Parameter">Parameter</a></code></dt>
<dd>Shape <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/29747a597f22321d735abf51eca6a7ef50c63567/mogptk/gpr/likelihood.py#L613-L656" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class WeibullLikelihood(Likelihood):
    &#34;&#34;&#34;
    Weibull likelihood given by

    $$ p(y|f) = \\frac{k}{h(f)} \\left( \\frac{y}{h(f)} \\right)^{k-1} e^{-(y/h(f))^k} $$

    with \\(h\\) the link function, \\(k\\) the shape, and \\(y \\in (0.0,\\infty)\\).

    Args:
        shape (float): Shape.
        link (function): Link function to map function values to the support of the likelihood.
        quadratures (int): Number of quadrature points to use when approximating using Gauss-Hermite quadratures.

    Attributes:
        shape (mogptk.gpr.parameter.Parameter): Shape \\(k\\).
    &#34;&#34;&#34;
    def __init__(self, shape=1.0, link=exp, quadratures=20):
        super().__init__(quadratures)

        self.link = link
        self.shape = Parameter(shape, lower=config.positive_minimum)

    def validate_y(self, y, X=None):
        if torch.any(y &lt;= 0.0):
            raise ValueError(&#34;y must be in the range (0.0,inf)&#34;)

    def log_prob(self, y, f, X=None):
        # y: Nx1
        # f: NxM
        if self.link == exp:
            p = -self.shape()*f
        else:
            p = -self.shape()*self.link(f).log()
        p += self.shape().log() + (self.shape()-1.0)*y.log()
        p -= (y/self.link(f))**self.shape()
        return p  # NxM

    def mean(self, f, X=None):
        return f * torch.lgamma(1.0 + 1.0/self.shape()).exp()

    def variance(self, f, X=None):
        a = torch.lgamma(1.0 + 2.0/self.shape()).exp()
        b = torch.lgamma(1.0 + 1.0/self.shape()).exp()
        return f.square() * (a - b.square())</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.likelihood.Likelihood.log_prob" href="#mogptk.gpr.likelihood.Likelihood.log_prob">log_prob</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.mean" href="#mogptk.gpr.likelihood.Likelihood.mean">mean</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.predict" href="#mogptk.gpr.likelihood.Likelihood.predict">predict</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.validate_y" href="#mogptk.gpr.likelihood.Likelihood.validate_y">validate_y</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variance" href="#mogptk.gpr.likelihood.Likelihood.variance">variance</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variational_expectation" href="#mogptk.gpr.likelihood.Likelihood.variational_expectation">variational_expectation</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="mogptk.gpr" href="index.html">mogptk.gpr</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="mogptk.gpr.likelihood.exp" href="#mogptk.gpr.likelihood.exp">exp</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.identity" href="#mogptk.gpr.likelihood.identity">identity</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.inv_probit" href="#mogptk.gpr.likelihood.inv_probit">inv_probit</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.logistic" href="#mogptk.gpr.likelihood.logistic">logistic</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.square" href="#mogptk.gpr.likelihood.square">square</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="mogptk.gpr.likelihood.BernoulliLikelihood" href="#mogptk.gpr.likelihood.BernoulliLikelihood">BernoulliLikelihood</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.likelihood.BetaLikelihood" href="#mogptk.gpr.likelihood.BetaLikelihood">BetaLikelihood</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.likelihood.ChiSquaredLikelihood" href="#mogptk.gpr.likelihood.ChiSquaredLikelihood">ChiSquaredLikelihood</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.likelihood.ExponentialLikelihood" href="#mogptk.gpr.likelihood.ExponentialLikelihood">ExponentialLikelihood</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.likelihood.GammaLikelihood" href="#mogptk.gpr.likelihood.GammaLikelihood">GammaLikelihood</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.likelihood.GaussHermiteQuadrature" href="#mogptk.gpr.likelihood.GaussHermiteQuadrature">GaussHermiteQuadrature</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.likelihood.GaussianLikelihood" href="#mogptk.gpr.likelihood.GaussianLikelihood">GaussianLikelihood</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.likelihood.LaplaceLikelihood" href="#mogptk.gpr.likelihood.LaplaceLikelihood">LaplaceLikelihood</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.likelihood.Likelihood" href="#mogptk.gpr.likelihood.Likelihood">Likelihood</a></code></h4>
<ul class="">
<li><code><a title="mogptk.gpr.likelihood.Likelihood.log_prob" href="#mogptk.gpr.likelihood.Likelihood.log_prob">log_prob</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.mean" href="#mogptk.gpr.likelihood.Likelihood.mean">mean</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.predict" href="#mogptk.gpr.likelihood.Likelihood.predict">predict</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.validate_y" href="#mogptk.gpr.likelihood.Likelihood.validate_y">validate_y</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variance" href="#mogptk.gpr.likelihood.Likelihood.variance">variance</a></code></li>
<li><code><a title="mogptk.gpr.likelihood.Likelihood.variational_expectation" href="#mogptk.gpr.likelihood.Likelihood.variational_expectation">variational_expectation</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mogptk.gpr.likelihood.LogGaussianLikelihood" href="#mogptk.gpr.likelihood.LogGaussianLikelihood">LogGaussianLikelihood</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.likelihood.LogLogisticLikelihood" href="#mogptk.gpr.likelihood.LogLogisticLikelihood">LogLogisticLikelihood</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.likelihood.MultiOutputLikelihood" href="#mogptk.gpr.likelihood.MultiOutputLikelihood">MultiOutputLikelihood</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.likelihood.PoissonLikelihood" href="#mogptk.gpr.likelihood.PoissonLikelihood">PoissonLikelihood</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.likelihood.StudentTLikelihood" href="#mogptk.gpr.likelihood.StudentTLikelihood">StudentTLikelihood</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.likelihood.WeibullLikelihood" href="#mogptk.gpr.likelihood.WeibullLikelihood">WeibullLikelihood</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>