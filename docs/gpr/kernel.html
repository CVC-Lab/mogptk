<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>mogptk.gpr.kernel API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{overflow:hidden;margin:0;padding:0;line-height:1.5em}iframe{border:0}#content{overflow:auto;height:100vh;box-sizing:border-box;padding:20px}#sidebar{box-sizing:border-box;padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{display:none;font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;min-width:300px;max-width:400px;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>mogptk.gpr.kernel</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L1-L435" class="git-link">Browse git</a>
</summary>
<pre><code class="python">import torch
import copy
from . import Parameter, config

class Kernel(torch.nn.Module):
    &#34;&#34;&#34;
    Base kernel.

    Args:
        input_dims (int): Number of input dimensions.
        active_dims (list of int): Indices of active dimensions of shape (input_dims,).
    &#34;&#34;&#34;
    def __init__(self, input_dims=None, active_dims=None):
        super().__init__()

        self.input_dims = input_dims
        self.active_dims = active_dims  # checks input
        self.output_dims = None

    def name(self):
        return self.__class__.__name__

    def __call__(self, X1, X2=None):
        &#34;&#34;&#34;
        Calculate kernel matrix. This is the same as calling `K(X1,X2)` but `X1` and `X2` don&#39;t necessarily have to be tensors. If `X2` is not given, it is assumed to be the same as `X1`. Not passing `X2` may be faster for some kernels.

        Args:
            X1 (torch.tensor): Input of shape (data_points0,input_dims).
            X2 (torch.tensor): Input of shape (data_points1,input_dims).

        Returns:
            torch.tensor: Kernel matrix of shape (data_points0,data_points1).
        &#34;&#34;&#34;
        X1, X2 = self._check_input(X1, X2)
        return self.K(X1, X2)

    def __setattr__(self, name, val):
        if name == &#39;train&#39;:
            for p in self.parameters():
                p.train = val
            return
        if hasattr(self, name) and isinstance(getattr(self, name), Parameter):
            raise AttributeError(&#34;parameter is read-only, use Parameter.assign()&#34;)
        if isinstance(val, Parameter) and val._name is None:
            val._name = &#39;%s.%s&#39; % (self.__class__.__name__, name)
        elif isinstance(val, torch.nn.ModuleList):
            for i, item in enumerate(val):
                for p in item.parameters():
                    p._name = &#39;%s[%d].%s&#39; % (self.__class__.__name__, i, p._name)

        super().__setattr__(name, val)

    def _active_input(self, X1, X2=None):
        if self.active_dims is not None:
            X1 = torch.index_select(X1, dim=1, index=self.active_dims)
            if X2 is not None:
                X2 = torch.index_select(X2, dim=1, index=self.active_dims)
        return X1, X2

    def _check_input(self, X1, X2=None):
        if not torch.is_tensor(X1):
            X1 = torch.tensor(X1, device=config.device, dtype=config.dtype)
        elif X1.device != config.device or X1.dtype != config.dtype:
            X1 = X1.to(config.device, config.dtype)
        if X1.ndim != 2:
            raise ValueError(&#34;X should have two dimensions (data_points,input_dims)&#34;)
        if X1.shape[0] == 0 or X1.shape[1] == 0:
            raise ValueError(&#34;X must not be empty&#34;)
        if X2 is not None:
            if not torch.is_tensor(X2):
                X2 = torch.tensor(X2, device=config.device, dtype=config.dtype)
            elif X2.device != config.device or X2.dtype != config.dtype:
                X2 = X2.to(device, dtype)
            if X2.ndim != 2:
                raise ValueError(&#34;X should have two dimensions (data_points,input_dims)&#34;)
            if X2.shape[0] == 0:
                raise ValueError(&#34;X must not be empty&#34;)
            if X1.shape[1] != X2.shape[1]:
                raise ValueError(&#34;input dimensions for X1 and X2 must match&#34;)
        return X1, X2

    def _check_kernels(self, kernels, length=None):
        if isinstance(kernels, tuple):
            if len(kernels) == 1 and isinstance(kernels[0], list):
                kernels = kernels[0]
            else:
                kernels = list(kernels)
        elif not isinstance(kernels, list):
            kernels = [kernels]
        if len(kernels) == 0:
            raise ValueError(&#34;must pass at least one kernel&#34;)
        elif length is not None and len(kernels) != length:
            if len(kernels) != 1:
                raise ValueError(&#34;must pass %d kernels&#34; % length)
            for i in range(length - len(kernels)):
                kernels.append(kernels[0].clone())
        for i, kernel in enumerate(kernels):
            if not issubclass(type(kernel), Kernel):
                raise ValueError(&#34;must pass kernels&#34;)
        if any(kernel.input_dims != kernels[0].input_dims for kernel in kernels[1:]):
            raise ValueError(&#34;kernels must have same input dimensions&#34;)
        output_dims = [kernel.output_dims for kernel in kernels if kernel.output_dims is not None]
        if any(output_dim != output_dims[0] for output_dim in output_dims[1:]):
            raise ValueError(&#34;multi-output kernels must have same output dimensions&#34;)
        if len(output_dims) != 0:
            for kernel in kernels:
                if kernel.active_dims is None and kernel.output_dims is None:
                    input_dims = kernel.input_dims if kernel.input_dims is not None else 1
                    kernel.active_dims = [input_dim+1 for input_dim in range(input_dims)]
        return kernels

    def clone(self):
        return copy.deepcopy(self)

    @property
    def active_dims(self):
        return self._active_dims

    @active_dims.setter
    def active_dims(self, active_dims):
        if active_dims is not None:
            if not isinstance(active_dims, list):
                active_dims = [active_dims]
            if not all(isinstance(item, int) for item in active_dims):
                raise ValueError(&#34;active dimensions must be a list of integers&#34;)
            active_dims = torch.tensor(active_dims, device=config.device, dtype=torch.long)
            if self.input_dims is not None and self.input_dims != active_dims.shape[0]:
                raise ValueError(&#34;input dimensions must match the number of active dimensions&#34;)
            self.input_dims = active_dims.shape[0]
        self._active_dims = active_dims

    def iterkernels(self):
        &#34;&#34;&#34;
        Iterator of all kernels and subkernels. This is useful as some kernels are composed of other kernels (for example the `AddKernel`).
        &#34;&#34;&#34;
        yield self

    def K(self, X1, X2=None):
        &#34;&#34;&#34;
        Calculate kernel matrix. If `X2` is not given, it is assumed to be the same as `X1`. Not passing `X2` may be faster for some kernels.

        Args:
            X1 (torch.tensor): Input of shape (data_points0,input_dims).
            X2 (torch.tensor): Input of shape (data_points1,input_dims).

        Returns:
            torch.tensor: Kernel matrix of shape (data_points0,data_points1).
        &#34;&#34;&#34;
        # X1 is NxD, X2 is MxD, then ret is NxM
        raise NotImplementedError()

    def K_diag(self, X1):
        &#34;&#34;&#34;
        Calculate the diagonal of the kernel matrix. This is usually faster than `K(X1).diagonal()`.

        Args:
            X1 (torch.tensor): Input of shape (data_points,input_dims).

        Returns:
            torch.tensor: Kernel matrix diagonal of shape (data_points,).
        &#34;&#34;&#34;
        # X1 is NxD, then ret is N
        return self.K(X1).diagonal()

    @staticmethod
    def average(X1, X2=None):
        # X1 is NxD, X2 is MxD, then ret is NxMxD
        if X2 is None:
            X2 = X1
        return 0.5 * (X1.unsqueeze(1) + X2)

    @staticmethod
    def distance(X1, X2=None):
        # X1 is NxD, X2 is MxD, then ret is NxMxD
        if X2 is None:
            X2 = X1
        return X1.unsqueeze(1) - X2

    @staticmethod
    def squared_distance(X1, X2=None):
        # X1 is NxD, X2 is MxD, then ret is NxMxD
        if X2 is None:
            X2 = X1
        #return (X1.unsqueeze(1) - X2)**2  # slower than cdist for large X
        return torch.cdist(X2.T.unsqueeze(2), X1.T.unsqueeze(2)).permute((2,1,0))**2

    def __add__(self, other):
        return AddKernel(self, other)

    def __mul__(self, other):
        return MulKernel(self, other)

class Kernels(Kernel):
    &#34;&#34;&#34;
    Base kernel for list of kernels.

    Args:
        kernels (list of Kernel): Kernels.
    &#34;&#34;&#34;
    def __init__(self, *kernels):
        super().__init__()
        kernels = self._check_kernels(kernels)

        i = 0
        while i &lt; len(kernels):
            if isinstance(kernels[i], self.__class__):
                subkernels = kernels[i].kernels
                kernels = kernels[:i] + subkernels + kernels[i+1:]
                i += len(subkernels) - 1
            i += 1
        self.kernels = torch.nn.ModuleList(kernels)

        self.input_dims = kernels[0].input_dims
        output_dims = [kernel.output_dims for kernel in kernels if kernel.output_dims is not None]
        if len(output_dims) == 0:
            self.output_dims = None
        else:
            self.output_dims = output_dims[0]  # they are all equal

    def name(self):
        names = [kernel.name() for kernel in self.kernels]
        return &#39;%s[%s]&#39; % (self.__class__.__name__, names.join(&#39;,&#39;))

    def __getitem__(self, key):
        return self.kernels[key]

    def iterkernels(self):
        yield self
        for kernel in self.kernels:
            yield kernel

class AddKernel(Kernels):
    &#34;&#34;&#34;
    Addition kernel that sums kernels.

    Args:
        kernels (list of Kernel): Kernels.
    &#34;&#34;&#34;
    def __init__(self, *kernels):
        super().__init__(*kernels)

    def K(self, X1, X2=None):
        return torch.stack([kernel(X1, X2) for kernel in self.kernels], dim=2).sum(dim=2)

    def K_diag(self, X1):
        return torch.stack([kernel.K_diag(X1) for kernel in self.kernels], dim=1).sum(dim=1)

class MulKernel(Kernels):
    &#34;&#34;&#34;
    Multiplication kernel that multiplies kernels.

    Args:
        kernels (list of Kernel): Kernels.
    &#34;&#34;&#34;
    def __init__(self, *kernels):
        super().__init__(*kernels)

    def K(self, X1, X2=None):
        return torch.stack([kernel(X1, X2) for kernel in self.kernels], dim=2).prod(dim=2)

    def K_diag(self, X1):
        return torch.stack([kernel.K_diag(X1) for kernel in self.kernels], dim=1).prod(dim=1)

class MixtureKernel(AddKernel):
    &#34;&#34;&#34;
    Mixture kernel that sums `Q` kernels.

    Args:
        kernel (Kernel): Single kernel.
        Q (int): Number of mixtures.
    &#34;&#34;&#34;
    def __init__(self, kernel, Q):
        if not issubclass(type(kernel), Kernel):
            raise ValueError(&#34;must pass kernel&#34;)
        kernels = self._check_kernels(kernel, Q)
        super().__init__(*kernels)

class AutomaticRelevanceDeterminationKernel(MulKernel):
    &#34;&#34;&#34;
    Automatic relevance determination (ARD) kernel that multiplies kernels for each input dimension.

    Args:
        kernel (Kernel): Single kernel.
        input_dims (int): Number of input dimensions.
    &#34;&#34;&#34;
    def __init__(self, kernel, input_dims):
        if not issubclass(type(kernel), Kernel):
            raise ValueError(&#34;must pass kernel&#34;)
        kernels = self._check_kernels(kernel, input_dims)
        for i, kernel in enumerate(kernels):
            kernel.set_active_dims(i)
        super().__init__(*kernels)

cb = None

class MultiOutputKernel(Kernel):
    &#34;&#34;&#34;
    The `MultiOutputKernel` is a base class for multi-output kernels. It assumes that the first dimension of `X` contains channel IDs (integers) and calculates the final kernel matrix accordingly. Concretely, it will call the `Ksub` method for derived kernels from this class, which should return the kernel matrix between channel `i` and `j`, given inputs `X1` and `X2`. This class will automatically split and recombine the input vectors and kernel matrices respectively, in order to create the final kernel matrix of the multi-output kernel.

    Be aware that for implementation of `Ksub`, `i==j` is true for the diagonal matrices. `X2==None` is true when calculating the Gram matrix (i.e. `X1==X2`) and when `i==j`. It is thus a subset of the case `i==j`, and if `X2==None` than `i` is always equal to `j`.

    Args:
        output_dims (int): Number of output dimensions.
        input_dims (int): Number of input dimensions.
        active_dims (list of int): Indices of active dimensions of shape (input_dims,).
    &#34;&#34;&#34;
    # TODO: seems to accumulate a lot of memory in the loops to call Ksub, perhaps it&#39;s keeping the computational graph while indexing?

    def __init__(self, output_dims, input_dims=None, active_dims=None):
        super().__init__(input_dims, active_dims)
        self.output_dims = output_dims

    def _check_input(self, X1, X2=None):
        X1, X2 = super()._check_input(X1, X2)
        if not torch.all(X1[:,0] == X1[:,0].long()) or not torch.all(X1[:,0] &lt; self.output_dims):
            raise ValueError(&#34;X must have integers for the channel IDs in the first input dimension&#34;)
        if X2 is not None and not torch.all(X2[:,0] == X2[:,0].long()) or not torch.all(X1[:,0] &lt; self.output_dims):
            raise ValueError(&#34;X must have integers for the channel IDs in the first input dimension&#34;)
        return X1, X2

    #def K2(self, X1, X2=None):
    #    # X has shape (data_points,1+input_dims) where the first column is the channel ID
    #    # extract channel mask, get data, and find indices that belong to the channels
    #    c1 = X1[:,:1].long() # Nx1
    #    dims = torch.arange(-1, self.output_dims, device=config.device, dtype=torch.long)
    #    m1 = c1 == dims.reshape(1,-1) # Nx(O+1)
    #    i1 = torch.cumsum(torch.count_nonzero(m1, dim=0), dim=0) # O+1

    #    if X2 is None:
    #        res = torch.empty(X1.shape[0], X1.shape[0], device=config.device, dtype=config.dtype)  # NxM
    #        # calculate lower triangle of main kernel matrix, the upper triangle is a transpose
    #        for i in range(self.output_dims):
    #            for j in range(i+1):
    #                # calculate sub kernel matrix and add to main kernel matrix
    #                #n = int(1600/self.output_dims)
    #                if i == j:
    #                    k = self.Ksub(i, i, X1[i1[i]:i1[i+1],1:])
    #                    #res[n*i:n*(i+1),n*i:n*(i+1)] = k
    #                    res[i1[i]:i1[i+1],i1[i]:i1[i+1]] = k
    #                else:
    #                    k = self.Ksub(i, j, X1[i1[i]:i1[i+1],1:], X1[i1[j]:i1[j+1],1:])
    #                    #res[n*i:n*(i+1),n*j:n*(j+1)] = k
    #                    #res[n*j:n*(j+1),n*i:n*(i+1)] = k.T
    #                    res[i1[i]:i1[i+1],i1[j]:i1[j+1]] = k
    #                    res[i1[j]:i1[j+1],i1[i]:i1[i+1]] = k.T
    #    else:
    #        # extract channel mask, get data, and find indices that belong to the channels
    #        c2 = X2[:,:1].long() # Nx1
    #        m2 = c2 == torch.arange(-1, self.output_dims, device=config.device, dtype=config.dtype).reshape(1,-1) # Nx(O+1)
    #        i2 = torch.cumsum(torch.count_nonzero(m2, dim=0), dim=0) # O+1

    #        res = torch.empty(X1.shape[0], X2.shape[0], device=config.device, dtype=config.dtype)  # NxM
    #        for i in range(self.output_dims):
    #            for j in range(self.output_dims):
    #                # calculate sub kernel matrix and add to main kernel matrix
    #                k = self.Ksub(i, j, X1[i1[i]:i1[i+1],1:], X2[i2[j]:i2[j+1],1:])
    #                res[i1[i]:i1[i+1],i2[j]:i2[j+1]] = k

    #    return res

    def K(self, X1, X2=None):
        # X has shape (data_points,1+input_dims) where the first column is the channel ID
        # extract channel mask, get data, and find indices that belong to the channels
        c1 = X1[:,0].long()
        m1 = [c1==i for i in range(self.output_dims)]
        x1 = [X1[m1[i],1:] for i in range(self.output_dims)]
        r1 = [torch.nonzero(m1[i], as_tuple=False) for i in range(self.output_dims)]

        if X2 is None:
            r2 = [r1[i].reshape(1,-1) for i in range(self.output_dims)]

            res = torch.empty(X1.shape[0], X1.shape[0], device=config.device, dtype=config.dtype)  # NxM
            for i in range(self.output_dims):
                for j in range(i+1):
                    # calculate sub kernel matrix and add to main kernel matrix
                    if i == j:
                        k = self.Ksub(i, i, x1[i])
                        res[r1[i],r2[i]] = k
                    else:
                        k = self.Ksub(i, j, x1[i], x1[j])
                        res[r1[i],r2[j]] = k
                        res[r1[j],r2[i]] = k.T
        else:
            # extract channel mask, get data, and find indices that belong to the channels
            c2 = X2[:,0].long()
            m2 = [c2==j for j in range(self.output_dims)]
            x2 = [X2[m2[j],1:] for j in range(self.output_dims)]
            r2 = [torch.nonzero(m2[j], as_tuple=False).reshape(1,-1) for j in range(self.output_dims)]

            res = torch.empty(X1.shape[0], X2.shape[0], device=config.device, dtype=config.dtype)  # NxM
            for i in range(self.output_dims):
                for j in range(self.output_dims):
                    # calculate sub kernel matrix and add to main kernel matrix
                    res[r1[i],r2[j]] = self.Ksub(i, j, x1[i], x2[j])

        return res

    def K_diag(self, X1):
        # extract channel mask, get data, and find indices that belong to the channels
        c1 = X1[:,0].long()
        m1 = [c1==i for i in range(self.output_dims)]
        x1 = [X1[m1[i],1:] for i in range(self.output_dims)]
        r1 = [torch.nonzero(m1[i], as_tuple=False)[:,0] for i in range(self.output_dims)]

        res = torch.empty(X1.shape[0], device=config.device, dtype=config.dtype)  # NxM

        for i in range(self.output_dims):
            # calculate sub kernel matrix and add to main kernel matrix
            res[r1[i]] = self.Ksub_diag(i, x1[i])
        return res

    def Ksub(self, i, j, X1, X2=None):
        &#34;&#34;&#34;
        Calculate kernel matrix between two channels. If `X2` is not given, it is assumed to be the same as `X1`. Not passing `X2` may be faster for some kernels.

        Args:
            X1 (torch.tensor): Input of shape (data_points0,input_dims).
            X2 (torch.tensor): Input of shape (data_points1,input_dims).

        Returns:
            torch.tensor: Kernel matrix of shape (data_points0,data_points1).
        &#34;&#34;&#34;
        raise NotImplementedError()

    def Ksub_diag(self, i, X1):
        &#34;&#34;&#34;
        Calculate the diagonal of the kernel matrix between two channels. This is usually faster than `Ksub(X1).diagonal()`.

        Args:
            X1 (torch.tensor): Input of shape (data_points,input_dims).

        Returns:
            torch.tensor: Kernel matrix diagonal of shape (data_points,).
        &#34;&#34;&#34;
        return self.Ksub(i, i, X1).diagonal()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="mogptk.gpr.kernel.AddKernel"><code class="flex name class">
<span>class <span class="ident">AddKernel</span></span>
<span>(</span><span>*kernels)</span>
</code></dt>
<dd>
<div class="desc"><p>Addition kernel that sums kernels.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kernels</code></strong> :&ensp;<code>list</code> of <code><a title="mogptk.gpr.kernel.Kernel" href="#mogptk.gpr.kernel.Kernel">Kernel</a></code></dt>
<dd>Kernels.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L232-L246" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class AddKernel(Kernels):
    &#34;&#34;&#34;
    Addition kernel that sums kernels.

    Args:
        kernels (list of Kernel): Kernels.
    &#34;&#34;&#34;
    def __init__(self, *kernels):
        super().__init__(*kernels)

    def K(self, X1, X2=None):
        return torch.stack([kernel(X1, X2) for kernel in self.kernels], dim=2).sum(dim=2)

    def K_diag(self, X1):
        return torch.stack([kernel.K_diag(X1) for kernel in self.kernels], dim=1).sum(dim=1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.kernel.Kernels" href="#mogptk.gpr.kernel.Kernels">Kernels</a></li>
<li><a title="mogptk.gpr.kernel.Kernel" href="#mogptk.gpr.kernel.Kernel">Kernel</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.kernel.MixtureKernel" href="#mogptk.gpr.kernel.MixtureKernel">MixtureKernel</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.kernel.Kernels" href="#mogptk.gpr.kernel.Kernels">Kernels</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.kernel.Kernels.K" href="#mogptk.gpr.kernel.Kernel.K">K</a></code></li>
<li><code><a title="mogptk.gpr.kernel.Kernels.K_diag" href="#mogptk.gpr.kernel.Kernel.K_diag">K_diag</a></code></li>
<li><code><a title="mogptk.gpr.kernel.Kernels.iterkernels" href="#mogptk.gpr.kernel.Kernel.iterkernels">iterkernels</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.kernel.AutomaticRelevanceDeterminationKernel"><code class="flex name class">
<span>class <span class="ident">AutomaticRelevanceDeterminationKernel</span></span>
<span>(</span><span>kernel, input_dims)</span>
</code></dt>
<dd>
<div class="desc"><p>Automatic relevance determination (ARD) kernel that multiplies kernels for each input dimension.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kernel</code></strong> :&ensp;<code><a title="mogptk.gpr.kernel.Kernel" href="#mogptk.gpr.kernel.Kernel">Kernel</a></code></dt>
<dd>Single kernel.</dd>
<dt><strong><code>input_dims</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of input dimensions.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L278-L292" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class AutomaticRelevanceDeterminationKernel(MulKernel):
    &#34;&#34;&#34;
    Automatic relevance determination (ARD) kernel that multiplies kernels for each input dimension.

    Args:
        kernel (Kernel): Single kernel.
        input_dims (int): Number of input dimensions.
    &#34;&#34;&#34;
    def __init__(self, kernel, input_dims):
        if not issubclass(type(kernel), Kernel):
            raise ValueError(&#34;must pass kernel&#34;)
        kernels = self._check_kernels(kernel, input_dims)
        for i, kernel in enumerate(kernels):
            kernel.set_active_dims(i)
        super().__init__(*kernels)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.kernel.MulKernel" href="#mogptk.gpr.kernel.MulKernel">MulKernel</a></li>
<li><a title="mogptk.gpr.kernel.Kernels" href="#mogptk.gpr.kernel.Kernels">Kernels</a></li>
<li><a title="mogptk.gpr.kernel.Kernel" href="#mogptk.gpr.kernel.Kernel">Kernel</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.kernel.MulKernel" href="#mogptk.gpr.kernel.MulKernel">MulKernel</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.kernel.MulKernel.K" href="#mogptk.gpr.kernel.Kernel.K">K</a></code></li>
<li><code><a title="mogptk.gpr.kernel.MulKernel.K_diag" href="#mogptk.gpr.kernel.Kernel.K_diag">K_diag</a></code></li>
<li><code><a title="mogptk.gpr.kernel.MulKernel.iterkernels" href="#mogptk.gpr.kernel.Kernel.iterkernels">iterkernels</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.kernel.Kernel"><code class="flex name class">
<span>class <span class="ident">Kernel</span></span>
<span>(</span><span>input_dims=None, active_dims=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Base kernel.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_dims</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of input dimensions.</dd>
<dt><strong><code>active_dims</code></strong> :&ensp;<code>list</code> of <code>int</code></dt>
<dd>Indices of active dimensions of shape (input_dims,).</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L5-L191" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Kernel(torch.nn.Module):
    &#34;&#34;&#34;
    Base kernel.

    Args:
        input_dims (int): Number of input dimensions.
        active_dims (list of int): Indices of active dimensions of shape (input_dims,).
    &#34;&#34;&#34;
    def __init__(self, input_dims=None, active_dims=None):
        super().__init__()

        self.input_dims = input_dims
        self.active_dims = active_dims  # checks input
        self.output_dims = None

    def name(self):
        return self.__class__.__name__

    def __call__(self, X1, X2=None):
        &#34;&#34;&#34;
        Calculate kernel matrix. This is the same as calling `K(X1,X2)` but `X1` and `X2` don&#39;t necessarily have to be tensors. If `X2` is not given, it is assumed to be the same as `X1`. Not passing `X2` may be faster for some kernels.

        Args:
            X1 (torch.tensor): Input of shape (data_points0,input_dims).
            X2 (torch.tensor): Input of shape (data_points1,input_dims).

        Returns:
            torch.tensor: Kernel matrix of shape (data_points0,data_points1).
        &#34;&#34;&#34;
        X1, X2 = self._check_input(X1, X2)
        return self.K(X1, X2)

    def __setattr__(self, name, val):
        if name == &#39;train&#39;:
            for p in self.parameters():
                p.train = val
            return
        if hasattr(self, name) and isinstance(getattr(self, name), Parameter):
            raise AttributeError(&#34;parameter is read-only, use Parameter.assign()&#34;)
        if isinstance(val, Parameter) and val._name is None:
            val._name = &#39;%s.%s&#39; % (self.__class__.__name__, name)
        elif isinstance(val, torch.nn.ModuleList):
            for i, item in enumerate(val):
                for p in item.parameters():
                    p._name = &#39;%s[%d].%s&#39; % (self.__class__.__name__, i, p._name)

        super().__setattr__(name, val)

    def _active_input(self, X1, X2=None):
        if self.active_dims is not None:
            X1 = torch.index_select(X1, dim=1, index=self.active_dims)
            if X2 is not None:
                X2 = torch.index_select(X2, dim=1, index=self.active_dims)
        return X1, X2

    def _check_input(self, X1, X2=None):
        if not torch.is_tensor(X1):
            X1 = torch.tensor(X1, device=config.device, dtype=config.dtype)
        elif X1.device != config.device or X1.dtype != config.dtype:
            X1 = X1.to(config.device, config.dtype)
        if X1.ndim != 2:
            raise ValueError(&#34;X should have two dimensions (data_points,input_dims)&#34;)
        if X1.shape[0] == 0 or X1.shape[1] == 0:
            raise ValueError(&#34;X must not be empty&#34;)
        if X2 is not None:
            if not torch.is_tensor(X2):
                X2 = torch.tensor(X2, device=config.device, dtype=config.dtype)
            elif X2.device != config.device or X2.dtype != config.dtype:
                X2 = X2.to(device, dtype)
            if X2.ndim != 2:
                raise ValueError(&#34;X should have two dimensions (data_points,input_dims)&#34;)
            if X2.shape[0] == 0:
                raise ValueError(&#34;X must not be empty&#34;)
            if X1.shape[1] != X2.shape[1]:
                raise ValueError(&#34;input dimensions for X1 and X2 must match&#34;)
        return X1, X2

    def _check_kernels(self, kernels, length=None):
        if isinstance(kernels, tuple):
            if len(kernels) == 1 and isinstance(kernels[0], list):
                kernels = kernels[0]
            else:
                kernels = list(kernels)
        elif not isinstance(kernels, list):
            kernels = [kernels]
        if len(kernels) == 0:
            raise ValueError(&#34;must pass at least one kernel&#34;)
        elif length is not None and len(kernels) != length:
            if len(kernels) != 1:
                raise ValueError(&#34;must pass %d kernels&#34; % length)
            for i in range(length - len(kernels)):
                kernels.append(kernels[0].clone())
        for i, kernel in enumerate(kernels):
            if not issubclass(type(kernel), Kernel):
                raise ValueError(&#34;must pass kernels&#34;)
        if any(kernel.input_dims != kernels[0].input_dims for kernel in kernels[1:]):
            raise ValueError(&#34;kernels must have same input dimensions&#34;)
        output_dims = [kernel.output_dims for kernel in kernels if kernel.output_dims is not None]
        if any(output_dim != output_dims[0] for output_dim in output_dims[1:]):
            raise ValueError(&#34;multi-output kernels must have same output dimensions&#34;)
        if len(output_dims) != 0:
            for kernel in kernels:
                if kernel.active_dims is None and kernel.output_dims is None:
                    input_dims = kernel.input_dims if kernel.input_dims is not None else 1
                    kernel.active_dims = [input_dim+1 for input_dim in range(input_dims)]
        return kernels

    def clone(self):
        return copy.deepcopy(self)

    @property
    def active_dims(self):
        return self._active_dims

    @active_dims.setter
    def active_dims(self, active_dims):
        if active_dims is not None:
            if not isinstance(active_dims, list):
                active_dims = [active_dims]
            if not all(isinstance(item, int) for item in active_dims):
                raise ValueError(&#34;active dimensions must be a list of integers&#34;)
            active_dims = torch.tensor(active_dims, device=config.device, dtype=torch.long)
            if self.input_dims is not None and self.input_dims != active_dims.shape[0]:
                raise ValueError(&#34;input dimensions must match the number of active dimensions&#34;)
            self.input_dims = active_dims.shape[0]
        self._active_dims = active_dims

    def iterkernels(self):
        &#34;&#34;&#34;
        Iterator of all kernels and subkernels. This is useful as some kernels are composed of other kernels (for example the `AddKernel`).
        &#34;&#34;&#34;
        yield self

    def K(self, X1, X2=None):
        &#34;&#34;&#34;
        Calculate kernel matrix. If `X2` is not given, it is assumed to be the same as `X1`. Not passing `X2` may be faster for some kernels.

        Args:
            X1 (torch.tensor): Input of shape (data_points0,input_dims).
            X2 (torch.tensor): Input of shape (data_points1,input_dims).

        Returns:
            torch.tensor: Kernel matrix of shape (data_points0,data_points1).
        &#34;&#34;&#34;
        # X1 is NxD, X2 is MxD, then ret is NxM
        raise NotImplementedError()

    def K_diag(self, X1):
        &#34;&#34;&#34;
        Calculate the diagonal of the kernel matrix. This is usually faster than `K(X1).diagonal()`.

        Args:
            X1 (torch.tensor): Input of shape (data_points,input_dims).

        Returns:
            torch.tensor: Kernel matrix diagonal of shape (data_points,).
        &#34;&#34;&#34;
        # X1 is NxD, then ret is N
        return self.K(X1).diagonal()

    @staticmethod
    def average(X1, X2=None):
        # X1 is NxD, X2 is MxD, then ret is NxMxD
        if X2 is None:
            X2 = X1
        return 0.5 * (X1.unsqueeze(1) + X2)

    @staticmethod
    def distance(X1, X2=None):
        # X1 is NxD, X2 is MxD, then ret is NxMxD
        if X2 is None:
            X2 = X1
        return X1.unsqueeze(1) - X2

    @staticmethod
    def squared_distance(X1, X2=None):
        # X1 is NxD, X2 is MxD, then ret is NxMxD
        if X2 is None:
            X2 = X1
        #return (X1.unsqueeze(1) - X2)**2  # slower than cdist for large X
        return torch.cdist(X2.T.unsqueeze(2), X1.T.unsqueeze(2)).permute((2,1,0))**2

    def __add__(self, other):
        return AddKernel(self, other)

    def __mul__(self, other):
        return MulKernel(self, other)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.kernel.Kernels" href="#mogptk.gpr.kernel.Kernels">Kernels</a></li>
<li><a title="mogptk.gpr.kernel.MultiOutputKernel" href="#mogptk.gpr.kernel.MultiOutputKernel">MultiOutputKernel</a></li>
<li><a title="mogptk.gpr.singleoutput.ConstantKernel" href="singleoutput.html#mogptk.gpr.singleoutput.ConstantKernel">ConstantKernel</a></li>
<li><a title="mogptk.gpr.singleoutput.CosineKernel" href="singleoutput.html#mogptk.gpr.singleoutput.CosineKernel">CosineKernel</a></li>
<li><a title="mogptk.gpr.singleoutput.ExponentialKernel" href="singleoutput.html#mogptk.gpr.singleoutput.ExponentialKernel">ExponentialKernel</a></li>
<li><a title="mogptk.gpr.singleoutput.FunctionKernel" href="singleoutput.html#mogptk.gpr.singleoutput.FunctionKernel">FunctionKernel</a></li>
<li><a title="mogptk.gpr.singleoutput.LinearKernel" href="singleoutput.html#mogptk.gpr.singleoutput.LinearKernel">LinearKernel</a></li>
<li><a title="mogptk.gpr.singleoutput.LocallyPeriodicKernel" href="singleoutput.html#mogptk.gpr.singleoutput.LocallyPeriodicKernel">LocallyPeriodicKernel</a></li>
<li><a title="mogptk.gpr.singleoutput.MaternKernel" href="singleoutput.html#mogptk.gpr.singleoutput.MaternKernel">MaternKernel</a></li>
<li><a title="mogptk.gpr.singleoutput.PeriodicKernel" href="singleoutput.html#mogptk.gpr.singleoutput.PeriodicKernel">PeriodicKernel</a></li>
<li><a title="mogptk.gpr.singleoutput.PolynomialKernel" href="singleoutput.html#mogptk.gpr.singleoutput.PolynomialKernel">PolynomialKernel</a></li>
<li><a title="mogptk.gpr.singleoutput.RationalQuadraticKernel" href="singleoutput.html#mogptk.gpr.singleoutput.RationalQuadraticKernel">RationalQuadraticKernel</a></li>
<li><a title="mogptk.gpr.singleoutput.SincKernel" href="singleoutput.html#mogptk.gpr.singleoutput.SincKernel">SincKernel</a></li>
<li><a title="mogptk.gpr.singleoutput.SpectralKernel" href="singleoutput.html#mogptk.gpr.singleoutput.SpectralKernel">SpectralKernel</a></li>
<li><a title="mogptk.gpr.singleoutput.SpectralMixtureKernel" href="singleoutput.html#mogptk.gpr.singleoutput.SpectralMixtureKernel">SpectralMixtureKernel</a></li>
<li><a title="mogptk.gpr.singleoutput.SquaredExponentialKernel" href="singleoutput.html#mogptk.gpr.singleoutput.SquaredExponentialKernel">SquaredExponentialKernel</a></li>
<li><a title="mogptk.gpr.singleoutput.WhiteKernel" href="singleoutput.html#mogptk.gpr.singleoutput.WhiteKernel">WhiteKernel</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="mogptk.gpr.kernel.Kernel.average"><code class="name flex">
<span>def <span class="ident">average</span></span>(<span>X1, X2=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L165-L170" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@staticmethod
def average(X1, X2=None):
    # X1 is NxD, X2 is MxD, then ret is NxMxD
    if X2 is None:
        X2 = X1
    return 0.5 * (X1.unsqueeze(1) + X2)</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.kernel.Kernel.distance"><code class="name flex">
<span>def <span class="ident">distance</span></span>(<span>X1, X2=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L172-L177" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@staticmethod
def distance(X1, X2=None):
    # X1 is NxD, X2 is MxD, then ret is NxMxD
    if X2 is None:
        X2 = X1
    return X1.unsqueeze(1) - X2</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.kernel.Kernel.squared_distance"><code class="name flex">
<span>def <span class="ident">squared_distance</span></span>(<span>X1, X2=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L179-L185" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@staticmethod
def squared_distance(X1, X2=None):
    # X1 is NxD, X2 is MxD, then ret is NxMxD
    if X2 is None:
        X2 = X1
    #return (X1.unsqueeze(1) - X2)**2  # slower than cdist for large X
    return torch.cdist(X2.T.unsqueeze(2), X1.T.unsqueeze(2)).permute((2,1,0))**2</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="mogptk.gpr.kernel.Kernel.active_dims"><code class="name">var <span class="ident">active_dims</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L115-L117" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@property
def active_dims(self):
    return self._active_dims</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mogptk.gpr.kernel.Kernel.K"><code class="name flex">
<span>def <span class="ident">K</span></span>(<span>self, X1, X2=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate kernel matrix. If <code>X2</code> is not given, it is assumed to be the same as <code>X1</code>. Not passing <code>X2</code> may be faster for some kernels.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X1</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Input of shape (data_points0,input_dims).</dd>
<dt><strong><code>X2</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Input of shape (data_points1,input_dims).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.tensor</code></dt>
<dd>Kernel matrix of shape (data_points0,data_points1).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L138-L150" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def K(self, X1, X2=None):
    &#34;&#34;&#34;
    Calculate kernel matrix. If `X2` is not given, it is assumed to be the same as `X1`. Not passing `X2` may be faster for some kernels.

    Args:
        X1 (torch.tensor): Input of shape (data_points0,input_dims).
        X2 (torch.tensor): Input of shape (data_points1,input_dims).

    Returns:
        torch.tensor: Kernel matrix of shape (data_points0,data_points1).
    &#34;&#34;&#34;
    # X1 is NxD, X2 is MxD, then ret is NxM
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.kernel.Kernel.K_diag"><code class="name flex">
<span>def <span class="ident">K_diag</span></span>(<span>self, X1)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the diagonal of the kernel matrix. This is usually faster than <code>K(X1).diagonal()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X1</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Input of shape (data_points,input_dims).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.tensor</code></dt>
<dd>Kernel matrix diagonal of shape (data_points,).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L152-L163" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def K_diag(self, X1):
    &#34;&#34;&#34;
    Calculate the diagonal of the kernel matrix. This is usually faster than `K(X1).diagonal()`.

    Args:
        X1 (torch.tensor): Input of shape (data_points,input_dims).

    Returns:
        torch.tensor: Kernel matrix diagonal of shape (data_points,).
    &#34;&#34;&#34;
    # X1 is NxD, then ret is N
    return self.K(X1).diagonal()</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.kernel.Kernel.clone"><code class="name flex">
<span>def <span class="ident">clone</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L112-L113" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def clone(self):
    return copy.deepcopy(self)</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.kernel.Kernel.iterkernels"><code class="name flex">
<span>def <span class="ident">iterkernels</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Iterator of all kernels and subkernels. This is useful as some kernels are composed of other kernels (for example the <code><a title="mogptk.gpr.kernel.AddKernel" href="#mogptk.gpr.kernel.AddKernel">AddKernel</a></code>).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L132-L136" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def iterkernels(self):
    &#34;&#34;&#34;
    Iterator of all kernels and subkernels. This is useful as some kernels are composed of other kernels (for example the `AddKernel`).
    &#34;&#34;&#34;
    yield self</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.kernel.Kernel.name"><code class="name flex">
<span>def <span class="ident">name</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L20-L21" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def name(self):
    return self.__class__.__name__</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="mogptk.gpr.kernel.Kernels"><code class="flex name class">
<span>class <span class="ident">Kernels</span></span>
<span>(</span><span>*kernels)</span>
</code></dt>
<dd>
<div class="desc"><p>Base kernel for list of kernels.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kernels</code></strong> :&ensp;<code>list</code> of <code><a title="mogptk.gpr.kernel.Kernel" href="#mogptk.gpr.kernel.Kernel">Kernel</a></code></dt>
<dd>Kernels.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L193-L230" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Kernels(Kernel):
    &#34;&#34;&#34;
    Base kernel for list of kernels.

    Args:
        kernels (list of Kernel): Kernels.
    &#34;&#34;&#34;
    def __init__(self, *kernels):
        super().__init__()
        kernels = self._check_kernels(kernels)

        i = 0
        while i &lt; len(kernels):
            if isinstance(kernels[i], self.__class__):
                subkernels = kernels[i].kernels
                kernels = kernels[:i] + subkernels + kernels[i+1:]
                i += len(subkernels) - 1
            i += 1
        self.kernels = torch.nn.ModuleList(kernels)

        self.input_dims = kernels[0].input_dims
        output_dims = [kernel.output_dims for kernel in kernels if kernel.output_dims is not None]
        if len(output_dims) == 0:
            self.output_dims = None
        else:
            self.output_dims = output_dims[0]  # they are all equal

    def name(self):
        names = [kernel.name() for kernel in self.kernels]
        return &#39;%s[%s]&#39; % (self.__class__.__name__, names.join(&#39;,&#39;))

    def __getitem__(self, key):
        return self.kernels[key]

    def iterkernels(self):
        yield self
        for kernel in self.kernels:
            yield kernel</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.kernel.Kernel" href="#mogptk.gpr.kernel.Kernel">Kernel</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.kernel.AddKernel" href="#mogptk.gpr.kernel.AddKernel">AddKernel</a></li>
<li><a title="mogptk.gpr.kernel.MulKernel" href="#mogptk.gpr.kernel.MulKernel">MulKernel</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mogptk.gpr.kernel.Kernels.name"><code class="name flex">
<span>def <span class="ident">name</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L220-L222" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def name(self):
    names = [kernel.name() for kernel in self.kernels]
    return &#39;%s[%s]&#39; % (self.__class__.__name__, names.join(&#39;,&#39;))</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.kernel.Kernel" href="#mogptk.gpr.kernel.Kernel">Kernel</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.kernel.Kernel.K" href="#mogptk.gpr.kernel.Kernel.K">K</a></code></li>
<li><code><a title="mogptk.gpr.kernel.Kernel.K_diag" href="#mogptk.gpr.kernel.Kernel.K_diag">K_diag</a></code></li>
<li><code><a title="mogptk.gpr.kernel.Kernel.iterkernels" href="#mogptk.gpr.kernel.Kernel.iterkernels">iterkernels</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.kernel.MixtureKernel"><code class="flex name class">
<span>class <span class="ident">MixtureKernel</span></span>
<span>(</span><span>kernel, Q)</span>
</code></dt>
<dd>
<div class="desc"><p>Mixture kernel that sums <code>Q</code> kernels.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kernel</code></strong> :&ensp;<code><a title="mogptk.gpr.kernel.Kernel" href="#mogptk.gpr.kernel.Kernel">Kernel</a></code></dt>
<dd>Single kernel.</dd>
<dt><strong><code>Q</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of mixtures.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L264-L276" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class MixtureKernel(AddKernel):
    &#34;&#34;&#34;
    Mixture kernel that sums `Q` kernels.

    Args:
        kernel (Kernel): Single kernel.
        Q (int): Number of mixtures.
    &#34;&#34;&#34;
    def __init__(self, kernel, Q):
        if not issubclass(type(kernel), Kernel):
            raise ValueError(&#34;must pass kernel&#34;)
        kernels = self._check_kernels(kernel, Q)
        super().__init__(*kernels)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.kernel.AddKernel" href="#mogptk.gpr.kernel.AddKernel">AddKernel</a></li>
<li><a title="mogptk.gpr.kernel.Kernels" href="#mogptk.gpr.kernel.Kernels">Kernels</a></li>
<li><a title="mogptk.gpr.kernel.Kernel" href="#mogptk.gpr.kernel.Kernel">Kernel</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.kernel.AddKernel" href="#mogptk.gpr.kernel.AddKernel">AddKernel</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.kernel.AddKernel.K" href="#mogptk.gpr.kernel.Kernel.K">K</a></code></li>
<li><code><a title="mogptk.gpr.kernel.AddKernel.K_diag" href="#mogptk.gpr.kernel.Kernel.K_diag">K_diag</a></code></li>
<li><code><a title="mogptk.gpr.kernel.AddKernel.iterkernels" href="#mogptk.gpr.kernel.Kernel.iterkernels">iterkernels</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.kernel.MulKernel"><code class="flex name class">
<span>class <span class="ident">MulKernel</span></span>
<span>(</span><span>*kernels)</span>
</code></dt>
<dd>
<div class="desc"><p>Multiplication kernel that multiplies kernels.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kernels</code></strong> :&ensp;<code>list</code> of <code><a title="mogptk.gpr.kernel.Kernel" href="#mogptk.gpr.kernel.Kernel">Kernel</a></code></dt>
<dd>Kernels.</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L248-L262" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class MulKernel(Kernels):
    &#34;&#34;&#34;
    Multiplication kernel that multiplies kernels.

    Args:
        kernels (list of Kernel): Kernels.
    &#34;&#34;&#34;
    def __init__(self, *kernels):
        super().__init__(*kernels)

    def K(self, X1, X2=None):
        return torch.stack([kernel(X1, X2) for kernel in self.kernels], dim=2).prod(dim=2)

    def K_diag(self, X1):
        return torch.stack([kernel.K_diag(X1) for kernel in self.kernels], dim=1).prod(dim=1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.kernel.Kernels" href="#mogptk.gpr.kernel.Kernels">Kernels</a></li>
<li><a title="mogptk.gpr.kernel.Kernel" href="#mogptk.gpr.kernel.Kernel">Kernel</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.kernel.AutomaticRelevanceDeterminationKernel" href="#mogptk.gpr.kernel.AutomaticRelevanceDeterminationKernel">AutomaticRelevanceDeterminationKernel</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.kernel.Kernels" href="#mogptk.gpr.kernel.Kernels">Kernels</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.kernel.Kernels.K" href="#mogptk.gpr.kernel.Kernel.K">K</a></code></li>
<li><code><a title="mogptk.gpr.kernel.Kernels.K_diag" href="#mogptk.gpr.kernel.Kernel.K_diag">K_diag</a></code></li>
<li><code><a title="mogptk.gpr.kernel.Kernels.iterkernels" href="#mogptk.gpr.kernel.Kernel.iterkernels">iterkernels</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mogptk.gpr.kernel.MultiOutputKernel"><code class="flex name class">
<span>class <span class="ident">MultiOutputKernel</span></span>
<span>(</span><span>output_dims, input_dims=None, active_dims=None)</span>
</code></dt>
<dd>
<div class="desc"><p>The <code><a title="mogptk.gpr.kernel.MultiOutputKernel" href="#mogptk.gpr.kernel.MultiOutputKernel">MultiOutputKernel</a></code> is a base class for multi-output kernels. It assumes that the first dimension of <code>X</code> contains channel IDs (integers) and calculates the final kernel matrix accordingly. Concretely, it will call the <code>Ksub</code> method for derived kernels from this class, which should return the kernel matrix between channel <code>i</code> and <code>j</code>, given inputs <code>X1</code> and <code>X2</code>. This class will automatically split and recombine the input vectors and kernel matrices respectively, in order to create the final kernel matrix of the multi-output kernel.</p>
<p>Be aware that for implementation of <code>Ksub</code>, <code>i==j</code> is true for the diagonal matrices. <code>X2==None</code> is true when calculating the Gram matrix (i.e. <code>X1==X2</code>) and when <code>i==j</code>. It is thus a subset of the case <code>i==j</code>, and if <code>X2==None</code> than <code>i</code> is always equal to <code>j</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_dims</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output dimensions.</dd>
<dt><strong><code>input_dims</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of input dimensions.</dd>
<dt><strong><code>active_dims</code></strong> :&ensp;<code>list</code> of <code>int</code></dt>
<dd>Indices of active dimensions of shape (input_dims,).</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L296-L435" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class MultiOutputKernel(Kernel):
    &#34;&#34;&#34;
    The `MultiOutputKernel` is a base class for multi-output kernels. It assumes that the first dimension of `X` contains channel IDs (integers) and calculates the final kernel matrix accordingly. Concretely, it will call the `Ksub` method for derived kernels from this class, which should return the kernel matrix between channel `i` and `j`, given inputs `X1` and `X2`. This class will automatically split and recombine the input vectors and kernel matrices respectively, in order to create the final kernel matrix of the multi-output kernel.

    Be aware that for implementation of `Ksub`, `i==j` is true for the diagonal matrices. `X2==None` is true when calculating the Gram matrix (i.e. `X1==X2`) and when `i==j`. It is thus a subset of the case `i==j`, and if `X2==None` than `i` is always equal to `j`.

    Args:
        output_dims (int): Number of output dimensions.
        input_dims (int): Number of input dimensions.
        active_dims (list of int): Indices of active dimensions of shape (input_dims,).
    &#34;&#34;&#34;
    # TODO: seems to accumulate a lot of memory in the loops to call Ksub, perhaps it&#39;s keeping the computational graph while indexing?

    def __init__(self, output_dims, input_dims=None, active_dims=None):
        super().__init__(input_dims, active_dims)
        self.output_dims = output_dims

    def _check_input(self, X1, X2=None):
        X1, X2 = super()._check_input(X1, X2)
        if not torch.all(X1[:,0] == X1[:,0].long()) or not torch.all(X1[:,0] &lt; self.output_dims):
            raise ValueError(&#34;X must have integers for the channel IDs in the first input dimension&#34;)
        if X2 is not None and not torch.all(X2[:,0] == X2[:,0].long()) or not torch.all(X1[:,0] &lt; self.output_dims):
            raise ValueError(&#34;X must have integers for the channel IDs in the first input dimension&#34;)
        return X1, X2

    #def K2(self, X1, X2=None):
    #    # X has shape (data_points,1+input_dims) where the first column is the channel ID
    #    # extract channel mask, get data, and find indices that belong to the channels
    #    c1 = X1[:,:1].long() # Nx1
    #    dims = torch.arange(-1, self.output_dims, device=config.device, dtype=torch.long)
    #    m1 = c1 == dims.reshape(1,-1) # Nx(O+1)
    #    i1 = torch.cumsum(torch.count_nonzero(m1, dim=0), dim=0) # O+1

    #    if X2 is None:
    #        res = torch.empty(X1.shape[0], X1.shape[0], device=config.device, dtype=config.dtype)  # NxM
    #        # calculate lower triangle of main kernel matrix, the upper triangle is a transpose
    #        for i in range(self.output_dims):
    #            for j in range(i+1):
    #                # calculate sub kernel matrix and add to main kernel matrix
    #                #n = int(1600/self.output_dims)
    #                if i == j:
    #                    k = self.Ksub(i, i, X1[i1[i]:i1[i+1],1:])
    #                    #res[n*i:n*(i+1),n*i:n*(i+1)] = k
    #                    res[i1[i]:i1[i+1],i1[i]:i1[i+1]] = k
    #                else:
    #                    k = self.Ksub(i, j, X1[i1[i]:i1[i+1],1:], X1[i1[j]:i1[j+1],1:])
    #                    #res[n*i:n*(i+1),n*j:n*(j+1)] = k
    #                    #res[n*j:n*(j+1),n*i:n*(i+1)] = k.T
    #                    res[i1[i]:i1[i+1],i1[j]:i1[j+1]] = k
    #                    res[i1[j]:i1[j+1],i1[i]:i1[i+1]] = k.T
    #    else:
    #        # extract channel mask, get data, and find indices that belong to the channels
    #        c2 = X2[:,:1].long() # Nx1
    #        m2 = c2 == torch.arange(-1, self.output_dims, device=config.device, dtype=config.dtype).reshape(1,-1) # Nx(O+1)
    #        i2 = torch.cumsum(torch.count_nonzero(m2, dim=0), dim=0) # O+1

    #        res = torch.empty(X1.shape[0], X2.shape[0], device=config.device, dtype=config.dtype)  # NxM
    #        for i in range(self.output_dims):
    #            for j in range(self.output_dims):
    #                # calculate sub kernel matrix and add to main kernel matrix
    #                k = self.Ksub(i, j, X1[i1[i]:i1[i+1],1:], X2[i2[j]:i2[j+1],1:])
    #                res[i1[i]:i1[i+1],i2[j]:i2[j+1]] = k

    #    return res

    def K(self, X1, X2=None):
        # X has shape (data_points,1+input_dims) where the first column is the channel ID
        # extract channel mask, get data, and find indices that belong to the channels
        c1 = X1[:,0].long()
        m1 = [c1==i for i in range(self.output_dims)]
        x1 = [X1[m1[i],1:] for i in range(self.output_dims)]
        r1 = [torch.nonzero(m1[i], as_tuple=False) for i in range(self.output_dims)]

        if X2 is None:
            r2 = [r1[i].reshape(1,-1) for i in range(self.output_dims)]

            res = torch.empty(X1.shape[0], X1.shape[0], device=config.device, dtype=config.dtype)  # NxM
            for i in range(self.output_dims):
                for j in range(i+1):
                    # calculate sub kernel matrix and add to main kernel matrix
                    if i == j:
                        k = self.Ksub(i, i, x1[i])
                        res[r1[i],r2[i]] = k
                    else:
                        k = self.Ksub(i, j, x1[i], x1[j])
                        res[r1[i],r2[j]] = k
                        res[r1[j],r2[i]] = k.T
        else:
            # extract channel mask, get data, and find indices that belong to the channels
            c2 = X2[:,0].long()
            m2 = [c2==j for j in range(self.output_dims)]
            x2 = [X2[m2[j],1:] for j in range(self.output_dims)]
            r2 = [torch.nonzero(m2[j], as_tuple=False).reshape(1,-1) for j in range(self.output_dims)]

            res = torch.empty(X1.shape[0], X2.shape[0], device=config.device, dtype=config.dtype)  # NxM
            for i in range(self.output_dims):
                for j in range(self.output_dims):
                    # calculate sub kernel matrix and add to main kernel matrix
                    res[r1[i],r2[j]] = self.Ksub(i, j, x1[i], x2[j])

        return res

    def K_diag(self, X1):
        # extract channel mask, get data, and find indices that belong to the channels
        c1 = X1[:,0].long()
        m1 = [c1==i for i in range(self.output_dims)]
        x1 = [X1[m1[i],1:] for i in range(self.output_dims)]
        r1 = [torch.nonzero(m1[i], as_tuple=False)[:,0] for i in range(self.output_dims)]

        res = torch.empty(X1.shape[0], device=config.device, dtype=config.dtype)  # NxM

        for i in range(self.output_dims):
            # calculate sub kernel matrix and add to main kernel matrix
            res[r1[i]] = self.Ksub_diag(i, x1[i])
        return res

    def Ksub(self, i, j, X1, X2=None):
        &#34;&#34;&#34;
        Calculate kernel matrix between two channels. If `X2` is not given, it is assumed to be the same as `X1`. Not passing `X2` may be faster for some kernels.

        Args:
            X1 (torch.tensor): Input of shape (data_points0,input_dims).
            X2 (torch.tensor): Input of shape (data_points1,input_dims).

        Returns:
            torch.tensor: Kernel matrix of shape (data_points0,data_points1).
        &#34;&#34;&#34;
        raise NotImplementedError()

    def Ksub_diag(self, i, X1):
        &#34;&#34;&#34;
        Calculate the diagonal of the kernel matrix between two channels. This is usually faster than `Ksub(X1).diagonal()`.

        Args:
            X1 (torch.tensor): Input of shape (data_points,input_dims).

        Returns:
            torch.tensor: Kernel matrix diagonal of shape (data_points,).
        &#34;&#34;&#34;
        return self.Ksub(i, i, X1).diagonal()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.kernel.Kernel" href="#mogptk.gpr.kernel.Kernel">Kernel</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="mogptk.gpr.multioutput.CrossSpectralKernel" href="multioutput.html#mogptk.gpr.multioutput.CrossSpectralKernel">CrossSpectralKernel</a></li>
<li><a title="mogptk.gpr.multioutput.GaussianConvolutionProcessKernel" href="multioutput.html#mogptk.gpr.multioutput.GaussianConvolutionProcessKernel">GaussianConvolutionProcessKernel</a></li>
<li><a title="mogptk.gpr.multioutput.IndependentMultiOutputKernel" href="multioutput.html#mogptk.gpr.multioutput.IndependentMultiOutputKernel">IndependentMultiOutputKernel</a></li>
<li><a title="mogptk.gpr.multioutput.LinearModelOfCoregionalizationKernel" href="multioutput.html#mogptk.gpr.multioutput.LinearModelOfCoregionalizationKernel">LinearModelOfCoregionalizationKernel</a></li>
<li><a title="mogptk.gpr.multioutput.MultiOutputHarmonizableSpectralKernel" href="multioutput.html#mogptk.gpr.multioutput.MultiOutputHarmonizableSpectralKernel">MultiOutputHarmonizableSpectralKernel</a></li>
<li><a title="mogptk.gpr.multioutput.MultiOutputSpectralKernel" href="multioutput.html#mogptk.gpr.multioutput.MultiOutputSpectralKernel">MultiOutputSpectralKernel</a></li>
<li><a title="mogptk.gpr.multioutput.MultiOutputSpectralMixtureKernel" href="multioutput.html#mogptk.gpr.multioutput.MultiOutputSpectralMixtureKernel">MultiOutputSpectralMixtureKernel</a></li>
<li><a title="mogptk.gpr.multioutput.UncoupledMultiOutputSpectralKernel" href="multioutput.html#mogptk.gpr.multioutput.UncoupledMultiOutputSpectralKernel">UncoupledMultiOutputSpectralKernel</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mogptk.gpr.kernel.MultiOutputKernel.Ksub"><code class="name flex">
<span>def <span class="ident">Ksub</span></span>(<span>self, i, j, X1, X2=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate kernel matrix between two channels. If <code>X2</code> is not given, it is assumed to be the same as <code>X1</code>. Not passing <code>X2</code> may be faster for some kernels.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X1</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Input of shape (data_points0,input_dims).</dd>
<dt><strong><code>X2</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Input of shape (data_points1,input_dims).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.tensor</code></dt>
<dd>Kernel matrix of shape (data_points0,data_points1).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L412-L423" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def Ksub(self, i, j, X1, X2=None):
    &#34;&#34;&#34;
    Calculate kernel matrix between two channels. If `X2` is not given, it is assumed to be the same as `X1`. Not passing `X2` may be faster for some kernels.

    Args:
        X1 (torch.tensor): Input of shape (data_points0,input_dims).
        X2 (torch.tensor): Input of shape (data_points1,input_dims).

    Returns:
        torch.tensor: Kernel matrix of shape (data_points0,data_points1).
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="mogptk.gpr.kernel.MultiOutputKernel.Ksub_diag"><code class="name flex">
<span>def <span class="ident">Ksub_diag</span></span>(<span>self, i, X1)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the diagonal of the kernel matrix between two channels. This is usually faster than <code>Ksub(X1).diagonal()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X1</code></strong> :&ensp;<code>torch.tensor</code></dt>
<dd>Input of shape (data_points,input_dims).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.tensor</code></dt>
<dd>Kernel matrix diagonal of shape (data_points,).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/60aea73510361cb7d2651924e80c4dcbbd6b8a96/mogptk/gpr/kernel.py#L425-L435" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def Ksub_diag(self, i, X1):
    &#34;&#34;&#34;
    Calculate the diagonal of the kernel matrix between two channels. This is usually faster than `Ksub(X1).diagonal()`.

    Args:
        X1 (torch.tensor): Input of shape (data_points,input_dims).

    Returns:
        torch.tensor: Kernel matrix diagonal of shape (data_points,).
    &#34;&#34;&#34;
    return self.Ksub(i, i, X1).diagonal()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mogptk.gpr.kernel.Kernel" href="#mogptk.gpr.kernel.Kernel">Kernel</a></b></code>:
<ul class="hlist">
<li><code><a title="mogptk.gpr.kernel.Kernel.K" href="#mogptk.gpr.kernel.Kernel.K">K</a></code></li>
<li><code><a title="mogptk.gpr.kernel.Kernel.K_diag" href="#mogptk.gpr.kernel.Kernel.K_diag">K_diag</a></code></li>
<li><code><a title="mogptk.gpr.kernel.Kernel.iterkernels" href="#mogptk.gpr.kernel.Kernel.iterkernels">iterkernels</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="mogptk.gpr" href="index.html">mogptk.gpr</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="mogptk.gpr.kernel.AddKernel" href="#mogptk.gpr.kernel.AddKernel">AddKernel</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.kernel.AutomaticRelevanceDeterminationKernel" href="#mogptk.gpr.kernel.AutomaticRelevanceDeterminationKernel">AutomaticRelevanceDeterminationKernel</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.kernel.Kernel" href="#mogptk.gpr.kernel.Kernel">Kernel</a></code></h4>
<ul class="two-column">
<li><code><a title="mogptk.gpr.kernel.Kernel.K" href="#mogptk.gpr.kernel.Kernel.K">K</a></code></li>
<li><code><a title="mogptk.gpr.kernel.Kernel.K_diag" href="#mogptk.gpr.kernel.Kernel.K_diag">K_diag</a></code></li>
<li><code><a title="mogptk.gpr.kernel.Kernel.active_dims" href="#mogptk.gpr.kernel.Kernel.active_dims">active_dims</a></code></li>
<li><code><a title="mogptk.gpr.kernel.Kernel.average" href="#mogptk.gpr.kernel.Kernel.average">average</a></code></li>
<li><code><a title="mogptk.gpr.kernel.Kernel.clone" href="#mogptk.gpr.kernel.Kernel.clone">clone</a></code></li>
<li><code><a title="mogptk.gpr.kernel.Kernel.distance" href="#mogptk.gpr.kernel.Kernel.distance">distance</a></code></li>
<li><code><a title="mogptk.gpr.kernel.Kernel.iterkernels" href="#mogptk.gpr.kernel.Kernel.iterkernels">iterkernels</a></code></li>
<li><code><a title="mogptk.gpr.kernel.Kernel.name" href="#mogptk.gpr.kernel.Kernel.name">name</a></code></li>
<li><code><a title="mogptk.gpr.kernel.Kernel.squared_distance" href="#mogptk.gpr.kernel.Kernel.squared_distance">squared_distance</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mogptk.gpr.kernel.Kernels" href="#mogptk.gpr.kernel.Kernels">Kernels</a></code></h4>
<ul class="">
<li><code><a title="mogptk.gpr.kernel.Kernels.name" href="#mogptk.gpr.kernel.Kernels.name">name</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mogptk.gpr.kernel.MixtureKernel" href="#mogptk.gpr.kernel.MixtureKernel">MixtureKernel</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.kernel.MulKernel" href="#mogptk.gpr.kernel.MulKernel">MulKernel</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.gpr.kernel.MultiOutputKernel" href="#mogptk.gpr.kernel.MultiOutputKernel">MultiOutputKernel</a></code></h4>
<ul class="">
<li><code><a title="mogptk.gpr.kernel.MultiOutputKernel.Ksub" href="#mogptk.gpr.kernel.MultiOutputKernel.Ksub">Ksub</a></code></li>
<li><code><a title="mogptk.gpr.kernel.MultiOutputKernel.Ksub_diag" href="#mogptk.gpr.kernel.MultiOutputKernel.Ksub_diag">Ksub_diag</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>