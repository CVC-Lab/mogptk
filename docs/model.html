<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>mogptk.model API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{display:none;font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>mogptk.model</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L1-L875" class="git-link">Browse git</a>
</summary>
<pre><code class="python">import os
import time
import math
import pickle
import inspect
import numpy as np
import torch
import logging
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from mpl_toolkits.axes_grid1 import make_axes_locatable

from . import gpr
from .dataset import DataSet
from .errors import *

logger = logging.getLogger(&#39;mogptk&#39;)

def LoadModel(filename):
    &#34;&#34;&#34;
    Load model from a given file that was previously saved with `model.save()`.

    Args:
        filename (str): File name to load from.

    Examples:
        &gt;&gt;&gt; LoadModel(&#39;filename&#39;)
    &#34;&#34;&#34;
    filename += &#34;.npy&#34; 
    with open(filename, &#39;rb&#39;) as r:
        return pickle.load(r)

class Exact:
    &#34;&#34;&#34;
    Exact inference for Gaussian process regression.

    Args:
        variance (float): Variance of the Gaussian likelihood.
        jitter (float): Jitter added before calculating a Cholesky.
    &#34;&#34;&#34;
    def __init__(self, variance=None, data_variance=None, jitter=1e-8):
        self.variance = variance
        self.data_variance = data_variance
        self.jitter = jitter

    def _build(self, kernel, x, y, y_err=None, mean=None, name=None):
        variance = self.variance
        if variance is None:
            if kernel.output_dims is not None:
                variance = [1.0] * kernel.output_dims
            else:
                variance = 1.0
        data_variance = self.data_variance
        if data_variance is None and y_err is not None:
            data_variance = y_err**2
        model = gpr.Exact(kernel, x, y, variance=variance, data_variance=data_variance, jitter=self.jitter, mean=mean, name=name)
        if data_variance is not None:
            model.likelihood.scale.assign(0.0, trainable=False)
        return model

class Snelson:
    &#34;&#34;&#34;
    Inference using Snelson and Ghahramani 2005 for Gaussian process regression.

    Args:
        inducing_points (int,list): Number of inducing points or the locations of the inducing points.
        init_inducing_points (str): Method for initialization of inducing points, can be `grid`, `random`, or `density`.
        variance (float): Variance of the Gaussian likelihood.
        jitter (float): Jitter added before calculating a Cholesky.
    &#34;&#34;&#34;
    def __init__(self, inducing_points=10, init_inducing_points=&#39;grid&#39;, variance=None, jitter=1e-6):
        self.inducing_points = inducing_points
        self.init_inducing_points = init_inducing_points
        self.variance = variance
        self.jitter = jitter

    def _build(self, kernel, x, y, y_err=None, mean=None, name=None):
        if variance is None:
            variance = [1.0] * kernel.output_dims
        return gpr.Snelson(kernel, x, y, Z=self.inducing_points, Z_init=self.init_inducing_points, variance=self.variance, jitter=self.jitter, mean=mean, name=name)

class OpperArchambeau:
    &#34;&#34;&#34;
    Inference using Opper and Archambeau 2009 for Gaussian process regression.

    Args:
        likelihood (gpr.Likelihood): Likelihood $p(y|f)$.
        jitter (float): Jitter added before calculating a Cholesky.
    &#34;&#34;&#34;
    def __init__(self, likelihood=gpr.GaussianLikelihood(1.0), jitter=1e-6):
        self.likelihood = likelihood
        self.jitter = jitter

    def _build(self, kernel, x, y, y_err=None, mean=None, name=None):
        return gpr.OpperArchambeau(kernel, x, y, likelihood=likelihood, jitter=self.jitter, mean=mean, name=name)

class Titsias:
    &#34;&#34;&#34;
    Inference using Titsias 2009 for Gaussian process regression.

    Args:
        inducing_points (int,list): Number of inducing points or the locations of the inducing points.
        init_inducing_points (str): Method for initialization of inducing points, can be `grid`, `random`, or `density`.
        variance (float): Variance of the Gaussian likelihood.
        jitter (float): Jitter added before calculating a Cholesky.
    &#34;&#34;&#34;
    def __init__(self, inducing_points=10, init_inducing_points=&#39;grid&#39;, variance=1.0, jitter=1e-6):
        self.inducing_points = inducing_points
        self.init_inducing_points = init_inducing_points
        self.variance = variance
        self.jitter = jitter

    def _build(self, kernel, x, y, y_err=None, mean=None, name=None):
        return gpr.Titsias(kernel, x, y, Z=self.inducing_points, Z_init=self.init_inducing_points, variance=self.variance, jitter=self.jitter, mean=mean, name=name)

class Hensman:
    &#34;&#34;&#34;
    Inference using Hensman 2015 for Gaussian process regression.

    Args:
        inducing_points (int,list): Number of inducing points or the locations of the inducing points. By default the non-sparse Hensman model is used.
        init_inducing_points (str): Method for initialization of inducing points, can be `grid`, `random`, or `density`.
        likelihood (gpr.Likelihood): Likelihood $p(y|f)$.
        jitter (float): Jitter added before calculating a Cholesky.
    &#34;&#34;&#34;
    def __init__(self, inducing_points=None, init_inducing_points=&#39;grid&#39;, likelihood=gpr.GaussianLikelihood(1.0), jitter=1e-6):
        self.inducing_points = inducing_points
        self.init_inducing_points = init_inducing_points
        self.likelihood = likelihood
        self.jitter = jitter

    def _build(self, kernel, x, y, y_err=None, mean=None, name=None):
        if self.inducing_points is None:
            return gpr.Hensman(kernel, x, y, likelihood=self.likelihood, jitter=self.jitter, mean=mean, name=name)
        return gpr.SparseHensman(kernel, x, y, Z=self.inducing_points, Z_init=self.init_inducing_points, likelihood=self.likelihood, jitter=self.jitter, mean=mean, name=name)

class Model:
    def __init__(self, dataset, kernel, inference=Exact(), mean=None, name=None):
        &#34;&#34;&#34;
        Model is the base class for multi-output Gaussian process models.

        Args:
            dataset (mogptk.dataset.DataSet, mogptk.data.Data): `DataSet` with `Data` objects for all the channels. When a (list or dict of) `Data` object is passed, it will automatically be converted to a `DataSet`.
            kernel (mogptk.gpr.kernel.Kernel): The kernel class.
            model: Gaussian process model to use, such as `mogptk.model.Exact`.
            mean (mogptk.gpr.mean.Mean): The mean class.
            name (str): Name of the model.

        Atributes:
            dataset: The associated mogptk.dataset.DataSet.
            gpr: The mogptk.gpr.model.Model.
        &#34;&#34;&#34;
        
        if not isinstance(dataset, DataSet):
            dataset = DataSet(dataset)
        if dataset.get_output_dims() == 0:
            raise ValueError(&#34;dataset must have at least one channel&#34;)
        names = [name for name in dataset.get_names() if name is not None]
        if len(set(names)) != len(names):
            raise ValueError(&#34;all data channels must have unique names&#34;)

        for channel in dataset:
            for dim in range(channel.get_input_dims()):
                xran = np.max(channel.X.transformed[:,dim]) - np.min(channel.X.transformed[:,dim])
                if xran &lt; 1e-3:
                    logger.warning(&#34;Very small X range may give problems, it is suggested to scale up your X axis&#34;)
                elif 1e4 &lt; xran:
                    logger.warning(&#34;Very large X range may give problems, it is suggested to scale down your X axis&#34;)

        self.name = name
        self.dataset = dataset
        self.is_multioutput = kernel.output_dims is not None

        X, Y = self.dataset.get_train_data()
        x, y = self._to_kernel_format(X, Y)

        y_err = None
        if all(channel.Y_err is not None for channel in self.dataset):
            # TODO: doesn&#39;t transform...
            Y_err = [np.array(channel.Y_err[channel.mask]) for channel in self.dataset]
            Y_err_lower = [self.dataset[j].Y.transform(Y[j] - Y_err[j], X[j]) for j in range(len(self.dataset))]
            Y_err_upper = [self.dataset[j].Y.transform(Y[j] + Y_err[j], X[j]) for j in range(len(self.dataset))]
            y_err_lower = np.concatenate(Y_err_lower, axis=0)
            y_err_upper = np.concatenate(Y_err_upper, axis=0)
            y_err = (y_err_upper-y_err_lower)/2.0 # TODO: strictly incorrect: takes average error after transformation
        self.gpr = inference._build(kernel, x, y, y_err, mean, name)

        self.times = np.zeros(0)
        self.losses = np.zeros(0)
        self.errors = np.zeros(0)

    ################################################################

    def print_parameters(self):
        &#34;&#34;&#34;
        Print the parameters of the model in a table.

        Examples:
            &gt;&gt;&gt; model.print_parameters()
        &#34;&#34;&#34;
        self.gpr.print_parameters()

    def get_parameters(self):
        &#34;&#34;&#34;
        Returns all parameters of the kernel.

        Returns:
            list: mogptk.gpr.parameter.Parameter

        Examples:
            &gt;&gt;&gt; params = model.get_parameters()
        &#34;&#34;&#34;
        return self.gpr.get_parameters()

    def copy_parameters(self, other):
        &#34;&#34;&#34;
        Copy the kernel parameters from another model.
        &#34;&#34;&#34;
        if not isinstance(other, Model):
            raise ValueError(&#34;other must be of type Model&#34;)

        self.gpr.kernel.copy_parameters(other.kernel)

    def save(self, filename):
        &#34;&#34;&#34;
        Save the model to a given file that can then be loaded using `LoadModel()`.

        Args:
            filename (str): File name to save to, automatically appends &#39;.npy&#39;.

        Examples:
            &gt;&gt;&gt; model.save(&#39;filename&#39;)
        &#34;&#34;&#34;
        filename += &#34;.npy&#34; 
        try:
            os.remove(filename)
        except OSError:
            pass
        with open(filename, &#39;wb&#39;) as w:
            pickle.dump(self, w)

    def log_marginal_likelihood(self):
        &#34;&#34;&#34;
        Returns the log marginal likelihood of the kernel and its data and parameters.

        Returns:
            float: The current log marginal likelihood.

        Examples:
            &gt;&gt;&gt; model.log_marginal_likelihood()
        &#34;&#34;&#34;
        return self.gpr.log_marginal_likelihood().detach().cpu().item()

    def loss(self):
        &#34;&#34;&#34;
        Returns the loss of the kernel and its data and parameters.

        Returns:
            float: The current loss.

        Examples:
            &gt;&gt;&gt; model.loss()
        &#34;&#34;&#34;
        return self.gpr.loss().detach().cpu().item()

    def error(self, method=&#39;MAE&#39;, use_all_data=False):
        &#34;&#34;&#34;
        Returns the error of the kernel prediction with the removed data points in the data set.

        Args:
            method (str,function): Error calculation method, such as MAE, MAPE, sMAPE, MSE, or RMSE. When a function is given, it should have parameters (y_true,y_pred) or (y_true,y_pred,model).

        Returns:
            float: The current error.

        Examples:
            &gt;&gt;&gt; model.error()
        &#34;&#34;&#34;
        if use_all_data:
            X, Y_true = self.dataset.get_data()
        else:
            X, Y_true = self.dataset.get_test_data()
        x, y_true  = self._to_kernel_format(X, Y_true)
        y_pred, _ = self.gpr.predict(x, predict_y=False)
        if callable(method):
            if len(inspect.signature(method).parameters) == 3:
                return method(y_true, y_pred, self)
            return method(y_true, y_pred)
        elif method.lower() == &#39;mae&#39;:
            return mean_absolute_error(y_true, y_pred)
        elif method.lower() == &#39;mape&#39;:
            return mean_absolute_percentage_error(y_true, y_pred)
        elif method.lower() == &#39;smape&#39;:
            return symmetric_mean_absolute_percentage_error(y_true, y_pred)
        elif method.lower() == &#39;mse&#39;:
            return mean_squared_error(y_true, y_pred)
        elif method.lower() == &#39;rmse&#39;:
            return root_mean_squared_error(y_true, y_pred)
        else:
            raise ValueError(&#34;valid error calculation methods are MAE, MAPE, sMAPE, MSE, and RMSE&#34;)

    def train(
        self,
        method=&#39;Adam&#39;,
        iters=500,
        verbose=False,
        error=None,
        plot=False,
        **kwargs):
        &#34;&#34;&#34;
        Trains the model by optimizing the (hyper)parameters of the kernel to approach the training data.

        Args:
            method (str): Optimizer to use such as LBFGS, Adam, Adagrad, or SGD.
            iters (int): Number of iterations, or maximum in case of LBFGS optimizer.
            verbose (bool): Print verbose output about the state of the optimizer.
            error (str,function): Calculate prediction error for each iteration by the given method, such as MAE, MAPE, sMAPE, MSE, or RMSE. When a function is given, it should have parameters (y_true,y_pred) or (y_true,y_pred,model).
            plot (bool): Plot the loss and, if error is data set, the error of the test data points.
            **kwargs (dict): Additional dictionary of parameters passed to the PyTorch optimizer. 

        Returns:
            numpy.ndarray: Losses for all iterations.
            numpy.ndarray: Errors for all iterations. Only if `error` is set, otherwise zero.

        Examples:
            &gt;&gt;&gt; model.train()
            
            &gt;&gt;&gt; model.train(method=&#39;lbfgs&#39;, tolerance_grad=1e-10, tolerance_change=1e-12)
            
            &gt;&gt;&gt; model.train(method=&#39;adam&#39;, lr=0.5)
        &#34;&#34;&#34;
        error_use_all_data = False
        if error is not None and all(not channel.has_test_data() for channel in self.dataset):
            error_use_all_data = True

        if method.lower() in (&#39;l-bfgs&#39;, &#39;lbfgs&#39;, &#39;l-bfgs-b&#39;, &#39;lbfgsb&#39;):
            method = &#39;LBFGS&#39;
        elif method.lower() == &#39;adam&#39;:
            method = &#39;Adam&#39;
        elif method.lower() == &#39;sgd&#39;:
            method = &#39;SGD&#39;
        elif method.lower() == &#39;adagrad&#39;:
            method = &#39;AdaGrad&#39;
        else:
            raise ValueError(&#39;optimizer must be LBFGS, Adam, SGD, or AdaGrad&#39;)

        if verbose:
            training_points = sum([len(channel.get_train_data()[1]) for channel in self.dataset])
            parameters = sum([p.num_parameters if p.trainable else 0 for p in self.gpr.get_parameters()])
            print(&#39;\nStarting optimization using&#39;, method)
            if self.name is not None:
                print(&#39;‣ Model: %s&#39; % self.name)
            print(&#39;‣ Channels: %d&#39; % len(self.dataset))
            print(&#39;‣ Parameters: %d&#39; % parameters)
            print(&#39;‣ Training points: %d&#39; % training_points)
            print(&#39;‣ Initial loss: %6g&#39; % self.loss())
            if error is not None:
                print(&#39;‣ Initial error: %6g&#39; % self.error(error, error_use_all_data))

        iter_offset = 0
        times = np.zeros((iters+1,))
        losses = np.zeros((iters+1,))
        errors = np.zeros((iters+1,))
        if self.times.shape[0] != 0:
            iter_offset = self.times.shape[0]-1
            times = np.concatenate((self.times[:-1],times))
            losses = np.concatenate((self.losses[:-1],losses))
            errors = np.concatenate((self.errors[:-1],errors))
        initial_time = time.time()

        iters_len = int(math.log10(iter_offset+iters)) + 1
        def progress(i, loss):
            elapsed_time = time.time() - initial_time
            write = verbose and i % max(1,iters/100) == 0
            i += iter_offset
            times[i] = elapsed_time
            losses[i] = loss
            if error is not None:
                errors[i] = self.error(error, error_use_all_data)
                if write:
                    print(&#34;  %*d/%*d %s  loss=%12g  error=%12g&#34; % (iters_len, i, iters_len, iter_offset+iters, _format_time(elapsed_time), losses[i], errors[i]))
            elif write:
                print(&#34;  %*d/%*d %s  loss=%12g&#34; % (iters_len, i, iters_len, iter_offset+iters, _format_time(elapsed_time), losses[i]))

        if verbose:
            print(&#34;\nStart %s:&#34; % (method,))
        if method == &#39;LBFGS&#39;:
            if not &#39;max_iter&#39; in kwargs:
                kwargs[&#39;max_iter&#39;] = iters
            else:
                iters = kwargs[&#39;max_iter&#39;]
            optimizer = torch.optim.LBFGS(self.gpr.parameters(), **kwargs)

            def loss():
                i = int(optimizer.state_dict()[&#39;state&#39;][0][&#39;func_evals&#39;])
                loss = self.loss()
                progress(i, loss)
                return loss
            optimizer.step(loss)
            iters = int(optimizer.state_dict()[&#39;state&#39;][0][&#39;func_evals&#39;])
        else:
            if method == &#39;Adam&#39;:
                optimizer = torch.optim.Adam(self.gpr.parameters(), **kwargs)
            elif method == &#39;SGD&#39;:
                optimizer = torch.optim.SGD(self.gpr.parameters(), **kwargs)
            elif method == &#39;AdaGrad&#39;:
                optimizer = torch.optim.Adagrad(self.gpr.parameters(), **kwargs)

            for i in range(iters):
                progress(i, self.loss())
                optimizer.step()
        progress(iters, self.loss())

        if verbose:
            elapsed_time = time.time() - initial_time
            print(&#34;Finished&#34;)
            print(&#39;\nOptimization finished in %s&#39; % _format_duration(elapsed_time))
            print(&#39;‣ Iterations: %d&#39; % iters)
            print(&#39;‣ Final loss: %6g&#39;% losses[iter_offset+iters])
            if error is not None:
                print(&#39;‣ Final error: %6g&#39; % errors[iter_offset+iters])

        self.iters = iter_offset+iters
        self.times = times[:iter_offset+iters+1]
        self.losses = losses[:iter_offset+iters+1]
        if error is not None:
            self.errors = errors[:iter_offset+iters+1]
        if plot:
            self.plot_losses()
        return losses, errors

    ################################################################################
    # Predictions ##################################################################
    ################################################################################

    def _to_kernel_format(self, X, Y=None):
        &#34;&#34;&#34;
        Return the data vectors in the format used by the kernels. If Y is not passed, than only X data is returned.

        Returns:
            numpy.ndarray: X data of shape (data_points,input_dims). If the kernel is multi output, an additional input dimension is prepended with the channel indices.
            numpy.ndarray: Y data of shape (data_points,1).
            numpy.ndarray: Original but normalized X data. Only if no Y is passed.
        &#34;&#34;&#34;
        X = X.copy()
        for j, channel_x in enumerate(X):
            X[j] = X[j].transformed

        x = np.concatenate(X, axis=0)
        if self.is_multioutput:
            chan = [j * np.ones(len(X[j])) for j in range(len(X))]
            chan = np.concatenate(chan).reshape(-1, 1)
            x = np.concatenate([chan, x], axis=1)
        if Y is None:
            return x

        Y = Y.copy()
        for j, channel_y in enumerate(Y):
            Y[j] = Y[j].transformed
        y = np.concatenate(Y, axis=0).reshape(-1, 1)
        return x, y

    def predict(self, X=None, sigma=2, transformed=False, predict_y=True):
        &#34;&#34;&#34;
        Predict using the prediction range of the data set and save the prediction in that data set. Otherwise, if `X` is passed, use that as the prediction range and return the prediction instead of saving it.

        Args:
            X (list, dict): Dictionary where keys are channel index and elements numpy arrays with channel inputs. If passed, results will be returned and not saved in the data set for later retrieval.
            sigma (float): Number of standard deviations to display upwards and downwards.
            transformed (boolean): Return transformed data as used for training.

        Returns:
            numpy.ndarray: X prediction of shape (n,) for each channel.
            numpy.ndarray: Y mean prediction of shape (n,) for each channel.
            numpy.ndarray: Y lower prediction of uncertainty interval of shape (n,) for each channel.
            numpy.ndarray: Y upper prediction of uncertainty interval of shape (n,) for each channel.

        Examples:
            &gt;&gt;&gt; model.predict(X)
        &#34;&#34;&#34;
        if X is None:
            X = self.dataset.get_prediction_data()
        else:
            X = self.dataset._format_X(X)
        x = self._to_kernel_format(X)

        mu, var = self.gpr.predict(x, predict_y=predict_y, tensor=True)
        lower = mu - sigma*torch.sqrt(var)
        upper = mu + sigma*torch.sqrt(var)

        mu = mu.cpu().numpy()
        var = var.cpu().numpy()
        lower = lower.cpu().numpy()
        upper = upper.cpu().numpy()

        i = 0
        Mu = []
        Var = []
        Lower = []
        Upper = []
        for j in range(self.dataset.get_output_dims()):
            N = X[j].shape[0]
            Mu.append(np.squeeze(mu[i:i+N]))
            Var.append(np.squeeze(var[i:i+N]))
            Lower.append(np.squeeze(lower[i:i+N]))
            Upper.append(np.squeeze(upper[i:i+N]))
            i += N

        if not transformed:
            for j in range(self.dataset.get_output_dims()):
                Mu[j] = self.dataset[j].Y.detransform(Mu[j], X[j])
                Lower[j] = self.dataset[j].Y.detransform(Lower[j], X[j])
                Upper[j] = self.dataset[j].Y.detransform(Upper[j], X[j])

        if len(self.dataset) == 1:
            return X[0], Mu[0], Lower[0], Upper[0]
        return X, Mu, Lower, Upper

    def K(self, X1, X2=None):
        &#34;&#34;&#34;
        Evaluate the kernel at K(X1,X2).

        Args:
            X1 (list, dict): Dictionary where keys are channel index and elements numpy arrays with channel inputs.
            X2 (list, dict): Same as X1 if None.

        Returns:
            numpy.ndarray: kernel evaluated at X1 and X2 of shape (n1,n2).

        Examples:
            &gt;&gt;&gt; channel0 = np.array([&#39;1987-05-20&#39;, &#39;1987-05-21&#39;])
            &gt;&gt;&gt; channel1 = np.array([[2.5, 534.6], [3.5, 898.22], [4.5, 566.98]])
            &gt;&gt;&gt; model.K([channel0,channel1])
        &#34;&#34;&#34;
        X1 = self.dataset._format_X(X1)
        x1 = self._to_kernel_format(X1)
        if X2 is None:
            return self.gpr.K(x1)
        else:
            X2 = self.dataset._format_X(X2)
            x2 = self._to_kernel_format(X2)
            return self.gpr.K(x1, x2)

    def sample(self, X=None, n=None, predict_y=True, transformed=False):
        &#34;&#34;&#34;
        Sample n times from the kernel at input X .

        Args:
            X (list, dict): Dictionary where keys are channel index and elements numpy arrays with channel inputs.
            n (int): Number of samples.
            predict_y (boolean): Predict data values instead of function values.
            transformed (boolean): Return transformed data as used for training.

        Returns:
            list: samples of shape len(X) for each channel if n is given.
            numpy.ndarray: sample of shape len(X) for each channel if n is None.

        Examples:
            &gt;&gt;&gt; model.sample(n=10)
        &#34;&#34;&#34;
        if X is None:
            X = self.dataset.get_prediction_data()
        else:
            X = self.dataset._format_X(X)
        x = self._to_kernel_format(X)

        samples = self.gpr.sample(Z=x, n=n, predict_y=predict_y)

        i = 0
        Samples = []
        for j in range(self.dataset.get_output_dims()):
            N = X[j][0].shape[0]
            if n is None:
                sample = np.squeeze(samples[i:i+N])
                if not transformed:
                    sample = self.dataset[j].Y.detransform(sample, X[j])
                Samples.append(sample)
            else:
                ss = []
                for k in range(n):
                    sample = np.squeeze(samples[i:i+N,k])
                    if not transformed:
                        sample = self.dataset[j].Y.detransform(sample, X[j])
                    ss.append(sample)
                Samples.append(ss)
            i += N
        if self.dataset.get_output_dims() == 1:
            return Samples[0]
        return Samples

    def plot_losses(self, title=None, figsize=(18,6), legend=True, errors=True, log=False):
        &#34;&#34;&#34;
        Plot the losses and errors during training. In order to display the errors, make sure to set the error parameter when training.

        Args:
            title (str): Figure title.
            figsize (tuple): Figure size.
            legend (boolean): Show the legend.
            errors (boolean): Show the errors.
            log (boolean): Show in log scale.

        Returns:
            figure: Matplotlib figure.
            axis: Matplotlib axis.
        &#34;&#34;&#34;
        if not hasattr(self, &#39;losses&#39;):
            raise Exception(&#34;must be trained in order to plot the losses&#34;)

        fig, ax = plt.subplots(1, 1, figsize=figsize, constrained_layout=True)
        x = np.arange(0,self.iters+1)
        ax.set_xlim(0, self.iters)
        ax.set_xlabel(&#39;Iteration&#39;)
        ax.set_ylabel(&#39;Loss&#39;)
        if log:
            ax.set_yscale(&#39;log&#39;)

        ax.plot(x, self.losses, c=&#39;k&#39;, ls=&#39;-&#39;)

        legends = []
        legends.append(plt.Line2D([0], [0], ls=&#39;-&#39;, color=&#39;k&#39;, label=&#39;Loss&#39;))
        if errors and hasattr(self, &#39;errors&#39;):
            ax2 = ax.twinx()
            ax2.plot(x, self.errors, c=&#39;k&#39;, ls=&#39;-.&#39;)
            ax2.set_ylabel(&#39;Error&#39;)
            legends.append(plt.Line2D([0], [0], ls=&#39;-.&#39;, color=&#39;k&#39;, label=&#39;Error&#39;))
            if log:
                ax2.set_yscale(&#39;log&#39;)

        if title is not None:
            fig.suptitle(title, fontsize=18)

        if legend:
            ax.legend(handles=legends)
        return fig, ax

    def plot_prediction(self, X=None, title=None, figsize=None, legend=True, errorbars=True, predict_y=True, transformed=False):
        &#34;&#34;&#34;
        Plot the data including removed observations, latent function, and predictions of this model for each channel.

        Args:
            title (str): Set the title of the plot.
            figsize (tuple): Set the figure size.
            legend (boolean): Disable legend.
            errorbars (boolean): Plot data error bars if available.
            predict_y (boolean): Predict data values instead of function values.
            transformed (boolean): Display transformed Y data as used for training.

        Returns:
            matplotlib.figure.Figure: The figure.
            list of matplotlib.axes.Axes: List of axes.

        Examples:
            &gt;&gt;&gt; fig, axes = dataset.plot(title=&#39;Title&#39;)
        &#34;&#34;&#34;
        X, Mu, Lower, Upper = self.predict(X, predict_y=predict_y)
        if len(self.dataset) == 1:
            X = [X]
            Mu = [Mu]
            Lower = [Lower]
            Upper = [Upper]

        fig, ax = plt.subplots(len(self.dataset), 1, figsize=(18,6*len(self.dataset)), squeeze=True, constrained_layout=True)
        for j, data in enumerate(self.dataset):
            # TODO: ability to plot conditional or marginal distribution to reduce input dims
            if data.get_input_dims() &gt; 2:
                raise ValueError(&#34;cannot plot more than two input dimensions&#34;)
            if data.get_input_dims() == 2:
                raise NotImplementedError(&#34;two dimensional input data not yet implemented&#34;) # TODO

            legends = []
            if errorbars and data.Y_err is not None:
                x, y = data.get_train_data(transformed=transformed)
                yl = data.Y[data.mask] - data.Y_err[data.mask]
                yu = data.Y[data.mask] + data.Y_err[data.mask]
                if transformed:
                    yl = data.Y.transform(yl, x)
                    yu = data.Y.transform(yu, x)
                ax[j].errorbar(x, y, [y-yl, yu-y], elinewidth=1.5, ecolor=&#39;lightgray&#39;, capsize=0, ls=&#39;&#39;, marker=&#39;&#39;)

            # prediction
            idx = np.argsort(X[j][:,0])
            ax.plot(X[j][idx,0], Mu[j][idx], ls=&#39;:&#39;, color=&#39;blue&#39;, lw=2)
            ax.fill_between(X[j][idx,0], Lower[j][idx], Upper[j][idx], color=&#39;blue&#39;, alpha=0.3)
            label = &#39;Posterior Mean&#39;
            legends.append(patches.Rectangle(
                (1, 1), 1, 1, fill=True, color=&#39;blue&#39;, alpha=0.3, lw=0, label=&#39;95% Error Bars&#39;
            ))
            legends.append(plt.Line2D([0], [0], ls=&#39;:&#39;, color=&#39;blue&#39;, lw=2, label=label))

            xmin = min(np.min(data.X), np.min(X[j]))
            xmax = max(np.max(data.X), np.max(X[j]))
            if data.F is not None:
                if np.issubdtype(data.X.dtypes[0], np.datetime64):
                    dt = np.timedelta64(1,data.X.get_time_unit())
                    n = int((xmax-xmin) / dt) + 1
                    x = np.arange(xmin, xmax+np.timedelta64(1,&#39;us&#39;), dt, dtype=data.X.dtypes[0])
                else:
                    n = len(data.X)*10
                    x = np.linspace(xmin, xmax, n)

                y = data.F(x)
                if transformed:
                    y = data.Y.transform(y, x)

                ax.plot(x, y, &#39;r--&#39;, lw=1)
                legends.append(plt.Line2D([0], [0], ls=&#39;--&#39;, color=&#39;r&#39;, label=&#39;True&#39;))

            if data.has_test_data():
                x, y = data.get_test_data(transformed=transformed)
                ax.plot(x, y, &#39;g.&#39;, ms=10)
                legends.append(plt.Line2D([0], [0], ls=&#39;&#39;, color=&#39;g&#39;, marker=&#39;.&#39;, ms=10, label=&#39;Latent&#39;))

            x, y = data.get_train_data(transformed=transformed)
            ax.plot(x, y, &#39;r.&#39;, ms=10)
            legends.append(plt.Line2D([0], [0], ls=&#39;&#39;, color=&#39;r&#39;, marker=&#39;.&#39;, ms=10, label=&#39;Observations&#39;))

            if 0 &lt; len(data.removed_ranges[0]):
                for removed_range in data.removed_ranges[0]:
                    x0 = removed_range[0]
                    x1 = removed_range[1]
                    y0 = ax.get_ylim()[0]
                    y1 = ax.get_ylim()[1]
                    ax.add_patch(patches.Rectangle(
                        (x0, y0), x1-x0, y1-y0, fill=True, color=&#39;xkcd:strawberry&#39;, alpha=0.2, lw=0,
                    ))
                legends.append(patches.Rectangle(
                    (1, 1), 1, 1, fill=True, color=&#39;xkcd:strawberry&#39;, alpha=0.5, lw=0, label=&#39;Removed Ranges&#39;
                ))

            ax.set_xlim(xmin - (xmax - xmin)*0.001, xmax + (xmax - xmin)*0.001)
            ax.set_xlabel(data.X_labels[0])
            ax.set_ylabel(data.Y_label)
            ax.set_title(data.name if title is None else title, fontsize=14)

        if legend:
            ax.legend(handles=legends[::-1])
        return fig, ax

    def plot_gram(self, start=None, end=None, n=31, title=None, figsize=(18,18)):
        &#34;&#34;&#34;
        Plot the gram matrix of associated kernel.

        Args:
            start (float, list, array): Interval minimum.
            end (float, list, array): Interval maximum.
            n (int): Number of points per channel.
            title (str): Figure title.
            figsize (tuple): Figure size.

        Returns:
            figure: Matplotlib figure.
            axis: Matplotlib axis.
        &#34;&#34;&#34;
        if not all(channel.get_input_dims() == 1 for channel in self.dataset):
            raise ValueError(&#34;cannot plot for more than one input dimension&#34;)

        if start is None:
            start = [channel.X.transformed.min() for channel in self.dataset]
        if end is None:
            end = [channel.X.transformed.max() for channel in self.dataset]

        output_dims = len(self.dataset)
        if not isinstance(start, (list, np.ndarray)):
            start = [start] * output_dims
        if not isinstance(end, (list, np.ndarray)):
            end = [end] * output_dims

        X = np.zeros((output_dims*n, 2))
        X[:,0] = np.repeat(np.arange(output_dims), n)
        for j in range(output_dims):
            if n== 1:
                X[j*n:(j+1)*n,1] = np.array((start[j]+end[j])/2.0)
            else:
                X[j*n:(j+1)*n,1] = np.linspace(start[j], end[j], n)
        k = self.gpr.K(X)
            
        fig, ax = plt.subplots(1, 1, figsize=figsize, constrained_layout=True)
        if title is not None:
            fig.suptitle(title, fontsize=18)

        color_range = np.abs(k).max()
        norm = matplotlib.colors.Normalize(vmin=-color_range, vmax=color_range)
        im = ax.matshow(k, cmap=&#39;coolwarm&#39;, norm=norm)

        divider = make_axes_locatable(ax)
        cax = divider.append_axes(&#34;right&#34;, size=&#34;5%&#34;, pad=0.3)
        fig.colorbar(im, cax=cax)

        # Major ticks every 20, minor ticks every 5
        major_ticks = np.arange(-0.5, output_dims*n, n)
        minor_ticks = np.arange(-0.5, output_dims*n, 2)

        ax.set_xticks(major_ticks)
        ax.set_yticks(major_ticks)
        ax.grid(which=&#39;major&#39;, lw=1.5, c=&#39;k&#39;)
        ax.set_xticklabels([])
        ax.set_yticklabels([])
        ax.tick_params(axis=&#39;both&#39;, which=&#39;both&#39;, length=0)
        return fig, ax

    def plot_kernel(self, dist=None, n=101, title=None, figsize=(18,18)):
        &#34;&#34;&#34;
        Plot the kernel matrix at a range of data point distances for each channel for stationary kernels.

        Args:
            dist (list): Maximum distance for every channel.
            n (int): Number of points per channel.
            title (str): Figure title.
            figsize (tuple): Figure size.

        Returns:
            figure: Matplotlib figure.
            axis: Matplotlib axis.
        &#34;&#34;&#34;
        if not all(channel.get_input_dims() == 1 for channel in self.dataset):
            raise ValueError(&#34;cannot plot for more than one input dimension&#34;)

        if dist is None:
            dist = [(channel.X.transformed.max()-channel.X.transformed.min())/4.0 for channel in self.dataset]

        output_dims = len(self.dataset)
        if not isinstance(dist, (list, np.ndarray)):
            dist = [dist] * output_dims

        fig, ax = plt.subplots(output_dims, output_dims, figsize=figsize, constrained_layout=True, squeeze=False, sharex=True)
        if title is not None:
            fig.suptitle(title, fontsize=18)

        channel = np.ones((n,1))
        for j in range(output_dims):
            tau = np.linspace(-dist[j], dist[j], num=n).reshape(-1,1)
            X1 = np.array([[j,0.0]])
            for i in range(output_dims):
                if j &lt; i:
                    ax[j,i].set_axis_off()
                    continue

                X0 = np.concatenate((i*channel,tau), axis=1)
                k = self.gpr.K(X0,X1)
                ax[j,i].plot(tau, k, color=&#39;k&#39;)
                ax[j,i].set_yticks([])
        return fig, ax

def _format_duration(s):
    if s &lt; 60.0:
        return &#39;%.3f seconds&#39; % s

    s = math.floor(s)
    days = int(s/86400)
    hours = int(s%86400/3600)
    minutes = int(s%3600/60)
    seconds = int(s%60)

    duration = &#39;&#39;
    if 1 &lt; days:
        duration += &#39; %d days&#39; % days
    elif days == 1:
        duration += &#39; 1 day&#39;
    if 1 &lt; hours:
        duration += &#39; %d hours&#39; % hours
    elif hours == 1:
        duration += &#39; 1 hour&#39;
    if 1 &lt; minutes:
        duration += &#39; %d minutes&#39; % minutes
    elif minutes == 1:
        duration += &#39; 1 minute&#39;
    if 1 &lt; seconds:
        duration += &#39; %d seconds&#39; % seconds
    elif seconds == 1:
        duration += &#39; 1 second&#39;
    return duration[1:]

def _format_time(s):
    return &#34;%3d:%02d:%02d&#34; % (int(s/3600), int((s%3600)/60), int(s%60))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="mogptk.model.LoadModel"><code class="name flex">
<span>def <span class="ident">LoadModel</span></span>(<span>filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Load model from a given file that was previously saved with <code>model.save()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>File name to load from.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; LoadModel('filename')
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L20-L32" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def LoadModel(filename):
    &#34;&#34;&#34;
    Load model from a given file that was previously saved with `model.save()`.

    Args:
        filename (str): File name to load from.

    Examples:
        &gt;&gt;&gt; LoadModel(&#39;filename&#39;)
    &#34;&#34;&#34;
    filename += &#34;.npy&#34; 
    with open(filename, &#39;rb&#39;) as r:
        return pickle.load(r)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="mogptk.model.Exact"><code class="flex name class">
<span>class <span class="ident">Exact</span></span>
<span>(</span><span>variance=None, data_variance=None, jitter=1e-08)</span>
</code></dt>
<dd>
<div class="desc"><p>Exact inference for Gaussian process regression.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>variance</code></strong> :&ensp;<code>float</code></dt>
<dd>Variance of the Gaussian likelihood.</dd>
<dt><strong><code>jitter</code></strong> :&ensp;<code>float</code></dt>
<dd>Jitter added before calculating a Cholesky.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L34-L60" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Exact:
    &#34;&#34;&#34;
    Exact inference for Gaussian process regression.

    Args:
        variance (float): Variance of the Gaussian likelihood.
        jitter (float): Jitter added before calculating a Cholesky.
    &#34;&#34;&#34;
    def __init__(self, variance=None, data_variance=None, jitter=1e-8):
        self.variance = variance
        self.data_variance = data_variance
        self.jitter = jitter

    def _build(self, kernel, x, y, y_err=None, mean=None, name=None):
        variance = self.variance
        if variance is None:
            if kernel.output_dims is not None:
                variance = [1.0] * kernel.output_dims
            else:
                variance = 1.0
        data_variance = self.data_variance
        if data_variance is None and y_err is not None:
            data_variance = y_err**2
        model = gpr.Exact(kernel, x, y, variance=variance, data_variance=data_variance, jitter=self.jitter, mean=mean, name=name)
        if data_variance is not None:
            model.likelihood.scale.assign(0.0, trainable=False)
        return model</code></pre>
</details>
</dd>
<dt id="mogptk.model.Hensman"><code class="flex name class">
<span>class <span class="ident">Hensman</span></span>
<span>(</span><span>inducing_points=None, init_inducing_points='grid', likelihood=&lt;mogptk.gpr.likelihood.GaussianLikelihood object&gt;, jitter=1e-06)</span>
</code></dt>
<dd>
<div class="desc"><p>Inference using Hensman 2015 for Gaussian process regression.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inducing_points</code></strong> :&ensp;<code>int,list</code></dt>
<dd>Number of inducing points or the locations of the inducing points. By default the non-sparse Hensman model is used.</dd>
<dt><strong><code>init_inducing_points</code></strong> :&ensp;<code>str</code></dt>
<dd>Method for initialization of inducing points, can be <code>grid</code>, <code>random</code>, or <code>density</code>.</dd>
<dt><strong><code>likelihood</code></strong> :&ensp;<code>gpr.Likelihood</code></dt>
<dd>Likelihood $p(y|f)$.</dd>
<dt><strong><code>jitter</code></strong> :&ensp;<code>float</code></dt>
<dd>Jitter added before calculating a Cholesky.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L117-L136" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Hensman:
    &#34;&#34;&#34;
    Inference using Hensman 2015 for Gaussian process regression.

    Args:
        inducing_points (int,list): Number of inducing points or the locations of the inducing points. By default the non-sparse Hensman model is used.
        init_inducing_points (str): Method for initialization of inducing points, can be `grid`, `random`, or `density`.
        likelihood (gpr.Likelihood): Likelihood $p(y|f)$.
        jitter (float): Jitter added before calculating a Cholesky.
    &#34;&#34;&#34;
    def __init__(self, inducing_points=None, init_inducing_points=&#39;grid&#39;, likelihood=gpr.GaussianLikelihood(1.0), jitter=1e-6):
        self.inducing_points = inducing_points
        self.init_inducing_points = init_inducing_points
        self.likelihood = likelihood
        self.jitter = jitter

    def _build(self, kernel, x, y, y_err=None, mean=None, name=None):
        if self.inducing_points is None:
            return gpr.Hensman(kernel, x, y, likelihood=self.likelihood, jitter=self.jitter, mean=mean, name=name)
        return gpr.SparseHensman(kernel, x, y, Z=self.inducing_points, Z_init=self.init_inducing_points, likelihood=self.likelihood, jitter=self.jitter, mean=mean, name=name)</code></pre>
</details>
</dd>
<dt id="mogptk.model.Model"><code class="flex name class">
<span>class <span class="ident">Model</span></span>
<span>(</span><span>dataset, kernel, inference=&lt;mogptk.model.Exact object&gt;, mean=None, name=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Model is the base class for multi-output Gaussian process models.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code><a title="mogptk.dataset.DataSet" href="dataset.html#mogptk.dataset.DataSet">DataSet</a>, <a title="mogptk.data.Data" href="data.html#mogptk.data.Data">Data</a></code></dt>
<dd><code>DataSet</code> with <code>Data</code> objects for all the channels. When a (list or dict of) <code>Data</code> object is passed, it will automatically be converted to a <code>DataSet</code>.</dd>
<dt><strong><code>kernel</code></strong> :&ensp;<code><a title="mogptk.gpr.kernel.Kernel" href="gpr/kernel.html#mogptk.gpr.kernel.Kernel">Kernel</a></code></dt>
<dd>The kernel class.</dd>
<dt><strong><code>model</code></strong></dt>
<dd>Gaussian process model to use, such as <code><a title="mogptk.model.Exact" href="#mogptk.model.Exact">Exact</a></code>.</dd>
<dt><strong><code>mean</code></strong> :&ensp;<code><a title="mogptk.gpr.mean.Mean" href="gpr/mean.html#mogptk.gpr.mean.Mean">Mean</a></code></dt>
<dd>The mean class.</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the model.</dd>
</dl>
<h2 id="atributes">Atributes</h2>
<p>dataset: The associated mogptk.dataset.DataSet.
gpr: The mogptk.gpr.model.Model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L138-L843" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Model:
    def __init__(self, dataset, kernel, inference=Exact(), mean=None, name=None):
        &#34;&#34;&#34;
        Model is the base class for multi-output Gaussian process models.

        Args:
            dataset (mogptk.dataset.DataSet, mogptk.data.Data): `DataSet` with `Data` objects for all the channels. When a (list or dict of) `Data` object is passed, it will automatically be converted to a `DataSet`.
            kernel (mogptk.gpr.kernel.Kernel): The kernel class.
            model: Gaussian process model to use, such as `mogptk.model.Exact`.
            mean (mogptk.gpr.mean.Mean): The mean class.
            name (str): Name of the model.

        Atributes:
            dataset: The associated mogptk.dataset.DataSet.
            gpr: The mogptk.gpr.model.Model.
        &#34;&#34;&#34;
        
        if not isinstance(dataset, DataSet):
            dataset = DataSet(dataset)
        if dataset.get_output_dims() == 0:
            raise ValueError(&#34;dataset must have at least one channel&#34;)
        names = [name for name in dataset.get_names() if name is not None]
        if len(set(names)) != len(names):
            raise ValueError(&#34;all data channels must have unique names&#34;)

        for channel in dataset:
            for dim in range(channel.get_input_dims()):
                xran = np.max(channel.X.transformed[:,dim]) - np.min(channel.X.transformed[:,dim])
                if xran &lt; 1e-3:
                    logger.warning(&#34;Very small X range may give problems, it is suggested to scale up your X axis&#34;)
                elif 1e4 &lt; xran:
                    logger.warning(&#34;Very large X range may give problems, it is suggested to scale down your X axis&#34;)

        self.name = name
        self.dataset = dataset
        self.is_multioutput = kernel.output_dims is not None

        X, Y = self.dataset.get_train_data()
        x, y = self._to_kernel_format(X, Y)

        y_err = None
        if all(channel.Y_err is not None for channel in self.dataset):
            # TODO: doesn&#39;t transform...
            Y_err = [np.array(channel.Y_err[channel.mask]) for channel in self.dataset]
            Y_err_lower = [self.dataset[j].Y.transform(Y[j] - Y_err[j], X[j]) for j in range(len(self.dataset))]
            Y_err_upper = [self.dataset[j].Y.transform(Y[j] + Y_err[j], X[j]) for j in range(len(self.dataset))]
            y_err_lower = np.concatenate(Y_err_lower, axis=0)
            y_err_upper = np.concatenate(Y_err_upper, axis=0)
            y_err = (y_err_upper-y_err_lower)/2.0 # TODO: strictly incorrect: takes average error after transformation
        self.gpr = inference._build(kernel, x, y, y_err, mean, name)

        self.times = np.zeros(0)
        self.losses = np.zeros(0)
        self.errors = np.zeros(0)

    ################################################################

    def print_parameters(self):
        &#34;&#34;&#34;
        Print the parameters of the model in a table.

        Examples:
            &gt;&gt;&gt; model.print_parameters()
        &#34;&#34;&#34;
        self.gpr.print_parameters()

    def get_parameters(self):
        &#34;&#34;&#34;
        Returns all parameters of the kernel.

        Returns:
            list: mogptk.gpr.parameter.Parameter

        Examples:
            &gt;&gt;&gt; params = model.get_parameters()
        &#34;&#34;&#34;
        return self.gpr.get_parameters()

    def copy_parameters(self, other):
        &#34;&#34;&#34;
        Copy the kernel parameters from another model.
        &#34;&#34;&#34;
        if not isinstance(other, Model):
            raise ValueError(&#34;other must be of type Model&#34;)

        self.gpr.kernel.copy_parameters(other.kernel)

    def save(self, filename):
        &#34;&#34;&#34;
        Save the model to a given file that can then be loaded using `LoadModel()`.

        Args:
            filename (str): File name to save to, automatically appends &#39;.npy&#39;.

        Examples:
            &gt;&gt;&gt; model.save(&#39;filename&#39;)
        &#34;&#34;&#34;
        filename += &#34;.npy&#34; 
        try:
            os.remove(filename)
        except OSError:
            pass
        with open(filename, &#39;wb&#39;) as w:
            pickle.dump(self, w)

    def log_marginal_likelihood(self):
        &#34;&#34;&#34;
        Returns the log marginal likelihood of the kernel and its data and parameters.

        Returns:
            float: The current log marginal likelihood.

        Examples:
            &gt;&gt;&gt; model.log_marginal_likelihood()
        &#34;&#34;&#34;
        return self.gpr.log_marginal_likelihood().detach().cpu().item()

    def loss(self):
        &#34;&#34;&#34;
        Returns the loss of the kernel and its data and parameters.

        Returns:
            float: The current loss.

        Examples:
            &gt;&gt;&gt; model.loss()
        &#34;&#34;&#34;
        return self.gpr.loss().detach().cpu().item()

    def error(self, method=&#39;MAE&#39;, use_all_data=False):
        &#34;&#34;&#34;
        Returns the error of the kernel prediction with the removed data points in the data set.

        Args:
            method (str,function): Error calculation method, such as MAE, MAPE, sMAPE, MSE, or RMSE. When a function is given, it should have parameters (y_true,y_pred) or (y_true,y_pred,model).

        Returns:
            float: The current error.

        Examples:
            &gt;&gt;&gt; model.error()
        &#34;&#34;&#34;
        if use_all_data:
            X, Y_true = self.dataset.get_data()
        else:
            X, Y_true = self.dataset.get_test_data()
        x, y_true  = self._to_kernel_format(X, Y_true)
        y_pred, _ = self.gpr.predict(x, predict_y=False)
        if callable(method):
            if len(inspect.signature(method).parameters) == 3:
                return method(y_true, y_pred, self)
            return method(y_true, y_pred)
        elif method.lower() == &#39;mae&#39;:
            return mean_absolute_error(y_true, y_pred)
        elif method.lower() == &#39;mape&#39;:
            return mean_absolute_percentage_error(y_true, y_pred)
        elif method.lower() == &#39;smape&#39;:
            return symmetric_mean_absolute_percentage_error(y_true, y_pred)
        elif method.lower() == &#39;mse&#39;:
            return mean_squared_error(y_true, y_pred)
        elif method.lower() == &#39;rmse&#39;:
            return root_mean_squared_error(y_true, y_pred)
        else:
            raise ValueError(&#34;valid error calculation methods are MAE, MAPE, sMAPE, MSE, and RMSE&#34;)

    def train(
        self,
        method=&#39;Adam&#39;,
        iters=500,
        verbose=False,
        error=None,
        plot=False,
        **kwargs):
        &#34;&#34;&#34;
        Trains the model by optimizing the (hyper)parameters of the kernel to approach the training data.

        Args:
            method (str): Optimizer to use such as LBFGS, Adam, Adagrad, or SGD.
            iters (int): Number of iterations, or maximum in case of LBFGS optimizer.
            verbose (bool): Print verbose output about the state of the optimizer.
            error (str,function): Calculate prediction error for each iteration by the given method, such as MAE, MAPE, sMAPE, MSE, or RMSE. When a function is given, it should have parameters (y_true,y_pred) or (y_true,y_pred,model).
            plot (bool): Plot the loss and, if error is data set, the error of the test data points.
            **kwargs (dict): Additional dictionary of parameters passed to the PyTorch optimizer. 

        Returns:
            numpy.ndarray: Losses for all iterations.
            numpy.ndarray: Errors for all iterations. Only if `error` is set, otherwise zero.

        Examples:
            &gt;&gt;&gt; model.train()
            
            &gt;&gt;&gt; model.train(method=&#39;lbfgs&#39;, tolerance_grad=1e-10, tolerance_change=1e-12)
            
            &gt;&gt;&gt; model.train(method=&#39;adam&#39;, lr=0.5)
        &#34;&#34;&#34;
        error_use_all_data = False
        if error is not None and all(not channel.has_test_data() for channel in self.dataset):
            error_use_all_data = True

        if method.lower() in (&#39;l-bfgs&#39;, &#39;lbfgs&#39;, &#39;l-bfgs-b&#39;, &#39;lbfgsb&#39;):
            method = &#39;LBFGS&#39;
        elif method.lower() == &#39;adam&#39;:
            method = &#39;Adam&#39;
        elif method.lower() == &#39;sgd&#39;:
            method = &#39;SGD&#39;
        elif method.lower() == &#39;adagrad&#39;:
            method = &#39;AdaGrad&#39;
        else:
            raise ValueError(&#39;optimizer must be LBFGS, Adam, SGD, or AdaGrad&#39;)

        if verbose:
            training_points = sum([len(channel.get_train_data()[1]) for channel in self.dataset])
            parameters = sum([p.num_parameters if p.trainable else 0 for p in self.gpr.get_parameters()])
            print(&#39;\nStarting optimization using&#39;, method)
            if self.name is not None:
                print(&#39;‣ Model: %s&#39; % self.name)
            print(&#39;‣ Channels: %d&#39; % len(self.dataset))
            print(&#39;‣ Parameters: %d&#39; % parameters)
            print(&#39;‣ Training points: %d&#39; % training_points)
            print(&#39;‣ Initial loss: %6g&#39; % self.loss())
            if error is not None:
                print(&#39;‣ Initial error: %6g&#39; % self.error(error, error_use_all_data))

        iter_offset = 0
        times = np.zeros((iters+1,))
        losses = np.zeros((iters+1,))
        errors = np.zeros((iters+1,))
        if self.times.shape[0] != 0:
            iter_offset = self.times.shape[0]-1
            times = np.concatenate((self.times[:-1],times))
            losses = np.concatenate((self.losses[:-1],losses))
            errors = np.concatenate((self.errors[:-1],errors))
        initial_time = time.time()

        iters_len = int(math.log10(iter_offset+iters)) + 1
        def progress(i, loss):
            elapsed_time = time.time() - initial_time
            write = verbose and i % max(1,iters/100) == 0
            i += iter_offset
            times[i] = elapsed_time
            losses[i] = loss
            if error is not None:
                errors[i] = self.error(error, error_use_all_data)
                if write:
                    print(&#34;  %*d/%*d %s  loss=%12g  error=%12g&#34; % (iters_len, i, iters_len, iter_offset+iters, _format_time(elapsed_time), losses[i], errors[i]))
            elif write:
                print(&#34;  %*d/%*d %s  loss=%12g&#34; % (iters_len, i, iters_len, iter_offset+iters, _format_time(elapsed_time), losses[i]))

        if verbose:
            print(&#34;\nStart %s:&#34; % (method,))
        if method == &#39;LBFGS&#39;:
            if not &#39;max_iter&#39; in kwargs:
                kwargs[&#39;max_iter&#39;] = iters
            else:
                iters = kwargs[&#39;max_iter&#39;]
            optimizer = torch.optim.LBFGS(self.gpr.parameters(), **kwargs)

            def loss():
                i = int(optimizer.state_dict()[&#39;state&#39;][0][&#39;func_evals&#39;])
                loss = self.loss()
                progress(i, loss)
                return loss
            optimizer.step(loss)
            iters = int(optimizer.state_dict()[&#39;state&#39;][0][&#39;func_evals&#39;])
        else:
            if method == &#39;Adam&#39;:
                optimizer = torch.optim.Adam(self.gpr.parameters(), **kwargs)
            elif method == &#39;SGD&#39;:
                optimizer = torch.optim.SGD(self.gpr.parameters(), **kwargs)
            elif method == &#39;AdaGrad&#39;:
                optimizer = torch.optim.Adagrad(self.gpr.parameters(), **kwargs)

            for i in range(iters):
                progress(i, self.loss())
                optimizer.step()
        progress(iters, self.loss())

        if verbose:
            elapsed_time = time.time() - initial_time
            print(&#34;Finished&#34;)
            print(&#39;\nOptimization finished in %s&#39; % _format_duration(elapsed_time))
            print(&#39;‣ Iterations: %d&#39; % iters)
            print(&#39;‣ Final loss: %6g&#39;% losses[iter_offset+iters])
            if error is not None:
                print(&#39;‣ Final error: %6g&#39; % errors[iter_offset+iters])

        self.iters = iter_offset+iters
        self.times = times[:iter_offset+iters+1]
        self.losses = losses[:iter_offset+iters+1]
        if error is not None:
            self.errors = errors[:iter_offset+iters+1]
        if plot:
            self.plot_losses()
        return losses, errors

    ################################################################################
    # Predictions ##################################################################
    ################################################################################

    def _to_kernel_format(self, X, Y=None):
        &#34;&#34;&#34;
        Return the data vectors in the format used by the kernels. If Y is not passed, than only X data is returned.

        Returns:
            numpy.ndarray: X data of shape (data_points,input_dims). If the kernel is multi output, an additional input dimension is prepended with the channel indices.
            numpy.ndarray: Y data of shape (data_points,1).
            numpy.ndarray: Original but normalized X data. Only if no Y is passed.
        &#34;&#34;&#34;
        X = X.copy()
        for j, channel_x in enumerate(X):
            X[j] = X[j].transformed

        x = np.concatenate(X, axis=0)
        if self.is_multioutput:
            chan = [j * np.ones(len(X[j])) for j in range(len(X))]
            chan = np.concatenate(chan).reshape(-1, 1)
            x = np.concatenate([chan, x], axis=1)
        if Y is None:
            return x

        Y = Y.copy()
        for j, channel_y in enumerate(Y):
            Y[j] = Y[j].transformed
        y = np.concatenate(Y, axis=0).reshape(-1, 1)
        return x, y

    def predict(self, X=None, sigma=2, transformed=False, predict_y=True):
        &#34;&#34;&#34;
        Predict using the prediction range of the data set and save the prediction in that data set. Otherwise, if `X` is passed, use that as the prediction range and return the prediction instead of saving it.

        Args:
            X (list, dict): Dictionary where keys are channel index and elements numpy arrays with channel inputs. If passed, results will be returned and not saved in the data set for later retrieval.
            sigma (float): Number of standard deviations to display upwards and downwards.
            transformed (boolean): Return transformed data as used for training.

        Returns:
            numpy.ndarray: X prediction of shape (n,) for each channel.
            numpy.ndarray: Y mean prediction of shape (n,) for each channel.
            numpy.ndarray: Y lower prediction of uncertainty interval of shape (n,) for each channel.
            numpy.ndarray: Y upper prediction of uncertainty interval of shape (n,) for each channel.

        Examples:
            &gt;&gt;&gt; model.predict(X)
        &#34;&#34;&#34;
        if X is None:
            X = self.dataset.get_prediction_data()
        else:
            X = self.dataset._format_X(X)
        x = self._to_kernel_format(X)

        mu, var = self.gpr.predict(x, predict_y=predict_y, tensor=True)
        lower = mu - sigma*torch.sqrt(var)
        upper = mu + sigma*torch.sqrt(var)

        mu = mu.cpu().numpy()
        var = var.cpu().numpy()
        lower = lower.cpu().numpy()
        upper = upper.cpu().numpy()

        i = 0
        Mu = []
        Var = []
        Lower = []
        Upper = []
        for j in range(self.dataset.get_output_dims()):
            N = X[j].shape[0]
            Mu.append(np.squeeze(mu[i:i+N]))
            Var.append(np.squeeze(var[i:i+N]))
            Lower.append(np.squeeze(lower[i:i+N]))
            Upper.append(np.squeeze(upper[i:i+N]))
            i += N

        if not transformed:
            for j in range(self.dataset.get_output_dims()):
                Mu[j] = self.dataset[j].Y.detransform(Mu[j], X[j])
                Lower[j] = self.dataset[j].Y.detransform(Lower[j], X[j])
                Upper[j] = self.dataset[j].Y.detransform(Upper[j], X[j])

        if len(self.dataset) == 1:
            return X[0], Mu[0], Lower[0], Upper[0]
        return X, Mu, Lower, Upper

    def K(self, X1, X2=None):
        &#34;&#34;&#34;
        Evaluate the kernel at K(X1,X2).

        Args:
            X1 (list, dict): Dictionary where keys are channel index and elements numpy arrays with channel inputs.
            X2 (list, dict): Same as X1 if None.

        Returns:
            numpy.ndarray: kernel evaluated at X1 and X2 of shape (n1,n2).

        Examples:
            &gt;&gt;&gt; channel0 = np.array([&#39;1987-05-20&#39;, &#39;1987-05-21&#39;])
            &gt;&gt;&gt; channel1 = np.array([[2.5, 534.6], [3.5, 898.22], [4.5, 566.98]])
            &gt;&gt;&gt; model.K([channel0,channel1])
        &#34;&#34;&#34;
        X1 = self.dataset._format_X(X1)
        x1 = self._to_kernel_format(X1)
        if X2 is None:
            return self.gpr.K(x1)
        else:
            X2 = self.dataset._format_X(X2)
            x2 = self._to_kernel_format(X2)
            return self.gpr.K(x1, x2)

    def sample(self, X=None, n=None, predict_y=True, transformed=False):
        &#34;&#34;&#34;
        Sample n times from the kernel at input X .

        Args:
            X (list, dict): Dictionary where keys are channel index and elements numpy arrays with channel inputs.
            n (int): Number of samples.
            predict_y (boolean): Predict data values instead of function values.
            transformed (boolean): Return transformed data as used for training.

        Returns:
            list: samples of shape len(X) for each channel if n is given.
            numpy.ndarray: sample of shape len(X) for each channel if n is None.

        Examples:
            &gt;&gt;&gt; model.sample(n=10)
        &#34;&#34;&#34;
        if X is None:
            X = self.dataset.get_prediction_data()
        else:
            X = self.dataset._format_X(X)
        x = self._to_kernel_format(X)

        samples = self.gpr.sample(Z=x, n=n, predict_y=predict_y)

        i = 0
        Samples = []
        for j in range(self.dataset.get_output_dims()):
            N = X[j][0].shape[0]
            if n is None:
                sample = np.squeeze(samples[i:i+N])
                if not transformed:
                    sample = self.dataset[j].Y.detransform(sample, X[j])
                Samples.append(sample)
            else:
                ss = []
                for k in range(n):
                    sample = np.squeeze(samples[i:i+N,k])
                    if not transformed:
                        sample = self.dataset[j].Y.detransform(sample, X[j])
                    ss.append(sample)
                Samples.append(ss)
            i += N
        if self.dataset.get_output_dims() == 1:
            return Samples[0]
        return Samples

    def plot_losses(self, title=None, figsize=(18,6), legend=True, errors=True, log=False):
        &#34;&#34;&#34;
        Plot the losses and errors during training. In order to display the errors, make sure to set the error parameter when training.

        Args:
            title (str): Figure title.
            figsize (tuple): Figure size.
            legend (boolean): Show the legend.
            errors (boolean): Show the errors.
            log (boolean): Show in log scale.

        Returns:
            figure: Matplotlib figure.
            axis: Matplotlib axis.
        &#34;&#34;&#34;
        if not hasattr(self, &#39;losses&#39;):
            raise Exception(&#34;must be trained in order to plot the losses&#34;)

        fig, ax = plt.subplots(1, 1, figsize=figsize, constrained_layout=True)
        x = np.arange(0,self.iters+1)
        ax.set_xlim(0, self.iters)
        ax.set_xlabel(&#39;Iteration&#39;)
        ax.set_ylabel(&#39;Loss&#39;)
        if log:
            ax.set_yscale(&#39;log&#39;)

        ax.plot(x, self.losses, c=&#39;k&#39;, ls=&#39;-&#39;)

        legends = []
        legends.append(plt.Line2D([0], [0], ls=&#39;-&#39;, color=&#39;k&#39;, label=&#39;Loss&#39;))
        if errors and hasattr(self, &#39;errors&#39;):
            ax2 = ax.twinx()
            ax2.plot(x, self.errors, c=&#39;k&#39;, ls=&#39;-.&#39;)
            ax2.set_ylabel(&#39;Error&#39;)
            legends.append(plt.Line2D([0], [0], ls=&#39;-.&#39;, color=&#39;k&#39;, label=&#39;Error&#39;))
            if log:
                ax2.set_yscale(&#39;log&#39;)

        if title is not None:
            fig.suptitle(title, fontsize=18)

        if legend:
            ax.legend(handles=legends)
        return fig, ax

    def plot_prediction(self, X=None, title=None, figsize=None, legend=True, errorbars=True, predict_y=True, transformed=False):
        &#34;&#34;&#34;
        Plot the data including removed observations, latent function, and predictions of this model for each channel.

        Args:
            title (str): Set the title of the plot.
            figsize (tuple): Set the figure size.
            legend (boolean): Disable legend.
            errorbars (boolean): Plot data error bars if available.
            predict_y (boolean): Predict data values instead of function values.
            transformed (boolean): Display transformed Y data as used for training.

        Returns:
            matplotlib.figure.Figure: The figure.
            list of matplotlib.axes.Axes: List of axes.

        Examples:
            &gt;&gt;&gt; fig, axes = dataset.plot(title=&#39;Title&#39;)
        &#34;&#34;&#34;
        X, Mu, Lower, Upper = self.predict(X, predict_y=predict_y)
        if len(self.dataset) == 1:
            X = [X]
            Mu = [Mu]
            Lower = [Lower]
            Upper = [Upper]

        fig, ax = plt.subplots(len(self.dataset), 1, figsize=(18,6*len(self.dataset)), squeeze=True, constrained_layout=True)
        for j, data in enumerate(self.dataset):
            # TODO: ability to plot conditional or marginal distribution to reduce input dims
            if data.get_input_dims() &gt; 2:
                raise ValueError(&#34;cannot plot more than two input dimensions&#34;)
            if data.get_input_dims() == 2:
                raise NotImplementedError(&#34;two dimensional input data not yet implemented&#34;) # TODO

            legends = []
            if errorbars and data.Y_err is not None:
                x, y = data.get_train_data(transformed=transformed)
                yl = data.Y[data.mask] - data.Y_err[data.mask]
                yu = data.Y[data.mask] + data.Y_err[data.mask]
                if transformed:
                    yl = data.Y.transform(yl, x)
                    yu = data.Y.transform(yu, x)
                ax[j].errorbar(x, y, [y-yl, yu-y], elinewidth=1.5, ecolor=&#39;lightgray&#39;, capsize=0, ls=&#39;&#39;, marker=&#39;&#39;)

            # prediction
            idx = np.argsort(X[j][:,0])
            ax.plot(X[j][idx,0], Mu[j][idx], ls=&#39;:&#39;, color=&#39;blue&#39;, lw=2)
            ax.fill_between(X[j][idx,0], Lower[j][idx], Upper[j][idx], color=&#39;blue&#39;, alpha=0.3)
            label = &#39;Posterior Mean&#39;
            legends.append(patches.Rectangle(
                (1, 1), 1, 1, fill=True, color=&#39;blue&#39;, alpha=0.3, lw=0, label=&#39;95% Error Bars&#39;
            ))
            legends.append(plt.Line2D([0], [0], ls=&#39;:&#39;, color=&#39;blue&#39;, lw=2, label=label))

            xmin = min(np.min(data.X), np.min(X[j]))
            xmax = max(np.max(data.X), np.max(X[j]))
            if data.F is not None:
                if np.issubdtype(data.X.dtypes[0], np.datetime64):
                    dt = np.timedelta64(1,data.X.get_time_unit())
                    n = int((xmax-xmin) / dt) + 1
                    x = np.arange(xmin, xmax+np.timedelta64(1,&#39;us&#39;), dt, dtype=data.X.dtypes[0])
                else:
                    n = len(data.X)*10
                    x = np.linspace(xmin, xmax, n)

                y = data.F(x)
                if transformed:
                    y = data.Y.transform(y, x)

                ax.plot(x, y, &#39;r--&#39;, lw=1)
                legends.append(plt.Line2D([0], [0], ls=&#39;--&#39;, color=&#39;r&#39;, label=&#39;True&#39;))

            if data.has_test_data():
                x, y = data.get_test_data(transformed=transformed)
                ax.plot(x, y, &#39;g.&#39;, ms=10)
                legends.append(plt.Line2D([0], [0], ls=&#39;&#39;, color=&#39;g&#39;, marker=&#39;.&#39;, ms=10, label=&#39;Latent&#39;))

            x, y = data.get_train_data(transformed=transformed)
            ax.plot(x, y, &#39;r.&#39;, ms=10)
            legends.append(plt.Line2D([0], [0], ls=&#39;&#39;, color=&#39;r&#39;, marker=&#39;.&#39;, ms=10, label=&#39;Observations&#39;))

            if 0 &lt; len(data.removed_ranges[0]):
                for removed_range in data.removed_ranges[0]:
                    x0 = removed_range[0]
                    x1 = removed_range[1]
                    y0 = ax.get_ylim()[0]
                    y1 = ax.get_ylim()[1]
                    ax.add_patch(patches.Rectangle(
                        (x0, y0), x1-x0, y1-y0, fill=True, color=&#39;xkcd:strawberry&#39;, alpha=0.2, lw=0,
                    ))
                legends.append(patches.Rectangle(
                    (1, 1), 1, 1, fill=True, color=&#39;xkcd:strawberry&#39;, alpha=0.5, lw=0, label=&#39;Removed Ranges&#39;
                ))

            ax.set_xlim(xmin - (xmax - xmin)*0.001, xmax + (xmax - xmin)*0.001)
            ax.set_xlabel(data.X_labels[0])
            ax.set_ylabel(data.Y_label)
            ax.set_title(data.name if title is None else title, fontsize=14)

        if legend:
            ax.legend(handles=legends[::-1])
        return fig, ax

    def plot_gram(self, start=None, end=None, n=31, title=None, figsize=(18,18)):
        &#34;&#34;&#34;
        Plot the gram matrix of associated kernel.

        Args:
            start (float, list, array): Interval minimum.
            end (float, list, array): Interval maximum.
            n (int): Number of points per channel.
            title (str): Figure title.
            figsize (tuple): Figure size.

        Returns:
            figure: Matplotlib figure.
            axis: Matplotlib axis.
        &#34;&#34;&#34;
        if not all(channel.get_input_dims() == 1 for channel in self.dataset):
            raise ValueError(&#34;cannot plot for more than one input dimension&#34;)

        if start is None:
            start = [channel.X.transformed.min() for channel in self.dataset]
        if end is None:
            end = [channel.X.transformed.max() for channel in self.dataset]

        output_dims = len(self.dataset)
        if not isinstance(start, (list, np.ndarray)):
            start = [start] * output_dims
        if not isinstance(end, (list, np.ndarray)):
            end = [end] * output_dims

        X = np.zeros((output_dims*n, 2))
        X[:,0] = np.repeat(np.arange(output_dims), n)
        for j in range(output_dims):
            if n== 1:
                X[j*n:(j+1)*n,1] = np.array((start[j]+end[j])/2.0)
            else:
                X[j*n:(j+1)*n,1] = np.linspace(start[j], end[j], n)
        k = self.gpr.K(X)
            
        fig, ax = plt.subplots(1, 1, figsize=figsize, constrained_layout=True)
        if title is not None:
            fig.suptitle(title, fontsize=18)

        color_range = np.abs(k).max()
        norm = matplotlib.colors.Normalize(vmin=-color_range, vmax=color_range)
        im = ax.matshow(k, cmap=&#39;coolwarm&#39;, norm=norm)

        divider = make_axes_locatable(ax)
        cax = divider.append_axes(&#34;right&#34;, size=&#34;5%&#34;, pad=0.3)
        fig.colorbar(im, cax=cax)

        # Major ticks every 20, minor ticks every 5
        major_ticks = np.arange(-0.5, output_dims*n, n)
        minor_ticks = np.arange(-0.5, output_dims*n, 2)

        ax.set_xticks(major_ticks)
        ax.set_yticks(major_ticks)
        ax.grid(which=&#39;major&#39;, lw=1.5, c=&#39;k&#39;)
        ax.set_xticklabels([])
        ax.set_yticklabels([])
        ax.tick_params(axis=&#39;both&#39;, which=&#39;both&#39;, length=0)
        return fig, ax

    def plot_kernel(self, dist=None, n=101, title=None, figsize=(18,18)):
        &#34;&#34;&#34;
        Plot the kernel matrix at a range of data point distances for each channel for stationary kernels.

        Args:
            dist (list): Maximum distance for every channel.
            n (int): Number of points per channel.
            title (str): Figure title.
            figsize (tuple): Figure size.

        Returns:
            figure: Matplotlib figure.
            axis: Matplotlib axis.
        &#34;&#34;&#34;
        if not all(channel.get_input_dims() == 1 for channel in self.dataset):
            raise ValueError(&#34;cannot plot for more than one input dimension&#34;)

        if dist is None:
            dist = [(channel.X.transformed.max()-channel.X.transformed.min())/4.0 for channel in self.dataset]

        output_dims = len(self.dataset)
        if not isinstance(dist, (list, np.ndarray)):
            dist = [dist] * output_dims

        fig, ax = plt.subplots(output_dims, output_dims, figsize=figsize, constrained_layout=True, squeeze=False, sharex=True)
        if title is not None:
            fig.suptitle(title, fontsize=18)

        channel = np.ones((n,1))
        for j in range(output_dims):
            tau = np.linspace(-dist[j], dist[j], num=n).reshape(-1,1)
            X1 = np.array([[j,0.0]])
            for i in range(output_dims):
                if j &lt; i:
                    ax[j,i].set_axis_off()
                    continue

                X0 = np.concatenate((i*channel,tau), axis=1)
                k = self.gpr.K(X0,X1)
                ax[j,i].plot(tau, k, color=&#39;k&#39;)
                ax[j,i].set_yticks([])
        return fig, ax</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="mogptk.models.conv.CONV" href="models/conv.html#mogptk.models.conv.CONV">CONV</a></li>
<li><a title="mogptk.models.csm.CSM" href="models/csm.html#mogptk.models.csm.CSM">CSM</a></li>
<li><a title="mogptk.models.mohsm.MOHSM" href="models/mohsm.html#mogptk.models.mohsm.MOHSM">MOHSM</a></li>
<li><a title="mogptk.models.mosm.MOSM" href="models/mosm.html#mogptk.models.mosm.MOSM">MOSM</a></li>
<li><a title="mogptk.models.sm.SM" href="models/sm.html#mogptk.models.sm.SM">SM</a></li>
<li><a title="mogptk.models.sm_lmc.SM_LMC" href="models/sm_lmc.html#mogptk.models.sm_lmc.SM_LMC">SM_LMC</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mogptk.model.Model.K"><code class="name flex">
<span>def <span class="ident">K</span></span>(<span>self, X1, X2=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the kernel at K(X1,X2).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X1</code></strong> :&ensp;<code>list, dict</code></dt>
<dd>Dictionary where keys are channel index and elements numpy arrays with channel inputs.</dd>
<dt><strong><code>X2</code></strong> :&ensp;<code>list, dict</code></dt>
<dd>Same as X1 if None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>kernel evaluated at X1 and X2 of shape (n1,n2).</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; channel0 = np.array(['1987-05-20', '1987-05-21'])
&gt;&gt;&gt; channel1 = np.array([[2.5, 534.6], [3.5, 898.22], [4.5, 566.98]])
&gt;&gt;&gt; model.K([channel0,channel1])
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L520-L543" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def K(self, X1, X2=None):
    &#34;&#34;&#34;
    Evaluate the kernel at K(X1,X2).

    Args:
        X1 (list, dict): Dictionary where keys are channel index and elements numpy arrays with channel inputs.
        X2 (list, dict): Same as X1 if None.

    Returns:
        numpy.ndarray: kernel evaluated at X1 and X2 of shape (n1,n2).

    Examples:
        &gt;&gt;&gt; channel0 = np.array([&#39;1987-05-20&#39;, &#39;1987-05-21&#39;])
        &gt;&gt;&gt; channel1 = np.array([[2.5, 534.6], [3.5, 898.22], [4.5, 566.98]])
        &gt;&gt;&gt; model.K([channel0,channel1])
    &#34;&#34;&#34;
    X1 = self.dataset._format_X(X1)
    x1 = self._to_kernel_format(X1)
    if X2 is None:
        return self.gpr.K(x1)
    else:
        X2 = self.dataset._format_X(X2)
        x2 = self._to_kernel_format(X2)
        return self.gpr.K(x1, x2)</code></pre>
</details>
</dd>
<dt id="mogptk.model.Model.copy_parameters"><code class="name flex">
<span>def <span class="ident">copy_parameters</span></span>(<span>self, other)</span>
</code></dt>
<dd>
<div class="desc"><p>Copy the kernel parameters from another model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L216-L223" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def copy_parameters(self, other):
    &#34;&#34;&#34;
    Copy the kernel parameters from another model.
    &#34;&#34;&#34;
    if not isinstance(other, Model):
        raise ValueError(&#34;other must be of type Model&#34;)

    self.gpr.kernel.copy_parameters(other.kernel)</code></pre>
</details>
</dd>
<dt id="mogptk.model.Model.error"><code class="name flex">
<span>def <span class="ident">error</span></span>(<span>self, method='MAE', use_all_data=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the error of the kernel prediction with the removed data points in the data set.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>method</code></strong> :&ensp;<code>str,function</code></dt>
<dd>Error calculation method, such as MAE, MAPE, sMAPE, MSE, or RMSE. When a function is given, it should have parameters (y_true,y_pred) or (y_true,y_pred,model).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>The current error.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; model.error()
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L267-L301" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def error(self, method=&#39;MAE&#39;, use_all_data=False):
    &#34;&#34;&#34;
    Returns the error of the kernel prediction with the removed data points in the data set.

    Args:
        method (str,function): Error calculation method, such as MAE, MAPE, sMAPE, MSE, or RMSE. When a function is given, it should have parameters (y_true,y_pred) or (y_true,y_pred,model).

    Returns:
        float: The current error.

    Examples:
        &gt;&gt;&gt; model.error()
    &#34;&#34;&#34;
    if use_all_data:
        X, Y_true = self.dataset.get_data()
    else:
        X, Y_true = self.dataset.get_test_data()
    x, y_true  = self._to_kernel_format(X, Y_true)
    y_pred, _ = self.gpr.predict(x, predict_y=False)
    if callable(method):
        if len(inspect.signature(method).parameters) == 3:
            return method(y_true, y_pred, self)
        return method(y_true, y_pred)
    elif method.lower() == &#39;mae&#39;:
        return mean_absolute_error(y_true, y_pred)
    elif method.lower() == &#39;mape&#39;:
        return mean_absolute_percentage_error(y_true, y_pred)
    elif method.lower() == &#39;smape&#39;:
        return symmetric_mean_absolute_percentage_error(y_true, y_pred)
    elif method.lower() == &#39;mse&#39;:
        return mean_squared_error(y_true, y_pred)
    elif method.lower() == &#39;rmse&#39;:
        return root_mean_squared_error(y_true, y_pred)
    else:
        raise ValueError(&#34;valid error calculation methods are MAE, MAPE, sMAPE, MSE, and RMSE&#34;)</code></pre>
</details>
</dd>
<dt id="mogptk.model.Model.get_parameters"><code class="name flex">
<span>def <span class="ident">get_parameters</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns all parameters of the kernel.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>mogptk.gpr.parameter.Parameter</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; params = model.get_parameters()
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L204-L214" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def get_parameters(self):
    &#34;&#34;&#34;
    Returns all parameters of the kernel.

    Returns:
        list: mogptk.gpr.parameter.Parameter

    Examples:
        &gt;&gt;&gt; params = model.get_parameters()
    &#34;&#34;&#34;
    return self.gpr.get_parameters()</code></pre>
</details>
</dd>
<dt id="mogptk.model.Model.log_marginal_likelihood"><code class="name flex">
<span>def <span class="ident">log_marginal_likelihood</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the log marginal likelihood of the kernel and its data and parameters.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>The current log marginal likelihood.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; model.log_marginal_likelihood()
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L243-L253" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def log_marginal_likelihood(self):
    &#34;&#34;&#34;
    Returns the log marginal likelihood of the kernel and its data and parameters.

    Returns:
        float: The current log marginal likelihood.

    Examples:
        &gt;&gt;&gt; model.log_marginal_likelihood()
    &#34;&#34;&#34;
    return self.gpr.log_marginal_likelihood().detach().cpu().item()</code></pre>
</details>
</dd>
<dt id="mogptk.model.Model.loss"><code class="name flex">
<span>def <span class="ident">loss</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the loss of the kernel and its data and parameters.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>The current loss.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; model.loss()
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L255-L265" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def loss(self):
    &#34;&#34;&#34;
    Returns the loss of the kernel and its data and parameters.

    Returns:
        float: The current loss.

    Examples:
        &gt;&gt;&gt; model.loss()
    &#34;&#34;&#34;
    return self.gpr.loss().detach().cpu().item()</code></pre>
</details>
</dd>
<dt id="mogptk.model.Model.plot_gram"><code class="name flex">
<span>def <span class="ident">plot_gram</span></span>(<span>self, start=None, end=None, n=31, title=None, figsize=(18, 18))</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the gram matrix of associated kernel.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>start</code></strong> :&ensp;<code>float, list, array</code></dt>
<dd>Interval minimum.</dd>
<dt><strong><code>end</code></strong> :&ensp;<code>float, list, array</code></dt>
<dd>Interval maximum.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of points per channel.</dd>
<dt><strong><code>title</code></strong> :&ensp;<code>str</code></dt>
<dd>Figure title.</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Figure size.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>figure</code></dt>
<dd>Matplotlib figure.</dd>
<dt><code>axis</code></dt>
<dd>Matplotlib axis.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L740-L800" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def plot_gram(self, start=None, end=None, n=31, title=None, figsize=(18,18)):
    &#34;&#34;&#34;
    Plot the gram matrix of associated kernel.

    Args:
        start (float, list, array): Interval minimum.
        end (float, list, array): Interval maximum.
        n (int): Number of points per channel.
        title (str): Figure title.
        figsize (tuple): Figure size.

    Returns:
        figure: Matplotlib figure.
        axis: Matplotlib axis.
    &#34;&#34;&#34;
    if not all(channel.get_input_dims() == 1 for channel in self.dataset):
        raise ValueError(&#34;cannot plot for more than one input dimension&#34;)

    if start is None:
        start = [channel.X.transformed.min() for channel in self.dataset]
    if end is None:
        end = [channel.X.transformed.max() for channel in self.dataset]

    output_dims = len(self.dataset)
    if not isinstance(start, (list, np.ndarray)):
        start = [start] * output_dims
    if not isinstance(end, (list, np.ndarray)):
        end = [end] * output_dims

    X = np.zeros((output_dims*n, 2))
    X[:,0] = np.repeat(np.arange(output_dims), n)
    for j in range(output_dims):
        if n== 1:
            X[j*n:(j+1)*n,1] = np.array((start[j]+end[j])/2.0)
        else:
            X[j*n:(j+1)*n,1] = np.linspace(start[j], end[j], n)
    k = self.gpr.K(X)
        
    fig, ax = plt.subplots(1, 1, figsize=figsize, constrained_layout=True)
    if title is not None:
        fig.suptitle(title, fontsize=18)

    color_range = np.abs(k).max()
    norm = matplotlib.colors.Normalize(vmin=-color_range, vmax=color_range)
    im = ax.matshow(k, cmap=&#39;coolwarm&#39;, norm=norm)

    divider = make_axes_locatable(ax)
    cax = divider.append_axes(&#34;right&#34;, size=&#34;5%&#34;, pad=0.3)
    fig.colorbar(im, cax=cax)

    # Major ticks every 20, minor ticks every 5
    major_ticks = np.arange(-0.5, output_dims*n, n)
    minor_ticks = np.arange(-0.5, output_dims*n, 2)

    ax.set_xticks(major_ticks)
    ax.set_yticks(major_ticks)
    ax.grid(which=&#39;major&#39;, lw=1.5, c=&#39;k&#39;)
    ax.set_xticklabels([])
    ax.set_yticklabels([])
    ax.tick_params(axis=&#39;both&#39;, which=&#39;both&#39;, length=0)
    return fig, ax</code></pre>
</details>
</dd>
<dt id="mogptk.model.Model.plot_kernel"><code class="name flex">
<span>def <span class="ident">plot_kernel</span></span>(<span>self, dist=None, n=101, title=None, figsize=(18, 18))</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the kernel matrix at a range of data point distances for each channel for stationary kernels.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dist</code></strong> :&ensp;<code>list</code></dt>
<dd>Maximum distance for every channel.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of points per channel.</dd>
<dt><strong><code>title</code></strong> :&ensp;<code>str</code></dt>
<dd>Figure title.</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Figure size.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>figure</code></dt>
<dd>Matplotlib figure.</dd>
<dt><code>axis</code></dt>
<dd>Matplotlib axis.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L802-L843" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def plot_kernel(self, dist=None, n=101, title=None, figsize=(18,18)):
    &#34;&#34;&#34;
    Plot the kernel matrix at a range of data point distances for each channel for stationary kernels.

    Args:
        dist (list): Maximum distance for every channel.
        n (int): Number of points per channel.
        title (str): Figure title.
        figsize (tuple): Figure size.

    Returns:
        figure: Matplotlib figure.
        axis: Matplotlib axis.
    &#34;&#34;&#34;
    if not all(channel.get_input_dims() == 1 for channel in self.dataset):
        raise ValueError(&#34;cannot plot for more than one input dimension&#34;)

    if dist is None:
        dist = [(channel.X.transformed.max()-channel.X.transformed.min())/4.0 for channel in self.dataset]

    output_dims = len(self.dataset)
    if not isinstance(dist, (list, np.ndarray)):
        dist = [dist] * output_dims

    fig, ax = plt.subplots(output_dims, output_dims, figsize=figsize, constrained_layout=True, squeeze=False, sharex=True)
    if title is not None:
        fig.suptitle(title, fontsize=18)

    channel = np.ones((n,1))
    for j in range(output_dims):
        tau = np.linspace(-dist[j], dist[j], num=n).reshape(-1,1)
        X1 = np.array([[j,0.0]])
        for i in range(output_dims):
            if j &lt; i:
                ax[j,i].set_axis_off()
                continue

            X0 = np.concatenate((i*channel,tau), axis=1)
            k = self.gpr.K(X0,X1)
            ax[j,i].plot(tau, k, color=&#39;k&#39;)
            ax[j,i].set_yticks([])
    return fig, ax</code></pre>
</details>
</dd>
<dt id="mogptk.model.Model.plot_losses"><code class="name flex">
<span>def <span class="ident">plot_losses</span></span>(<span>self, title=None, figsize=(18, 6), legend=True, errors=True, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the losses and errors during training. In order to display the errors, make sure to set the error parameter when training.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>title</code></strong> :&ensp;<code>str</code></dt>
<dd>Figure title.</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Figure size.</dd>
<dt><strong><code>legend</code></strong> :&ensp;<code>boolean</code></dt>
<dd>Show the legend.</dd>
<dt><strong><code>errors</code></strong> :&ensp;<code>boolean</code></dt>
<dd>Show the errors.</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>boolean</code></dt>
<dd>Show in log scale.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>figure</code></dt>
<dd>Matplotlib figure.</dd>
<dt><code>axis</code></dt>
<dd>Matplotlib axis.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L592-L635" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def plot_losses(self, title=None, figsize=(18,6), legend=True, errors=True, log=False):
    &#34;&#34;&#34;
    Plot the losses and errors during training. In order to display the errors, make sure to set the error parameter when training.

    Args:
        title (str): Figure title.
        figsize (tuple): Figure size.
        legend (boolean): Show the legend.
        errors (boolean): Show the errors.
        log (boolean): Show in log scale.

    Returns:
        figure: Matplotlib figure.
        axis: Matplotlib axis.
    &#34;&#34;&#34;
    if not hasattr(self, &#39;losses&#39;):
        raise Exception(&#34;must be trained in order to plot the losses&#34;)

    fig, ax = plt.subplots(1, 1, figsize=figsize, constrained_layout=True)
    x = np.arange(0,self.iters+1)
    ax.set_xlim(0, self.iters)
    ax.set_xlabel(&#39;Iteration&#39;)
    ax.set_ylabel(&#39;Loss&#39;)
    if log:
        ax.set_yscale(&#39;log&#39;)

    ax.plot(x, self.losses, c=&#39;k&#39;, ls=&#39;-&#39;)

    legends = []
    legends.append(plt.Line2D([0], [0], ls=&#39;-&#39;, color=&#39;k&#39;, label=&#39;Loss&#39;))
    if errors and hasattr(self, &#39;errors&#39;):
        ax2 = ax.twinx()
        ax2.plot(x, self.errors, c=&#39;k&#39;, ls=&#39;-.&#39;)
        ax2.set_ylabel(&#39;Error&#39;)
        legends.append(plt.Line2D([0], [0], ls=&#39;-.&#39;, color=&#39;k&#39;, label=&#39;Error&#39;))
        if log:
            ax2.set_yscale(&#39;log&#39;)

    if title is not None:
        fig.suptitle(title, fontsize=18)

    if legend:
        ax.legend(handles=legends)
    return fig, ax</code></pre>
</details>
</dd>
<dt id="mogptk.model.Model.plot_prediction"><code class="name flex">
<span>def <span class="ident">plot_prediction</span></span>(<span>self, X=None, title=None, figsize=None, legend=True, errorbars=True, predict_y=True, transformed=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the data including removed observations, latent function, and predictions of this model for each channel.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>title</code></strong> :&ensp;<code>str</code></dt>
<dd>Set the title of the plot.</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Set the figure size.</dd>
<dt><strong><code>legend</code></strong> :&ensp;<code>boolean</code></dt>
<dd>Disable legend.</dd>
<dt><strong><code>errorbars</code></strong> :&ensp;<code>boolean</code></dt>
<dd>Plot data error bars if available.</dd>
<dt><strong><code>predict_y</code></strong> :&ensp;<code>boolean</code></dt>
<dd>Predict data values instead of function values.</dd>
<dt><strong><code>transformed</code></strong> :&ensp;<code>boolean</code></dt>
<dd>Display transformed Y data as used for training.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>matplotlib.figure.Figure</code></dt>
<dd>The figure.</dd>
<dt><code>list</code> of <code>matplotlib.axes.Axes</code></dt>
<dd>List of axes.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; fig, axes = dataset.plot(title='Title')
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L637-L738" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def plot_prediction(self, X=None, title=None, figsize=None, legend=True, errorbars=True, predict_y=True, transformed=False):
    &#34;&#34;&#34;
    Plot the data including removed observations, latent function, and predictions of this model for each channel.

    Args:
        title (str): Set the title of the plot.
        figsize (tuple): Set the figure size.
        legend (boolean): Disable legend.
        errorbars (boolean): Plot data error bars if available.
        predict_y (boolean): Predict data values instead of function values.
        transformed (boolean): Display transformed Y data as used for training.

    Returns:
        matplotlib.figure.Figure: The figure.
        list of matplotlib.axes.Axes: List of axes.

    Examples:
        &gt;&gt;&gt; fig, axes = dataset.plot(title=&#39;Title&#39;)
    &#34;&#34;&#34;
    X, Mu, Lower, Upper = self.predict(X, predict_y=predict_y)
    if len(self.dataset) == 1:
        X = [X]
        Mu = [Mu]
        Lower = [Lower]
        Upper = [Upper]

    fig, ax = plt.subplots(len(self.dataset), 1, figsize=(18,6*len(self.dataset)), squeeze=True, constrained_layout=True)
    for j, data in enumerate(self.dataset):
        # TODO: ability to plot conditional or marginal distribution to reduce input dims
        if data.get_input_dims() &gt; 2:
            raise ValueError(&#34;cannot plot more than two input dimensions&#34;)
        if data.get_input_dims() == 2:
            raise NotImplementedError(&#34;two dimensional input data not yet implemented&#34;) # TODO

        legends = []
        if errorbars and data.Y_err is not None:
            x, y = data.get_train_data(transformed=transformed)
            yl = data.Y[data.mask] - data.Y_err[data.mask]
            yu = data.Y[data.mask] + data.Y_err[data.mask]
            if transformed:
                yl = data.Y.transform(yl, x)
                yu = data.Y.transform(yu, x)
            ax[j].errorbar(x, y, [y-yl, yu-y], elinewidth=1.5, ecolor=&#39;lightgray&#39;, capsize=0, ls=&#39;&#39;, marker=&#39;&#39;)

        # prediction
        idx = np.argsort(X[j][:,0])
        ax.plot(X[j][idx,0], Mu[j][idx], ls=&#39;:&#39;, color=&#39;blue&#39;, lw=2)
        ax.fill_between(X[j][idx,0], Lower[j][idx], Upper[j][idx], color=&#39;blue&#39;, alpha=0.3)
        label = &#39;Posterior Mean&#39;
        legends.append(patches.Rectangle(
            (1, 1), 1, 1, fill=True, color=&#39;blue&#39;, alpha=0.3, lw=0, label=&#39;95% Error Bars&#39;
        ))
        legends.append(plt.Line2D([0], [0], ls=&#39;:&#39;, color=&#39;blue&#39;, lw=2, label=label))

        xmin = min(np.min(data.X), np.min(X[j]))
        xmax = max(np.max(data.X), np.max(X[j]))
        if data.F is not None:
            if np.issubdtype(data.X.dtypes[0], np.datetime64):
                dt = np.timedelta64(1,data.X.get_time_unit())
                n = int((xmax-xmin) / dt) + 1
                x = np.arange(xmin, xmax+np.timedelta64(1,&#39;us&#39;), dt, dtype=data.X.dtypes[0])
            else:
                n = len(data.X)*10
                x = np.linspace(xmin, xmax, n)

            y = data.F(x)
            if transformed:
                y = data.Y.transform(y, x)

            ax.plot(x, y, &#39;r--&#39;, lw=1)
            legends.append(plt.Line2D([0], [0], ls=&#39;--&#39;, color=&#39;r&#39;, label=&#39;True&#39;))

        if data.has_test_data():
            x, y = data.get_test_data(transformed=transformed)
            ax.plot(x, y, &#39;g.&#39;, ms=10)
            legends.append(plt.Line2D([0], [0], ls=&#39;&#39;, color=&#39;g&#39;, marker=&#39;.&#39;, ms=10, label=&#39;Latent&#39;))

        x, y = data.get_train_data(transformed=transformed)
        ax.plot(x, y, &#39;r.&#39;, ms=10)
        legends.append(plt.Line2D([0], [0], ls=&#39;&#39;, color=&#39;r&#39;, marker=&#39;.&#39;, ms=10, label=&#39;Observations&#39;))

        if 0 &lt; len(data.removed_ranges[0]):
            for removed_range in data.removed_ranges[0]:
                x0 = removed_range[0]
                x1 = removed_range[1]
                y0 = ax.get_ylim()[0]
                y1 = ax.get_ylim()[1]
                ax.add_patch(patches.Rectangle(
                    (x0, y0), x1-x0, y1-y0, fill=True, color=&#39;xkcd:strawberry&#39;, alpha=0.2, lw=0,
                ))
            legends.append(patches.Rectangle(
                (1, 1), 1, 1, fill=True, color=&#39;xkcd:strawberry&#39;, alpha=0.5, lw=0, label=&#39;Removed Ranges&#39;
            ))

        ax.set_xlim(xmin - (xmax - xmin)*0.001, xmax + (xmax - xmin)*0.001)
        ax.set_xlabel(data.X_labels[0])
        ax.set_ylabel(data.Y_label)
        ax.set_title(data.name if title is None else title, fontsize=14)

    if legend:
        ax.legend(handles=legends[::-1])
    return fig, ax</code></pre>
</details>
</dd>
<dt id="mogptk.model.Model.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X=None, sigma=2, transformed=False, predict_y=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Predict using the prediction range of the data set and save the prediction in that data set. Otherwise, if <code>X</code> is passed, use that as the prediction range and return the prediction instead of saving it.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>list, dict</code></dt>
<dd>Dictionary where keys are channel index and elements numpy arrays with channel inputs. If passed, results will be returned and not saved in the data set for later retrieval.</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>float</code></dt>
<dd>Number of standard deviations to display upwards and downwards.</dd>
<dt><strong><code>transformed</code></strong> :&ensp;<code>boolean</code></dt>
<dd>Return transformed data as used for training.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>X prediction of shape (n,) for each channel.</dd>
<dt><code>numpy.ndarray</code></dt>
<dd>Y mean prediction of shape (n,) for each channel.</dd>
<dt><code>numpy.ndarray</code></dt>
<dd>Y lower prediction of uncertainty interval of shape (n,) for each channel.</dd>
<dt><code>numpy.ndarray</code></dt>
<dd>Y upper prediction of uncertainty interval of shape (n,) for each channel.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; model.predict(X)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L464-L518" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def predict(self, X=None, sigma=2, transformed=False, predict_y=True):
    &#34;&#34;&#34;
    Predict using the prediction range of the data set and save the prediction in that data set. Otherwise, if `X` is passed, use that as the prediction range and return the prediction instead of saving it.

    Args:
        X (list, dict): Dictionary where keys are channel index and elements numpy arrays with channel inputs. If passed, results will be returned and not saved in the data set for later retrieval.
        sigma (float): Number of standard deviations to display upwards and downwards.
        transformed (boolean): Return transformed data as used for training.

    Returns:
        numpy.ndarray: X prediction of shape (n,) for each channel.
        numpy.ndarray: Y mean prediction of shape (n,) for each channel.
        numpy.ndarray: Y lower prediction of uncertainty interval of shape (n,) for each channel.
        numpy.ndarray: Y upper prediction of uncertainty interval of shape (n,) for each channel.

    Examples:
        &gt;&gt;&gt; model.predict(X)
    &#34;&#34;&#34;
    if X is None:
        X = self.dataset.get_prediction_data()
    else:
        X = self.dataset._format_X(X)
    x = self._to_kernel_format(X)

    mu, var = self.gpr.predict(x, predict_y=predict_y, tensor=True)
    lower = mu - sigma*torch.sqrt(var)
    upper = mu + sigma*torch.sqrt(var)

    mu = mu.cpu().numpy()
    var = var.cpu().numpy()
    lower = lower.cpu().numpy()
    upper = upper.cpu().numpy()

    i = 0
    Mu = []
    Var = []
    Lower = []
    Upper = []
    for j in range(self.dataset.get_output_dims()):
        N = X[j].shape[0]
        Mu.append(np.squeeze(mu[i:i+N]))
        Var.append(np.squeeze(var[i:i+N]))
        Lower.append(np.squeeze(lower[i:i+N]))
        Upper.append(np.squeeze(upper[i:i+N]))
        i += N

    if not transformed:
        for j in range(self.dataset.get_output_dims()):
            Mu[j] = self.dataset[j].Y.detransform(Mu[j], X[j])
            Lower[j] = self.dataset[j].Y.detransform(Lower[j], X[j])
            Upper[j] = self.dataset[j].Y.detransform(Upper[j], X[j])

    if len(self.dataset) == 1:
        return X[0], Mu[0], Lower[0], Upper[0]
    return X, Mu, Lower, Upper</code></pre>
</details>
</dd>
<dt id="mogptk.model.Model.print_parameters"><code class="name flex">
<span>def <span class="ident">print_parameters</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Print the parameters of the model in a table.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; model.print_parameters()
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L195-L202" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def print_parameters(self):
    &#34;&#34;&#34;
    Print the parameters of the model in a table.

    Examples:
        &gt;&gt;&gt; model.print_parameters()
    &#34;&#34;&#34;
    self.gpr.print_parameters()</code></pre>
</details>
</dd>
<dt id="mogptk.model.Model.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>self, X=None, n=None, predict_y=True, transformed=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Sample n times from the kernel at input X .</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>list, dict</code></dt>
<dd>Dictionary where keys are channel index and elements numpy arrays with channel inputs.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of samples.</dd>
<dt><strong><code>predict_y</code></strong> :&ensp;<code>boolean</code></dt>
<dd>Predict data values instead of function values.</dd>
<dt><strong><code>transformed</code></strong> :&ensp;<code>boolean</code></dt>
<dd>Return transformed data as used for training.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>samples of shape len(X) for each channel if n is given.</dd>
<dt><code>numpy.ndarray</code></dt>
<dd>sample of shape len(X) for each channel if n is None.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; model.sample(n=10)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L545-L590" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def sample(self, X=None, n=None, predict_y=True, transformed=False):
    &#34;&#34;&#34;
    Sample n times from the kernel at input X .

    Args:
        X (list, dict): Dictionary where keys are channel index and elements numpy arrays with channel inputs.
        n (int): Number of samples.
        predict_y (boolean): Predict data values instead of function values.
        transformed (boolean): Return transformed data as used for training.

    Returns:
        list: samples of shape len(X) for each channel if n is given.
        numpy.ndarray: sample of shape len(X) for each channel if n is None.

    Examples:
        &gt;&gt;&gt; model.sample(n=10)
    &#34;&#34;&#34;
    if X is None:
        X = self.dataset.get_prediction_data()
    else:
        X = self.dataset._format_X(X)
    x = self._to_kernel_format(X)

    samples = self.gpr.sample(Z=x, n=n, predict_y=predict_y)

    i = 0
    Samples = []
    for j in range(self.dataset.get_output_dims()):
        N = X[j][0].shape[0]
        if n is None:
            sample = np.squeeze(samples[i:i+N])
            if not transformed:
                sample = self.dataset[j].Y.detransform(sample, X[j])
            Samples.append(sample)
        else:
            ss = []
            for k in range(n):
                sample = np.squeeze(samples[i:i+N,k])
                if not transformed:
                    sample = self.dataset[j].Y.detransform(sample, X[j])
                ss.append(sample)
            Samples.append(ss)
        i += N
    if self.dataset.get_output_dims() == 1:
        return Samples[0]
    return Samples</code></pre>
</details>
</dd>
<dt id="mogptk.model.Model.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Save the model to a given file that can then be loaded using <code><a title="mogptk.model.LoadModel" href="#mogptk.model.LoadModel">LoadModel()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>File name to save to, automatically appends '.npy'.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; model.save('filename')
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L225-L241" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def save(self, filename):
    &#34;&#34;&#34;
    Save the model to a given file that can then be loaded using `LoadModel()`.

    Args:
        filename (str): File name to save to, automatically appends &#39;.npy&#39;.

    Examples:
        &gt;&gt;&gt; model.save(&#39;filename&#39;)
    &#34;&#34;&#34;
    filename += &#34;.npy&#34; 
    try:
        os.remove(filename)
    except OSError:
        pass
    with open(filename, &#39;wb&#39;) as w:
        pickle.dump(self, w)</code></pre>
</details>
</dd>
<dt id="mogptk.model.Model.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, method='Adam', iters=500, verbose=False, error=None, plot=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the model by optimizing the (hyper)parameters of the kernel to approach the training data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>method</code></strong> :&ensp;<code>str</code></dt>
<dd>Optimizer to use such as LBFGS, Adam, Adagrad, or SGD.</dd>
<dt><strong><code>iters</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of iterations, or maximum in case of LBFGS optimizer.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Print verbose output about the state of the optimizer.</dd>
<dt><strong><code>error</code></strong> :&ensp;<code>str,function</code></dt>
<dd>Calculate prediction error for each iteration by the given method, such as MAE, MAPE, sMAPE, MSE, or RMSE. When a function is given, it should have parameters (y_true,y_pred) or (y_true,y_pred,model).</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code></dt>
<dd>Plot the loss and, if error is data set, the error of the test data points.</dd>
<dt><strong><code>**kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Additional dictionary of parameters passed to the PyTorch optimizer. </dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Losses for all iterations.</dd>
<dt><code>numpy.ndarray</code></dt>
<dd>Errors for all iterations. Only if <code>error</code> is set, otherwise zero.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; model.train()
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; model.train(method='lbfgs', tolerance_grad=1e-10, tolerance_change=1e-12)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; model.train(method='adam', lr=0.5)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L303-L431" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def train(
    self,
    method=&#39;Adam&#39;,
    iters=500,
    verbose=False,
    error=None,
    plot=False,
    **kwargs):
    &#34;&#34;&#34;
    Trains the model by optimizing the (hyper)parameters of the kernel to approach the training data.

    Args:
        method (str): Optimizer to use such as LBFGS, Adam, Adagrad, or SGD.
        iters (int): Number of iterations, or maximum in case of LBFGS optimizer.
        verbose (bool): Print verbose output about the state of the optimizer.
        error (str,function): Calculate prediction error for each iteration by the given method, such as MAE, MAPE, sMAPE, MSE, or RMSE. When a function is given, it should have parameters (y_true,y_pred) or (y_true,y_pred,model).
        plot (bool): Plot the loss and, if error is data set, the error of the test data points.
        **kwargs (dict): Additional dictionary of parameters passed to the PyTorch optimizer. 

    Returns:
        numpy.ndarray: Losses for all iterations.
        numpy.ndarray: Errors for all iterations. Only if `error` is set, otherwise zero.

    Examples:
        &gt;&gt;&gt; model.train()
        
        &gt;&gt;&gt; model.train(method=&#39;lbfgs&#39;, tolerance_grad=1e-10, tolerance_change=1e-12)
        
        &gt;&gt;&gt; model.train(method=&#39;adam&#39;, lr=0.5)
    &#34;&#34;&#34;
    error_use_all_data = False
    if error is not None and all(not channel.has_test_data() for channel in self.dataset):
        error_use_all_data = True

    if method.lower() in (&#39;l-bfgs&#39;, &#39;lbfgs&#39;, &#39;l-bfgs-b&#39;, &#39;lbfgsb&#39;):
        method = &#39;LBFGS&#39;
    elif method.lower() == &#39;adam&#39;:
        method = &#39;Adam&#39;
    elif method.lower() == &#39;sgd&#39;:
        method = &#39;SGD&#39;
    elif method.lower() == &#39;adagrad&#39;:
        method = &#39;AdaGrad&#39;
    else:
        raise ValueError(&#39;optimizer must be LBFGS, Adam, SGD, or AdaGrad&#39;)

    if verbose:
        training_points = sum([len(channel.get_train_data()[1]) for channel in self.dataset])
        parameters = sum([p.num_parameters if p.trainable else 0 for p in self.gpr.get_parameters()])
        print(&#39;\nStarting optimization using&#39;, method)
        if self.name is not None:
            print(&#39;‣ Model: %s&#39; % self.name)
        print(&#39;‣ Channels: %d&#39; % len(self.dataset))
        print(&#39;‣ Parameters: %d&#39; % parameters)
        print(&#39;‣ Training points: %d&#39; % training_points)
        print(&#39;‣ Initial loss: %6g&#39; % self.loss())
        if error is not None:
            print(&#39;‣ Initial error: %6g&#39; % self.error(error, error_use_all_data))

    iter_offset = 0
    times = np.zeros((iters+1,))
    losses = np.zeros((iters+1,))
    errors = np.zeros((iters+1,))
    if self.times.shape[0] != 0:
        iter_offset = self.times.shape[0]-1
        times = np.concatenate((self.times[:-1],times))
        losses = np.concatenate((self.losses[:-1],losses))
        errors = np.concatenate((self.errors[:-1],errors))
    initial_time = time.time()

    iters_len = int(math.log10(iter_offset+iters)) + 1
    def progress(i, loss):
        elapsed_time = time.time() - initial_time
        write = verbose and i % max(1,iters/100) == 0
        i += iter_offset
        times[i] = elapsed_time
        losses[i] = loss
        if error is not None:
            errors[i] = self.error(error, error_use_all_data)
            if write:
                print(&#34;  %*d/%*d %s  loss=%12g  error=%12g&#34; % (iters_len, i, iters_len, iter_offset+iters, _format_time(elapsed_time), losses[i], errors[i]))
        elif write:
            print(&#34;  %*d/%*d %s  loss=%12g&#34; % (iters_len, i, iters_len, iter_offset+iters, _format_time(elapsed_time), losses[i]))

    if verbose:
        print(&#34;\nStart %s:&#34; % (method,))
    if method == &#39;LBFGS&#39;:
        if not &#39;max_iter&#39; in kwargs:
            kwargs[&#39;max_iter&#39;] = iters
        else:
            iters = kwargs[&#39;max_iter&#39;]
        optimizer = torch.optim.LBFGS(self.gpr.parameters(), **kwargs)

        def loss():
            i = int(optimizer.state_dict()[&#39;state&#39;][0][&#39;func_evals&#39;])
            loss = self.loss()
            progress(i, loss)
            return loss
        optimizer.step(loss)
        iters = int(optimizer.state_dict()[&#39;state&#39;][0][&#39;func_evals&#39;])
    else:
        if method == &#39;Adam&#39;:
            optimizer = torch.optim.Adam(self.gpr.parameters(), **kwargs)
        elif method == &#39;SGD&#39;:
            optimizer = torch.optim.SGD(self.gpr.parameters(), **kwargs)
        elif method == &#39;AdaGrad&#39;:
            optimizer = torch.optim.Adagrad(self.gpr.parameters(), **kwargs)

        for i in range(iters):
            progress(i, self.loss())
            optimizer.step()
    progress(iters, self.loss())

    if verbose:
        elapsed_time = time.time() - initial_time
        print(&#34;Finished&#34;)
        print(&#39;\nOptimization finished in %s&#39; % _format_duration(elapsed_time))
        print(&#39;‣ Iterations: %d&#39; % iters)
        print(&#39;‣ Final loss: %6g&#39;% losses[iter_offset+iters])
        if error is not None:
            print(&#39;‣ Final error: %6g&#39; % errors[iter_offset+iters])

    self.iters = iter_offset+iters
    self.times = times[:iter_offset+iters+1]
    self.losses = losses[:iter_offset+iters+1]
    if error is not None:
        self.errors = errors[:iter_offset+iters+1]
    if plot:
        self.plot_losses()
    return losses, errors</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="mogptk.model.OpperArchambeau"><code class="flex name class">
<span>class <span class="ident">OpperArchambeau</span></span>
<span>(</span><span>likelihood=&lt;mogptk.gpr.likelihood.GaussianLikelihood object&gt;, jitter=1e-06)</span>
</code></dt>
<dd>
<div class="desc"><p>Inference using Opper and Archambeau 2009 for Gaussian process regression.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>likelihood</code></strong> :&ensp;<code>gpr.Likelihood</code></dt>
<dd>Likelihood $p(y|f)$.</dd>
<dt><strong><code>jitter</code></strong> :&ensp;<code>float</code></dt>
<dd>Jitter added before calculating a Cholesky.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L83-L96" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class OpperArchambeau:
    &#34;&#34;&#34;
    Inference using Opper and Archambeau 2009 for Gaussian process regression.

    Args:
        likelihood (gpr.Likelihood): Likelihood $p(y|f)$.
        jitter (float): Jitter added before calculating a Cholesky.
    &#34;&#34;&#34;
    def __init__(self, likelihood=gpr.GaussianLikelihood(1.0), jitter=1e-6):
        self.likelihood = likelihood
        self.jitter = jitter

    def _build(self, kernel, x, y, y_err=None, mean=None, name=None):
        return gpr.OpperArchambeau(kernel, x, y, likelihood=likelihood, jitter=self.jitter, mean=mean, name=name)</code></pre>
</details>
</dd>
<dt id="mogptk.model.Snelson"><code class="flex name class">
<span>class <span class="ident">Snelson</span></span>
<span>(</span><span>inducing_points=10, init_inducing_points='grid', variance=None, jitter=1e-06)</span>
</code></dt>
<dd>
<div class="desc"><p>Inference using Snelson and Ghahramani 2005 for Gaussian process regression.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inducing_points</code></strong> :&ensp;<code>int,list</code></dt>
<dd>Number of inducing points or the locations of the inducing points.</dd>
<dt><strong><code>init_inducing_points</code></strong> :&ensp;<code>str</code></dt>
<dd>Method for initialization of inducing points, can be <code>grid</code>, <code>random</code>, or <code>density</code>.</dd>
<dt><strong><code>variance</code></strong> :&ensp;<code>float</code></dt>
<dd>Variance of the Gaussian likelihood.</dd>
<dt><strong><code>jitter</code></strong> :&ensp;<code>float</code></dt>
<dd>Jitter added before calculating a Cholesky.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L62-L81" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Snelson:
    &#34;&#34;&#34;
    Inference using Snelson and Ghahramani 2005 for Gaussian process regression.

    Args:
        inducing_points (int,list): Number of inducing points or the locations of the inducing points.
        init_inducing_points (str): Method for initialization of inducing points, can be `grid`, `random`, or `density`.
        variance (float): Variance of the Gaussian likelihood.
        jitter (float): Jitter added before calculating a Cholesky.
    &#34;&#34;&#34;
    def __init__(self, inducing_points=10, init_inducing_points=&#39;grid&#39;, variance=None, jitter=1e-6):
        self.inducing_points = inducing_points
        self.init_inducing_points = init_inducing_points
        self.variance = variance
        self.jitter = jitter

    def _build(self, kernel, x, y, y_err=None, mean=None, name=None):
        if variance is None:
            variance = [1.0] * kernel.output_dims
        return gpr.Snelson(kernel, x, y, Z=self.inducing_points, Z_init=self.init_inducing_points, variance=self.variance, jitter=self.jitter, mean=mean, name=name)</code></pre>
</details>
</dd>
<dt id="mogptk.model.Titsias"><code class="flex name class">
<span>class <span class="ident">Titsias</span></span>
<span>(</span><span>inducing_points=10, init_inducing_points='grid', variance=1.0, jitter=1e-06)</span>
</code></dt>
<dd>
<div class="desc"><p>Inference using Titsias 2009 for Gaussian process regression.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inducing_points</code></strong> :&ensp;<code>int,list</code></dt>
<dd>Number of inducing points or the locations of the inducing points.</dd>
<dt><strong><code>init_inducing_points</code></strong> :&ensp;<code>str</code></dt>
<dd>Method for initialization of inducing points, can be <code>grid</code>, <code>random</code>, or <code>density</code>.</dd>
<dt><strong><code>variance</code></strong> :&ensp;<code>float</code></dt>
<dd>Variance of the Gaussian likelihood.</dd>
<dt><strong><code>jitter</code></strong> :&ensp;<code>float</code></dt>
<dd>Jitter added before calculating a Cholesky.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/GAMES-UChile/mogptk/blob/a052f17a108289cc80a267d138ab6e3810a2a1e7/mogptk/model.py#L98-L115" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Titsias:
    &#34;&#34;&#34;
    Inference using Titsias 2009 for Gaussian process regression.

    Args:
        inducing_points (int,list): Number of inducing points or the locations of the inducing points.
        init_inducing_points (str): Method for initialization of inducing points, can be `grid`, `random`, or `density`.
        variance (float): Variance of the Gaussian likelihood.
        jitter (float): Jitter added before calculating a Cholesky.
    &#34;&#34;&#34;
    def __init__(self, inducing_points=10, init_inducing_points=&#39;grid&#39;, variance=1.0, jitter=1e-6):
        self.inducing_points = inducing_points
        self.init_inducing_points = init_inducing_points
        self.variance = variance
        self.jitter = jitter

    def _build(self, kernel, x, y, y_err=None, mean=None, name=None):
        return gpr.Titsias(kernel, x, y, Z=self.inducing_points, Z_init=self.init_inducing_points, variance=self.variance, jitter=self.jitter, mean=mean, name=name)</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="mogptk" href="index.html">mogptk</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="mogptk.model.LoadModel" href="#mogptk.model.LoadModel">LoadModel</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="mogptk.model.Exact" href="#mogptk.model.Exact">Exact</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.model.Hensman" href="#mogptk.model.Hensman">Hensman</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.model.Model" href="#mogptk.model.Model">Model</a></code></h4>
<ul class="">
<li><code><a title="mogptk.model.Model.K" href="#mogptk.model.Model.K">K</a></code></li>
<li><code><a title="mogptk.model.Model.copy_parameters" href="#mogptk.model.Model.copy_parameters">copy_parameters</a></code></li>
<li><code><a title="mogptk.model.Model.error" href="#mogptk.model.Model.error">error</a></code></li>
<li><code><a title="mogptk.model.Model.get_parameters" href="#mogptk.model.Model.get_parameters">get_parameters</a></code></li>
<li><code><a title="mogptk.model.Model.log_marginal_likelihood" href="#mogptk.model.Model.log_marginal_likelihood">log_marginal_likelihood</a></code></li>
<li><code><a title="mogptk.model.Model.loss" href="#mogptk.model.Model.loss">loss</a></code></li>
<li><code><a title="mogptk.model.Model.plot_gram" href="#mogptk.model.Model.plot_gram">plot_gram</a></code></li>
<li><code><a title="mogptk.model.Model.plot_kernel" href="#mogptk.model.Model.plot_kernel">plot_kernel</a></code></li>
<li><code><a title="mogptk.model.Model.plot_losses" href="#mogptk.model.Model.plot_losses">plot_losses</a></code></li>
<li><code><a title="mogptk.model.Model.plot_prediction" href="#mogptk.model.Model.plot_prediction">plot_prediction</a></code></li>
<li><code><a title="mogptk.model.Model.predict" href="#mogptk.model.Model.predict">predict</a></code></li>
<li><code><a title="mogptk.model.Model.print_parameters" href="#mogptk.model.Model.print_parameters">print_parameters</a></code></li>
<li><code><a title="mogptk.model.Model.sample" href="#mogptk.model.Model.sample">sample</a></code></li>
<li><code><a title="mogptk.model.Model.save" href="#mogptk.model.Model.save">save</a></code></li>
<li><code><a title="mogptk.model.Model.train" href="#mogptk.model.Model.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mogptk.model.OpperArchambeau" href="#mogptk.model.OpperArchambeau">OpperArchambeau</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.model.Snelson" href="#mogptk.model.Snelson">Snelson</a></code></h4>
</li>
<li>
<h4><code><a title="mogptk.model.Titsias" href="#mogptk.model.Titsias">Titsias</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>